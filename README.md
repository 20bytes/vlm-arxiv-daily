<p align="center">
  <h1 align="center">ğŸ¤– VLM-Arxiv-Daily</h1>
  <p align="center">
    <img src="https://img.shields.io/badge/Robotics-VLA-orange?style=for-the-badge&logo=robotframework" alt="Robotics">
    <img src="https://img.shields.io/badge/Navigation-VLN-blue?style=for-the-badge&logo=googlemaps" alt="Navigation">
    <img src="https://img.shields.io/badge/Vision--Language-VLM-green?style=for-the-badge&logo=openai" alt="VLM">
  </p>
</p>

> ğŸš€ æ¯æ—¥è‡ªåŠ¨è¿½è¸ª **Vision-Language-Action (VLA)**, **Vision-Language Navigation (VLN)** å’Œ **Vision-Language Models (VLM)** çš„æœ€æ–° Arxiv è®ºæ–‡ã€‚

## ğŸ“… Updated on 2026.02.22
<details>
  <summary>ç‚¹å‡»æŸ¥çœ‹ç›®å½• (Table of Contents)</summary>
  <ol>
    <li><a href=#vla>VLA</a></li>
    <li><a href=#vln>VLN</a></li>
    <li><a href=#vlm>VLM</a></li>
  </ol>
</details>

### ğŸ“Œ VLA

|Publish Date (YYYY-MM-DD)|Title|Authors|PDF|HJFY|
|---|---|---|---|---|
|**2026-02-19**|**When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs<br>å½“è§†è§‰å‡Œé©¾äºè¯­è¨€ä¹‹ä¸Šï¼šè¯„ä¼°ä¸ç¼“è§£è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ä¸­çš„åäº‹å®å¤±è´¥<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17659.html)**|Mingyu Ding Team|[2602.17659](http://arxiv.org/abs/2602.17659)|[HJFY](https://hjfy.top/arxiv/2602.17659v1)|
|**2026-02-19**|**What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?<br>ä»€ä¹ˆåœ¨ç ´åå…·èº«äººå·¥æ™ºèƒ½å®‰å…¨ï¼šå¤§è¯­è¨€æ¨¡å‹æ¼æ´ã€ä¿¡æ¯ç‰©ç†ç³»ç»Ÿç¼ºé™·ï¼Œè¿˜æ˜¯å…¶ä»–å› ç´ ï¼Ÿ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17345.html)**|Yue Zhang Team|[2602.17345](http://arxiv.org/abs/2602.17345)|[HJFY](https://hjfy.top/arxiv/2602.17345v1)|
|**2026-02-19**|**FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment<br>FRAPPEï¼šé€šè¿‡å¤šæœªæ¥è¡¨ç¤ºå¯¹é½å°†ä¸–ç•Œå»ºæ¨¡èå…¥é€šç”¨ç­–ç•¥<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17259.html)**|Donglin Wang Team|[2602.17259](http://arxiv.org/abs/2602.17259)|[HJFY](https://hjfy.top/arxiv/2602.17259v1)|
|**2026-02-19**|**Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web<br>ç½‘ç»œåŠ¨è¯ï¼šé¢å‘æ™ºèƒ½ç½‘ç»œå¯é ä»»åŠ¡ç»„åˆçš„ç±»å‹åŒ–æŠ½è±¡<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17245.html)**|Suman Nath Team|[2602.17245](http://arxiv.org/abs/2602.17245)|[HJFY](https://hjfy.top/arxiv/2602.17245v1)|
|**2026-02-19**|**Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success<br>è¯„ä¼°ç‰©ä½“å§¿æ€ä¼°è®¡ä¸é‡å»ºå¯¹æœºå™¨äººæŠ“å–æˆåŠŸç‡å½±å“çš„åŸºå‡†ç ”ç©¶<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17101.html)**|Torsten Sattler Team|[2602.17101](http://arxiv.org/abs/2602.17101)|[HJFY](https://hjfy.top/arxiv/2602.17101v1)|
|**2026-02-18**|**MALLVI: a multi agent framework for integrated generalized robotics manipulation<br>MALLVIï¼šä¸€ç§é¢å‘é›†æˆé€šç”¨æœºå™¨äººæ“ä½œçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.16898.html)**|Babak Khalaj Team|[2602.16898](http://arxiv.org/abs/2602.16898)|[HJFY](https://hjfy.top/arxiv/2602.16898v1)|
|**2026-02-18**|**EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data<br>EgoScaleï¼šåˆ©ç”¨å¤šæ ·åŒ–çš„è‡ªæˆ‘ä¸­å¿ƒäººç±»æ•°æ®æ‰©å±•çµå·§æ“ä½œèƒ½åŠ›<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.16710.html)**|Linxi Fan Team|[2602.16710](http://arxiv.org/abs/2602.16710)|[HJFY](https://hjfy.top/arxiv/2602.16710v1)|
|**2026-02-19**|**RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation<br>RoboGeneï¼šé€šè¿‡å¤šæ ·æ€§é©±åŠ¨çš„æ™ºèƒ½ä½“æ¡†æ¶æå‡è§†è§‰è¯­è¨€åŠ¨ä½œé¢„è®­ç»ƒï¼Œå®ç°çœŸå®ä¸–ç•Œä»»åŠ¡ç”Ÿæˆ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.16444.html)**|Jian Tang Team|[2602.16444](http://arxiv.org/abs/2602.16444)|[HJFY](https://hjfy.top/arxiv/2602.16444v2)|
|**2026-02-17**|**Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation<br>å­¦ä¹ æ£€ç´¢å¯å¯¼èˆªå€™é€‰å¯¹è±¡ä»¥å®ç°é«˜æ•ˆçš„è§†è§‰ä¸è¯­è¨€å¯¼èˆª<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.15724.html)**|Lina Yao Team|[2602.15724](http://arxiv.org/abs/2602.15724)|[HJFY](https://hjfy.top/arxiv/2602.15724v1)|
|**2026-02-17**|**The Next Paradigm Is User-Centric Agent, Not Platform-Centric Service<br>ä¸‹ä¸€ä»£èŒƒå¼æ˜¯ç”¨æˆ·ä¸­å¿ƒæ™ºèƒ½ä½“ï¼Œè€Œéå¹³å°ä¸­å¿ƒæœåŠ¡<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.15682.html)**|Enhong Chen Team|[2602.15682](http://arxiv.org/abs/2602.15682)|[HJFY](https://hjfy.top/arxiv/2602.15682v1)|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment<br>æ‰©å±•éªŒè¯åœ¨è§†è§‰-è¯­è¨€-åŠ¨ä½œå¯¹é½ä¸­æ¯”æ‰©å±•ç­–ç•¥å­¦ä¹ æ›´æœ‰æ•ˆ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.12281.html)**|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|[HJFY](https://hjfy.top/arxiv/2602.12281v1)|
|**2026-02-12**|**Embodied AI Agents for Team Collaboration in Co-located Blue-Collar Work<br>é¢å‘å…±å€è“é¢†å·¥ä½œå›¢é˜Ÿåä½œçš„å…·èº«äººå·¥æ™ºèƒ½ä½“<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.12136.html)**|Thomas Olsson Team|[2602.12136](http://arxiv.org/abs/2602.12136)|[HJFY](https://hjfy.top/arxiv/2602.12136v1)|
|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning<br>GigaBrain-0.5M*ï¼šä¸€ç§åŸºäºä¸–ç•Œæ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.12099.html)**|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|[HJFY](https://hjfy.top/arxiv/2602.12099v1)|
|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model<br>VLAWï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥ä¸ä¸–ç•Œæ¨¡å‹çš„è¿­ä»£ååŒæ”¹è¿›<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.12063.html)**|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|[HJFY](https://hjfy.top/arxiv/2602.12063v1)|
|**2026-02-12**|**HoloBrain-0 Technical Report<br>HoloBrain-0æŠ€æœ¯æŠ¥å‘Š<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.12062.html)**|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|[HJFY](https://hjfy.top/arxiv/2602.12062v1)|
|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?<br>è§†è§‰-æœ¬ä½“æ„ŸçŸ¥ç­–ç•¥åœ¨æœºå™¨äººæ“ä½œä¸­ä½•æ—¶ä¼šå¤±æ•ˆï¼Ÿ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.12032.html)**|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|[HJFY](https://hjfy.top/arxiv/2602.12032v1)|
|**2026-02-12**|**Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control<br>Robot-DIFTï¼šæå–æ‰©æ•£ç‰¹å¾ä»¥å®ç°å‡ ä½•ä¸€è‡´çš„è§†è§‰è¿åŠ¨æ§åˆ¶<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.11934.html)**|Georgia Chalvatzaki Team|[2602.11934](http://arxiv.org/abs/2602.11934)|[HJFY](https://hjfy.top/arxiv/2602.11934v1)|
|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models<br>JEPA-VLAï¼šè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹éœ€è¦è§†é¢‘é¢„æµ‹æ€§åµŒå…¥<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.11832.html)**|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|[HJFY](https://hjfy.top/arxiv/2602.11832v1)|
|**2026-02-12**|**Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes<br>Clutt3R-Segï¼šé¢å‘æ‚ä¹±åœºæ™¯ä¸­è¯­è¨€é©±åŠ¨æŠ“å–çš„ç¨€ç–è§†è§’ä¸‰ç»´å®ä¾‹åˆ†å‰²<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.11660.html)**|Ayoung Kim Team|[2602.11660](http://arxiv.org/abs/2602.11660)|[HJFY](https://hjfy.top/arxiv/2602.11660v1)|
|**2026-02-12**|**ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning<br>ViTaSï¼šé¢å‘è§†è§‰è¿åŠ¨å­¦ä¹ çš„è§†è§‰è§¦è§‰è½¯èåˆå¯¹æ¯”å­¦ä¹ <br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.11643.html)**|Huazhe Xu Team|[2602.11643](http://arxiv.org/abs/2602.11643)|[HJFY](https://hjfy.top/arxiv/2602.11643v1)|
|**2026-02-10**|**MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation<br>MVISTA-4Dï¼šå…·æœ‰æµ‹è¯•æ—¶åŠ¨ä½œæ¨ç†èƒ½åŠ›çš„è§†å›¾ä¸€è‡´4Dä¸–ç•Œæ¨¡å‹ï¼Œç”¨äºæœºå™¨äººæ“ä½œ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09878.html)**|Xiangyu Yue Team|[2602.09878](http://arxiv.org/abs/2602.09878)|[HJFY](https://hjfy.top/arxiv/2602.09878v1)|
|**2026-02-10**|**Code2World: A GUI World Model via Renderable Code Generation<br>Code2Worldï¼šé€šè¿‡å¯æ¸²æŸ“ä»£ç ç”Ÿæˆçš„GUIä¸–ç•Œæ¨¡å‹<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09856.html)**|Kevin Qinghong Lin Team|[2602.09856](http://arxiv.org/abs/2602.09856)|[HJFY](https://hjfy.top/arxiv/2602.09856v1)|
|**2026-02-10**|**BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation<br>BagelVLAï¼šé€šè¿‡äº¤é”™è§†è§‰-è¯­è¨€-åŠ¨ä½œç”Ÿæˆå¢å¼ºé•¿æ—¶ç¨‹æ“ä½œèƒ½åŠ›<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09849.html)**|Jianyu Chen Team|[2602.09849](http://arxiv.org/abs/2602.09849)|[HJFY](https://hjfy.top/arxiv/2602.09849v1)|
|**2026-02-10**|**NavDreamer: Video Models as Zero-Shot 3D Navigators<br>NavDreamerï¼šè§†é¢‘æ¨¡å‹ä½œä¸ºé›¶æ ·æœ¬ä¸‰ç»´å¯¼èˆªå™¨<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09765.html)**|Fei Gao Team|[2602.09765](http://arxiv.org/abs/2602.09765)|[HJFY](https://hjfy.top/arxiv/2602.09765v1)|
|**2026-02-10**|**Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization<br>é‡æ–°å®¡è§†è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„è§„æ¨¡åŒ–ï¼šå¯¹é½ã€æ··åˆä¸æ­£åˆ™åŒ–<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09722.html)**|Qin Jin Team|[2602.09722](http://arxiv.org/abs/2602.09722)|[HJFY](https://hjfy.top/arxiv/2602.09722v1)|
|**2026-02-10**|**AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild<br>AutoFlyï¼šé¢å‘é‡å¤–æ— äººæœºè‡ªä¸»å¯¼èˆªçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09657.html)**|Hui Xiong Team|[2602.09657](http://arxiv.org/abs/2602.09657)|[HJFY](https://hjfy.top/arxiv/2602.09657v1)|
|**2026-02-10**|**VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model<br>VideoAffordï¼šåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»äºº-ç‰©äº¤äº’è§†é¢‘ä¸­å®ç°ä¸‰ç»´åŠŸèƒ½å¯åŠæ€§æ¥åœ°<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09638.html)**|Hui Xiong Team|[2602.09638](http://arxiv.org/abs/2602.09638)|[HJFY](https://hjfy.top/arxiv/2602.09638v1)|
|**2026-02-10**|**Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures<br>Hand2Worldï¼šåŸºäºè‡ªç”±ç©ºé—´æ‰‹åŠ¿çš„è‡ªå›å½’ç¬¬ä¸€äººç§°äº¤äº’ç”Ÿæˆ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09600.html)**|Xingang Pan Team|[2602.09600](http://arxiv.org/abs/2602.09600)|[HJFY](https://hjfy.top/arxiv/2602.09600v1)|
|**2026-02-10**|**Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation<br>é¢å‘å¯å˜å½¢ç‰©ä½“æ“ä½œçš„åå¥½å¯¹é½è§†è§‰è¿åŠ¨æ‰©æ•£ç­–ç•¥<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09583.html)**|Danica Kragic Team|[2602.09583](http://arxiv.org/abs/2602.09583)|[HJFY](https://hjfy.top/arxiv/2602.09583v1)|
|**2026-02-10**|**AUHead: Realistic Emotional Talking Head Generation via Action Units Control<br>AUHeadï¼šåŸºäºåŠ¨ä½œå•å…ƒæ§åˆ¶çš„é€¼çœŸæƒ…æ„Ÿè¯´è¯å¤´éƒ¨ç”Ÿæˆ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09534.html)**|Tat-Seng Chua Team|[2602.09534](http://arxiv.org/abs/2602.09534)|[HJFY](https://hjfy.top/arxiv/2602.09534v1)|
|**2026-02-04**|**Capturing Visual Environment Structure Correlates with Control Performance<br>æ•æ‰è§†è§‰ç¯å¢ƒç»“æ„ä¸æ§åˆ¶æ€§èƒ½çš„ç›¸å…³æ€§<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04880.html)**|Yu-Xiong Wang Team|[2602.04880](http://arxiv.org/abs/2602.04880)|[HJFY](https://hjfy.top/arxiv/2602.04880v1)|
|**2026-02-04**|**CoWTracker: Tracking by Warping instead of Correlation<br>CoWTrackerï¼šé€šè¿‡å˜å½¢è€Œéç›¸å…³æ€§è¿›è¡Œè·Ÿè¸ª<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04877.html)**|Andrea Vedaldi Team|[2602.04877](http://arxiv.org/abs/2602.04877)|[HJFY](https://hjfy.top/arxiv/2602.04877v1)|
|**2026-02-04**|**Relational Scene Graphs for Object Grounding of Natural Language Commands<br>é¢å‘è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­ç‰©ä½“å®šä½çš„å…³ç³»åœºæ™¯å›¾<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04635.html)**|Ville Kyrki Team|[2602.04635](http://arxiv.org/abs/2602.04635)|[HJFY](https://hjfy.top/arxiv/2602.04635v1)|
|**2026-02-04**|**Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data<br>è¡ŒåŠ¨ã€æ„ŸçŸ¥ã€å†è¡ŒåŠ¨ï¼šä»å¤§è§„æ¨¡ç¬¬ä¸€äººç§°äººç±»æ•°æ®ä¸­å­¦ä¹ éé©¬å°”å¯å¤«ä¸»åŠ¨æ„ŸçŸ¥ç­–ç•¥<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04600.html)**|Wenzhao Lian Team|[2602.04600](http://arxiv.org/abs/2602.04600)|[HJFY](https://hjfy.top/arxiv/2602.04600v1)|
|**2026-02-04**|**A Unified Complementarity-based Approach for Rigid-Body Manipulation and Motion Prediction<br>åŸºäºäº’è¡¥æ€§çš„ç»Ÿä¸€æ–¹æ³•åœ¨åˆšä½“æ“ä½œä¸è¿åŠ¨é¢„æµ‹ä¸­çš„åº”ç”¨<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04522.html)**|Riddhiman Laha Team|[2602.04522](http://arxiv.org/abs/2602.04522)|[HJFY](https://hjfy.top/arxiv/2602.04522v1)|
|**2026-02-04**|**EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models<br>EgoActorï¼šé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹å°†ä»»åŠ¡è§„åˆ’è½åœ°ä¸ºå…·èº«æœºå™¨äººçš„ç©ºé—´æ„ŸçŸ¥è‡ªæˆ‘ä¸­å¿ƒåŠ¨ä½œ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04515.html)**|BÃ¶rje F. Karlsson Team|[2602.04515](http://arxiv.org/abs/2602.04515)|[HJFY](https://hjfy.top/arxiv/2602.04515v1)|
|**2026-02-04**|**Self-evolving Embodied AI<br>è‡ªæ¼”åŒ–çš„å…·èº«äººå·¥æ™ºèƒ½<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04411.html)**|Wenwu Zhu Team|[2602.04411](http://arxiv.org/abs/2602.04411)|[HJFY](https://hjfy.top/arxiv/2602.04411v1)|
|**2026-02-04**|**GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning<br>GeneralVLAï¼šå…·å¤‡çŸ¥è¯†å¼•å¯¼è½¨è¿¹è§„åˆ’çš„é€šç”¨è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04315.html)**|Hao Tang Team|[2602.04315](http://arxiv.org/abs/2602.04315)|[HJFY](https://hjfy.top/arxiv/2602.04315v1)|
|**2026-02-04**|**Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation<br>è§†è§’è‡³å…³é‡è¦ï¼šåˆ©ç”¨æ©ç è‡ªç¼–ç å™¨åŠ¨æ€ä¼˜åŒ–è§†è§‰æ“æ§çš„è§†è§’<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04243.html)**|Wenzhao Lian Team|[2602.04243](http://arxiv.org/abs/2602.04243)|[HJFY](https://hjfy.top/arxiv/2602.04243v1)|
|**2026-02-04**|**GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning<br>GeoLanGï¼šåŸºäºç»Ÿä¸€RGB-Då¤šæ¨¡æ€å­¦ä¹ çš„å‡ ä½•æ„ŸçŸ¥è¯­è¨€å¼•å¯¼æŠ“å–<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04231.html)**|Hongliang Ren Team|[2602.04231](http://arxiv.org/abs/2602.04231)|[HJFY](https://hjfy.top/arxiv/2602.04231v1)|
|**2026-02-02**|**TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments<br>TIC-VLAï¼šä¸€ç§ç”¨äºåŠ¨æ€ç¯å¢ƒä¸­æœºå™¨äººå¯¼èˆªçš„æ€æ§ä¸€ä½“åŒ–è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02459.html)**|Jiaqi Ma Team|[2602.02459](http://arxiv.org/abs/2602.02459)|[HJFY](https://hjfy.top/arxiv/2602.02459v1)|
|**2026-02-02**|**World-Gymnast: Training Robots with Reinforcement Learning in a World Model<br>ä¸–ç•Œä½“æ“å®¶ï¼šåœ¨ä¸–ç•Œæ¨¡å‹ä¸­é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœºå™¨äºº<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02454.html)**|Sherry Yang Team|[2602.02454](http://arxiv.org/abs/2602.02454)|[HJFY](https://hjfy.top/arxiv/2602.02454v1)|
|**2026-02-02**|**SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation<br>SoMAï¼šé¢å‘æœºå™¨äººè½¯ä½“æ“ä½œçš„çœŸå®åˆ°ä»¿çœŸç¥ç»æ¨¡æ‹Ÿå™¨<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02402.html)**|Jiangmiao Pang Team|[2602.02402](http://arxiv.org/abs/2602.02402)|[HJFY](https://hjfy.top/arxiv/2602.02402v1)|
|**2026-02-02**|**MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models<br>MAIN-VLAï¼šä¸ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å»ºæ¨¡æ„å›¾ä¸ç¯å¢ƒçš„æŠ½è±¡<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02212.html)**|Lemiao Qiu Team|[2602.02212](http://arxiv.org/abs/2602.02212)|[HJFY](https://hjfy.top/arxiv/2602.02212v1)|
|**2026-02-02**|**FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation<br>FD-VLAï¼šç”¨äºæ¥è§¦ä¸°å¯Œæ“ä½œçš„åŠ›è’¸é¦è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02142.html)**|Haiyue Zhu Team|[2602.02142](http://arxiv.org/abs/2602.02142)|[HJFY](https://hjfy.top/arxiv/2602.02142v1)|
|**2026-02-02**|**See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers<br>See2Refineï¼šè§†è§‰-è¯­è¨€åé¦ˆæå‡åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„eHMIè¡Œä¸ºè®¾è®¡èƒ½åŠ›<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02063.html)**|Takeo Igarashi Team|[2602.02063](http://arxiv.org/abs/2602.02063)|[HJFY](https://hjfy.top/arxiv/2602.02063v1)|
|**2026-02-02**|**Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models<br>é¢å‘è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹æ¨ç†æ—¶å®‰å…¨æ€§çš„æ¦‚å¿µè¯å…¸å­¦ä¹ æ–¹æ³•<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.01834.html)**|Di Wang Team|[2602.01834](http://arxiv.org/abs/2602.01834)|[HJFY](https://hjfy.top/arxiv/2602.01834v1)|
|**2026-02-02**|**From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models<br>ä»ç²¾ç¡®è®¤çŸ¥åˆ°ç²¾å‡†æ‰§è¡Œï¼šé¢å‘è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹çš„é€šç”¨è‡ªæ ¡æ­£ä¸ç»ˆæ­¢æ¡†æ¶<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.01811.html)**|Jianzong Wang Team|[2602.01811](http://arxiv.org/abs/2602.01811)|[HJFY](https://hjfy.top/arxiv/2602.01811v1)|
|**2026-02-02**|**AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act<br>AgenticLabï¼šä¸€ä¸ªèƒ½å¤Ÿè§‚å¯Ÿã€æ€è€ƒä¸è¡ŒåŠ¨çš„çœŸå®ä¸–ç•Œæœºå™¨äººæ™ºèƒ½ä½“å¹³å°<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.01662.html)**|Yu She Team|[2602.01662](http://arxiv.org/abs/2602.01662)|[HJFY](https://hjfy.top/arxiv/2602.01662v1)|
|**2026-02-02**|**From Perception to Action: Spatial AI Agents and World Models<br>ä»æ„ŸçŸ¥åˆ°è¡ŒåŠ¨ï¼šç©ºé—´äººå·¥æ™ºèƒ½ä»£ç†ä¸ä¸–ç•Œæ¨¡å‹<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.01644.html)**|Esteban Rojas Team|[2602.01644](http://arxiv.org/abs/2602.01644)|[HJFY](https://hjfy.top/arxiv/2602.01644v1)|
|**2026-01-30**|**Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation**|Wu Songwei et.al.|[2601.23087](http://arxiv.org/abs/2601.23087)|
|**2026-01-30**|**EAG-PT: Emission-Aware Gaussians and Path Tracing for Indoor Scene Reconstruction and Editing**|Xijie Yang et.al.|[2601.23065](http://arxiv.org/abs/2601.23065)|
|**2026-01-30**|**Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation**|Di Zhang et.al.|[2601.22988](http://arxiv.org/abs/2601.22988)|
|**2026-01-30**|**Alignment among Language, Vision and Action Representations**|Nicola Milano et.al.|[2601.22948](http://arxiv.org/abs/2601.22948)|
|**2026-01-30**|**When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection**|Shashank Mishra et.al.|[2601.22868](http://arxiv.org/abs/2601.22868)|
|**2026-01-30**|**Vision-Language Models Unlock Task-Centric Latent Actions**|Alexander Nikulin et.al.|[2601.22714](http://arxiv.org/abs/2601.22714)|
|**2026-01-30**|**Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference**|Emilien BirÃ© et.al.|[2601.22701](http://arxiv.org/abs/2601.22701)|
|**2026-01-30**|**CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control**|Jiaqi Shi et.al.|[2601.22467](http://arxiv.org/abs/2601.22467)|
|**2026-01-29**|**PoSafeNet: Safe Learning with Poset-Structured Neural Nets**|Kiwan Wong et.al.|[2601.22356](http://arxiv.org/abs/2601.22356)|
|**2026-01-29**|**DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation**|Haozhe Xie et.al.|[2601.22153](http://arxiv.org/abs/2601.22153)|
|**2026-01-29**|**PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction**|Changjian Jiang et.al.|[2601.22046](http://arxiv.org/abs/2601.22046)|
|**2026-01-29**|**PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy**|Jinhao Zhang et.al.|[2601.22018](http://arxiv.org/abs/2601.22018)|
|**2026-01-29**|**Causal World Modeling for Robot Control**|Lin Li et.al.|[2601.21998](http://arxiv.org/abs/2601.21998)|
|**2026-01-29**|**MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts**|Lorenzo Mazza et.al.|[2601.21971](http://arxiv.org/abs/2601.21971)|
|**2026-01-29**|**Information Filtering via Variational Regularization for Robot Manipulation**|Jinhao Zhang et.al.|[2601.21926](http://arxiv.org/abs/2601.21926)|
|**2026-01-29**|**Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation**|Jiankun Peng et.al.|[2601.21751](http://arxiv.org/abs/2601.21751)|
|**2026-01-29**|**CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation**|Xuanran Zhai et.al.|[2601.21712](http://arxiv.org/abs/2601.21712)|
|**2026-01-29**|**AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation**|Jianli Sun et.al.|[2601.21602](http://arxiv.org/abs/2601.21602)|
|**2026-01-29**|**EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots**|Zixing Lei et.al.|[2601.21570](http://arxiv.org/abs/2601.21570)|

<p align=right>(<a href=#updated-on-20260222>back to top</a>)</p>

### ğŸ“Œ VLN

|Publish Date (YYYY-MM-DD)|Title|Authors|PDF|HJFY|
|---|---|---|---|---|
|**2026-02-17**|**Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation<br>å­¦ä¹ æ£€ç´¢å¯å¯¼èˆªå€™é€‰å¯¹è±¡ä»¥å®ç°é«˜æ•ˆçš„è§†è§‰ä¸è¯­è¨€å¯¼èˆª<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.15724.html)**|Lina Yao Team|[2602.15724](http://arxiv.org/abs/2602.15724)|[HJFY](https://hjfy.top/arxiv/2602.15724v1)|
|**2026-02-17**|**One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation<br>ä¸€æ™ºä½“å¼•é¢†å…¨å±€ï¼šé€šè¿‡æ˜¾å¼ä¸–ç•Œè¡¨å¾èµ‹èƒ½å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å®ç°è§†è§‰ä¸è¯­è¨€å¯¼èˆª<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.15400.html)**|Qi Wu Team|[2602.15400](http://arxiv.org/abs/2602.15400)|[HJFY](https://hjfy.top/arxiv/2602.15400v1)|
|**2026-02-16**|**pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI<br>pFedNaviï¼šé¢å‘å…·èº«AIçš„ç»“æ„æ„ŸçŸ¥ä¸ªæ€§åŒ–è”é‚¦è§†è§‰è¯­è¨€å¯¼èˆª<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.14401.html)**|Haibing Guan Team|[2602.14401](http://arxiv.org/abs/2602.14401)|[HJFY](https://hjfy.top/arxiv/2602.14401v1)|
|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation<br>ABot-N0ï¼šé¢å‘é€šç”¨å…·èº«å¯¼èˆªçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œåŸºç¡€æ¨¡å‹æŠ€æœ¯æŠ¥å‘Š<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.11598.html)**|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|[HJFY](https://hjfy.top/arxiv/2602.11598v1)|
|**2026-02-10**|**Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning<br>Hydra-Navï¼šåŸºäºè‡ªé€‚åº”åŒè¿‡ç¨‹æ¨ç†çš„ç›®æ ‡å¯¼èˆª<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09972.html)**|Yiming Gan Team|[2602.09972](http://arxiv.org/abs/2602.09972)|[HJFY](https://hjfy.top/arxiv/2602.09972v1)|
|**2026-02-10**|**AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild<br>AutoFlyï¼šé¢å‘é‡å¤–æ— äººæœºè‡ªä¸»å¯¼èˆªçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09657.html)**|Hui Xiong Team|[2602.09657](http://arxiv.org/abs/2602.09657)|[HJFY](https://hjfy.top/arxiv/2602.09657v1)|
|**2026-02-09**|**When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning<br>ä½•æ—¶æƒ³è±¡ä¸æƒ³è±¡å¤šå°‘ï¼šåŸºäºä¸–ç•Œæ¨¡å‹çš„è‡ªé€‚åº”æµ‹è¯•æ—¶ç¼©æ”¾ç”¨äºè§†è§‰ç©ºé—´æ¨ç†<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.08236.html)**|Mohit Bansal Team|[2602.08236](http://arxiv.org/abs/2602.08236)|[HJFY](https://hjfy.top/arxiv/2602.08236v1)|
|**2026-02-10**|**LCLA: Language-Conditioned Latent Alignment for Vision-Language Navigation<br>LCLAï¼šé¢å‘è§†è§‰è¯­è¨€å¯¼èˆªçš„è¯­è¨€æ¡ä»¶åŒ–æ½œåœ¨å¯¹é½æ¡†æ¶<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.07629.html)**|Soumik Sarkar Team|[2602.07629](http://arxiv.org/abs/2602.07629)|[HJFY](https://hjfy.top/arxiv/2602.07629v2)|
|**2026-02-06**|**Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters<br>å¼¥åˆå®¤å†…å¤–é¸¿æ²Ÿï¼šé¢å‘æœ€åå‡ ç±³çš„è§†è§‰ä¸­å¿ƒåŒ–æŒ‡ä»¤å¼•å¯¼å…·èº«å¯¼èˆª<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.06427.html)**|Mu Xu Team|[2602.06427](http://arxiv.org/abs/2602.06427)|[HJFY](https://hjfy.top/arxiv/2602.06427v1)|
|**2026-02-06**|**Nipping the Drift in the Bud: Retrospective Rectification for Robust Vision-Language Navigation<br>é˜²å¾®æœæ¸ï¼šåŸºäºå›æº¯ä¿®æ­£çš„é²æ£’è§†è§‰è¯­è¨€å¯¼èˆª<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.06356.html)**|Weiying Xie Team|[2602.06356](http://arxiv.org/abs/2602.06356)|[HJFY](https://hjfy.top/arxiv/2602.06356v1)|
|**2026-02-05**|**Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation<br>ç¨€ç–è§†é¢‘ç”Ÿæˆæ¨åŠ¨ç°å®ä¸–ç•Œè¶…è§†è·è§†è§‰è¯­è¨€å¯¼èˆª<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.05827.html)**|Hongyang Li Team|[2602.05827](http://arxiv.org/abs/2602.05827)|[HJFY](https://hjfy.top/arxiv/2602.05827v1)|
|**2026-02-05**|**Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation<br>ä»–è€…ä¸­å¿ƒæ„ŸçŸ¥å™¨ï¼šé€šè¿‡æ¡†æ¶å®ä¾‹åŒ–ä»ä»–è€…è§†è§‰å…ˆéªŒä¸­è§£è€¦ä»–è€…ä¸­å¿ƒæ¨ç†<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.05789.html)**|Weiming Zhang Team|[2602.05789](http://arxiv.org/abs/2602.05789)|[HJFY](https://hjfy.top/arxiv/2602.05789v1)|
|**2026-02-05**|**MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation<br>MerNavï¼šä¸€ç§é«˜åº¦å¯æ³›åŒ–çš„è®°å¿†-æ‰§è¡Œ-å›é¡¾æ¡†æ¶ï¼Œç”¨äºé›¶æ ·æœ¬ç›®æ ‡å¯¼èˆª<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.05467.html)**|Mu Xu Team|[2602.05467](http://arxiv.org/abs/2602.05467)|[HJFY](https://hjfy.top/arxiv/2602.05467v1)|
|**2026-02-02**|**LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation<br>LangMapï¼šé¢å‘å¼€æ”¾è¯æ±‡ç›®æ ‡å¯¼èˆªçš„åˆ†å±‚åŸºå‡†<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02220.html)**|Anton van den Hengel Team|[2602.02220](http://arxiv.org/abs/2602.02220)|[HJFY](https://hjfy.top/arxiv/2602.02220v1)|
|**2026-01-31**|**APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation<br>APEXï¼šä¸€ç§ç”¨äºå¼‚æ­¥ç©ºä¸­ç›®æ ‡å¯¼èˆªçš„è§£è€¦è®°å¿†å‹æ¢ç´¢å™¨<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.00551.html)**|Shuo Yang Team|[2602.00551](http://arxiv.org/abs/2602.00551)|[HJFY](https://hjfy.top/arxiv/2602.00551v1)|
|**2026-02-03**|**MapDream: Task-Driven Map Learning for Vision-Language Navigation<br>MapDreamï¼šé¢å‘è§†è§‰è¯­è¨€å¯¼èˆªçš„ä»»åŠ¡é©±åŠ¨åœ°å›¾å­¦ä¹ <br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.00222.html)**|Zhaoxin Fan Team|[2602.00222](http://arxiv.org/abs/2602.00222)|[HJFY](https://hjfy.top/arxiv/2602.00222v2)|
|**2026-01-29**|**Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation<br>åŠ¨æ€æ‹“æ‰‘æ„ŸçŸ¥ï¼šæ‰“ç ´è§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„ç²’åº¦åƒµåŒ–<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2601.21751.html)**|Xiaoming Wang Team|[2601.21751](http://arxiv.org/abs/2601.21751)|[HJFY](https://hjfy.top/arxiv/2601.21751v1)|
|**2026-01-26**|**DV-VLN: Dual Verification for Reliable LLM-Based Vision-and-Language Navigation<br>DV-VLNï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ä¸è¯­è¨€å¯¼èˆªåŒé‡éªŒè¯å¯é æ¡†æ¶<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2601.18492.html)**|Shoujun Zhou Team|[2601.18492](http://arxiv.org/abs/2601.18492)|[HJFY](https://hjfy.top/arxiv/2601.18492v1)|
|**2026-01-26**|**\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation<br>NaVIDAï¼šåŸºäºé€†åŠ¨åŠ›å­¦å¢å¼ºçš„è§†è§‰è¯­è¨€å¯¼èˆª<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2601.18188.html)**|Feng Zheng Team|[2601.18188](http://arxiv.org/abs/2601.18188)|[HJFY](https://hjfy.top/arxiv/2601.18188v1)|
|**2026-01-22**|**AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning<br>AIONï¼šåŸºäºåŒç­–ç•¥å¼ºåŒ–å­¦ä¹ çš„ç©ºä¸­å®¤å†…ç›®æ ‡å¯¼èˆªç³»ç»Ÿ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2601.15614.html)**|Lin Zhao Team|[2601.15614](http://arxiv.org/abs/2601.15614)|[HJFY](https://hjfy.top/arxiv/2601.15614v1)|
|**2026-01-23**|**FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation<br>FantasyVLNï¼šé¢å‘è§†è§‰è¯­è¨€å¯¼èˆªçš„ç»Ÿä¸€å¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†æ¡†æ¶<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2601.13976.html)**|Yonggang Qi Team|[2601.13976](http://arxiv.org/abs/2601.13976)|[HJFY](https://hjfy.top/arxiv/2601.13976v2)|
|**2026-01-19**|**Spatial-VLN: Zero-Shot Vision-and-Language Navigation With Explicit Spatial Perception and Exploration<br>Spatial-VLNï¼šå…·å¤‡æ˜¾å¼ç©ºé—´æ„ŸçŸ¥ä¸æ¢ç´¢èƒ½åŠ›çš„é›¶æ ·æœ¬è§†è§‰è¯­è¨€å¯¼èˆª<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2601.12766.html)**|Feitian Zhang Team|[2601.12766](http://arxiv.org/abs/2601.12766)|[HJFY](https://hjfy.top/arxiv/2601.12766v1)|
|**2026-01-14**|**Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning<br>è¿ˆå‘å¼€æ”¾ç¯å¢ƒä¸æŒ‡ä»¤ï¼šåŸºäºå¿«æ…¢äº¤äº’æ¨ç†çš„é€šç”¨è§†è§‰è¯­è¨€å¯¼èˆª<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2601.09111.html)**|Yahong Han Team|[2601.09111](http://arxiv.org/abs/2601.09111)|[HJFY](https://hjfy.top/arxiv/2601.09111v1)|
|**2026-01-11**|**Residual Cross-Modal Fusion Networks for Audio-Visual Navigation**|Yi Wang et.al.|[2601.08868](http://arxiv.org/abs/2601.08868)|
|**2026-01-13**|**VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory**|Shaoan Wang et.al.|[2601.08665](http://arxiv.org/abs/2601.08665)|
|**2026-01-12**|**GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap**|Farzad Shami et.al.|[2601.07375](http://arxiv.org/abs/2601.07375)|

<p align=right>(<a href=#updated-on-20260222>back to top</a>)</p>

### ğŸ“Œ VLM

|Publish Date (YYYY-MM-DD)|Title|Authors|PDF|HJFY|
|---|---|---|---|---|
|**2026-02-19**|**Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting<br>é€šè¿‡ç»†ç²’åº¦ç»†èŠ‚å®šä½æ¨åŠ¨é»‘ç›’å¤§è§†è§‰è¯­è¨€æ¨¡å‹æ”»å‡»å‰æ²¿<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17645.html)**|Zhiqiang Shen Team|[2602.17645](http://arxiv.org/abs/2602.17645)|[HJFY](https://hjfy.top/arxiv/2602.17645v1)|
|**2026-02-19**|**Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning<br>æŠ—ç¾éš¾æ€§é—å¿˜çš„å•æ¬¡å¢é‡è”é‚¦å­¦ä¹ <br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17625.html)**|Monowar Bhuyan Team|[2602.17625](http://arxiv.org/abs/2602.17625)|[HJFY](https://hjfy.top/arxiv/2602.17625v1)|
|**2026-02-19**|**AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games<br>AIæ¸¸æˆå•†åº—ï¼šé€šè¿‡äººç±»æ¸¸æˆå®ç°æœºå™¨é€šç”¨æ™ºèƒ½çš„å¯æ‰©å±•ã€å¼€æ”¾å¼è¯„ä¼°<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17594.html)**|Joshua B. Tenenbaum Team|[2602.17594](http://arxiv.org/abs/2602.17594)|[HJFY](https://hjfy.top/arxiv/2602.17594v1)|
|**2026-02-19**|**RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward<br>RetouchIQï¼šåŸºäºæŒ‡ä»¤çš„å›¾åƒä¿®é¥°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ä¸é€šç”¨å¥–åŠ±æœºåˆ¶<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17558.html)**|Handong Zhao Team|[2602.17558](http://arxiv.org/abs/2602.17558)|[HJFY](https://hjfy.top/arxiv/2602.17558v1)|
|**2026-02-19**|**GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking<br>GraphThinkerï¼šé€šè¿‡äº‹ä»¶å›¾æ€ç»´å¼ºåŒ–è§†é¢‘æ¨ç†<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17555.html)**|Shaogang Gong Team|[2602.17555](http://arxiv.org/abs/2602.17555)|[HJFY](https://hjfy.top/arxiv/2602.17555v1)|
|**2026-02-19**|**LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs<br>LATAï¼šé¢å‘åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ç½®ä¿¡åº¦æ ¡å‡†çš„æ‹‰æ™®æ‹‰æ–¯è¾…åŠ©è½¬å¯¼è‡ªé€‚åº”æ–¹æ³•<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17535.html)**|Zongyuan Ge Team|[2602.17535](http://arxiv.org/abs/2602.17535)|[HJFY](https://hjfy.top/arxiv/2602.17535v1)|
|**2026-02-19**|**QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery<br>QuPAINTï¼šé¢å‘é‡å­ææ–™å‘ç°çš„ç‰©ç†æ„ŸçŸ¥æŒ‡ä»¤è°ƒä¼˜æ–¹æ³•<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17478.html)**|Khoa Luu Team|[2602.17478](http://arxiv.org/abs/2602.17478)|[HJFY](https://hjfy.top/arxiv/2602.17478v1)|
|**2026-02-19**|**EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models<br>EAGLEï¼šé¢å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å…è°ƒä¼˜å·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„ä¸“å®¶å¢å¼ºæ³¨æ„åŠ›å¼•å¯¼æ–¹æ³•<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17419.html)**|Seon Han Choi Team|[2602.17419](http://arxiv.org/abs/2602.17419)|[HJFY](https://hjfy.top/arxiv/2602.17419v1)|
|**2026-02-19**|**EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models<br>EntropyPruneï¼šåŸºäºçŸ©é˜µç†µå¼•å¯¼çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è§†è§‰ä»¤ç‰Œå‰ªæ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17196.html)**|Lianghua He Team|[2602.17196](http://arxiv.org/abs/2602.17196)|[HJFY](https://hjfy.top/arxiv/2602.17196v1)|
|**2026-02-19**|**Selective Training for Large Vision Language Models via Visual Information Gain<br>åŸºäºè§†è§‰ä¿¡æ¯å¢ç›Šçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹é€‰æ‹©æ€§è®­ç»ƒ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.17186.html)**|Sangheum Hwang Team|[2602.17186](http://arxiv.org/abs/2602.17186)|[HJFY](https://hjfy.top/arxiv/2602.17186v1)|
|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment<br>æ‰©å±•éªŒè¯åœ¨è§†è§‰-è¯­è¨€-åŠ¨ä½œå¯¹é½ä¸­æ¯”æ‰©å±•ç­–ç•¥å­¦ä¹ æ›´æœ‰æ•ˆ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.12281.html)**|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|[HJFY](https://hjfy.top/arxiv/2602.12281v1)|
|**2026-02-12**|**ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images<br>ExStrucTinyï¼šé¢å‘æ–‡æ¡£å›¾åƒä¸­æ¨¡å¼å¯å˜ç»“æ„åŒ–ä¿¡æ¯æå–çš„åŸºå‡†æ•°æ®é›†<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.12203.html)**|Manuela Veloso Team|[2602.12203](http://arxiv.org/abs/2602.12203)|[HJFY](https://hjfy.top/arxiv/2602.12203v1)|
|**2026-02-12**|**Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education<br>è§†è§‰æ¨ç†åŸºå‡†ï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŸºç¡€æ•™è‚²è¯¾å ‚çœŸå®è§†è§‰é—®é¢˜ä¸Šçš„è¡¨ç°<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.12196.html)**|Oliver G. B. Garrod Team|[2602.12196](http://arxiv.org/abs/2602.12196)|[HJFY](https://hjfy.top/arxiv/2602.12196v1)|
|**2026-02-12**|**3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting<br>3DGSNavï¼šé€šè¿‡ä¸»åŠ¨3Dé«˜æ–¯æ³¼æº…å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç‰©ä½“å¯¼èˆªä¸­çš„æ¨ç†èƒ½åŠ›<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.12159.html)**|Xinyi Yu Team|[2602.12159](http://arxiv.org/abs/2602.12159)|[HJFY](https://hjfy.top/arxiv/2602.12159v1)|
|**2026-02-12**|**DeepSight: An All-in-One LM Safety Toolkit<br>DeepSightï¼šä¸€ä½“åŒ–å¤§å‹æ¨¡å‹å®‰å…¨å·¥å…·ç®±<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.12092.html)**|Xia Hu Team|[2602.12092](http://arxiv.org/abs/2602.12092)|[HJFY](https://hjfy.top/arxiv/2602.12092v1)|
|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning<br>å¯ä¾›æ€§å›¾åŒ–ä»»åŠ¡ä¸–ç•Œï¼šé¢å‘å¯æ‰©å±•å…·èº«å­¦ä¹ çš„è‡ªæ¼”åŒ–ä»»åŠ¡ç”Ÿæˆ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.12065.html)**|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|[HJFY](https://hjfy.top/arxiv/2602.12065v1)|
|**2026-02-12**|**Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation<br>æœ¬åœ°è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¦è¶…è¶Šè§†è§‰Transformeræå‡æ´»åŠ¨è¯†åˆ«èƒ½åŠ›ï¼Ÿâ€”â€”ä»¥æ–°ç”Ÿå„¿å¤è‹ä¸ºä¾‹çš„ç ”ç©¶<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.12002.html)**|Ã˜yvind Meinich-Bache Team|[2602.12002](http://arxiv.org/abs/2602.12002)|[HJFY](https://hjfy.top/arxiv/2602.12002v1)|
|**2026-02-12**|**Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation<br>ç©ºé—´æ€ç»´é“¾ï¼šè¿æ¥ç†è§£ä¸ç”Ÿæˆæ¨¡å‹ä»¥å®ç°ç©ºé—´æ¨ç†ç”Ÿæˆ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.11980.html)**|Long Chen Team|[2602.11980](http://arxiv.org/abs/2602.11980)|[HJFY](https://hjfy.top/arxiv/2602.11980v1)|
|**2026-02-12**|**Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion<br>è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ³•è¯­PDFè½¬Markdownä»»åŠ¡ä¸­çš„æ€§èƒ½åŸºå‡†<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.11960.html)**|Nicolas Mery Team|[2602.11960](http://arxiv.org/abs/2602.11960)|[HJFY](https://hjfy.top/arxiv/2602.11960v1)|
|**2026-02-12**|**Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization<br>åŒLLMæ˜¯å¦ä¼˜äºå•ä¸€æ¨¡å‹ï¼Ÿä¸€ç§ç”¨äºåŒ»è¯å†…å®¹ä¼˜åŒ–çš„å¸ˆç”ŸåŒå¤´LLMæ¶æ„<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.11957.html)**|Anubhav Girdhar Team|[2602.11957](http://arxiv.org/abs/2602.11957)|[HJFY](https://hjfy.top/arxiv/2602.11957v1)|
|**2026-02-10**|**Reason-IAD: Knowledge-Guided Dynamic Latent Reasoning for Explainable Industrial Anomaly Detection<br>Reason-IADï¼šé¢å‘å¯è§£é‡Šå·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„çŸ¥è¯†å¼•å¯¼åŠ¨æ€æ½œåœ¨æ¨ç†æ¡†æ¶<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09850.html)**|Xiaochun Cao Team|[2602.09850](http://arxiv.org/abs/2602.09850)|[HJFY](https://hjfy.top/arxiv/2602.09850v1)|
|**2026-02-10**|**Kelix Technique Report<br>KelixæŠ€æœ¯æŠ¥å‘Š<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09843.html)**|Ziqi Wang Team|[2602.09843](http://arxiv.org/abs/2602.09843)|[HJFY](https://hjfy.top/arxiv/2602.09843v1)|
|**2026-02-10**|**SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding<br>SAKEDï¼šé€šè¿‡ç¨³å®šæ€§æ„ŸçŸ¥çš„çŸ¥è¯†å¢å¼ºè§£ç ç¼“è§£å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09825.html)**|Xudong Jiang Team|[2602.09825](http://arxiv.org/abs/2602.09825)|[HJFY](https://hjfy.top/arxiv/2602.09825v1)|
|**2026-02-10**|**GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation<br>GenSeg-R1ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„è§†è§‰è¯­è¨€ç»†ç²’åº¦æŒ‡ä»£åˆ†å‰²<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09701.html)**|Uma Mahesh Team|[2602.09701](http://arxiv.org/abs/2602.09701)|[HJFY](https://hjfy.top/arxiv/2602.09701v1)|
|**2026-02-10**|**VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model<br>VideoAffordï¼šåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»äºº-ç‰©äº¤äº’è§†é¢‘ä¸­å®ç°ä¸‰ç»´åŠŸèƒ½å¯åŠæ€§æ¥åœ°<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09638.html)**|Hui Xiong Team|[2602.09638](http://arxiv.org/abs/2602.09638)|[HJFY](https://hjfy.top/arxiv/2602.09638v1)|
|**2026-02-10**|**AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models<br>AGMarkï¼šé¢å‘å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ³¨æ„åŠ›å¼•å¯¼åŠ¨æ€æ°´å°æŠ€æœ¯<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09611.html)**|Linlin Wang Team|[2602.09611](http://arxiv.org/abs/2602.09611)|[HJFY](https://hjfy.top/arxiv/2602.09611v1)|
|**2026-02-10**|**Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing<br>Tele-Omniï¼šé¢å‘è§†é¢‘ç”Ÿæˆä¸ç¼–è¾‘çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¡†æ¶<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09609.html)**|Xuelong Li Team|[2602.09609](http://arxiv.org/abs/2602.09609)|[HJFY](https://hjfy.top/arxiv/2602.09609v1)|
|**2026-02-10**|**Delving into Spectral Clustering with Vision-Language Representations<br>æ¢ç´¢åŸºäºè§†è§‰-è¯­è¨€è¡¨å¾çš„å…‰è°±èšç±»æ–¹æ³•<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09586.html)**|Zhen Fang Team|[2602.09586](http://arxiv.org/abs/2602.09586)|[HJFY](https://hjfy.top/arxiv/2602.09586v1)|
|**2026-02-10**|**Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination<br>æ‰‹æœ¯åˆ€ï¼šé€šè¿‡æ··åˆé«˜æ–¯æ¡¥ç²¾ç»†å¯¹é½æ³¨æ„åŠ›æ¿€æ´»æµå½¢ä»¥ç¼“è§£å¤šæ¨¡æ€å¹»è§‰<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09541.html)**|Koichi Shirahata Team|[2602.09541](http://arxiv.org/abs/2602.09541)|[HJFY](https://hjfy.top/arxiv/2602.09541v1)|
|**2026-02-10**|**DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment<br>DR.Expertsï¼šé¢å‘ç›²å›¾åƒè´¨é‡è¯„ä¼°çš„å¤±çœŸæ„ŸçŸ¥ä¸“å®¶å·®åˆ†ç»†åŒ–æ–¹æ³•<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.09531.html)**|Runze Hu Team|[2602.09531](http://arxiv.org/abs/2602.09531)|[HJFY](https://hjfy.top/arxiv/2602.09531v1)|
|**2026-02-04**|**When LLaVA Meets Objects: Token Composition for Vision-Language-Models<br>å½“LLaVAé‡è§ç‰©ä½“ï¼šè§†è§‰è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œç»„åˆ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04864.html)**|Hilde Kuehne Team|[2602.04864](http://arxiv.org/abs/2602.04864)|[HJFY](https://hjfy.top/arxiv/2602.04864v1)|
|**2026-02-04**|**El Agente Estructural: An Artificially Intelligent Molecular Editor<br>ç»“æ„æ™ºèƒ½ä½“ï¼šä¸€ç§äººå·¥æ™ºèƒ½åˆ†å­ç¼–è¾‘å™¨<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04849.html)**|Varinia Bernales Team|[2602.04849](http://arxiv.org/abs/2602.04849)|[HJFY](https://hjfy.top/arxiv/2602.04849v1)|
|**2026-02-04**|**VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?<br>VISTA-Benchï¼šè§†è§‰è¯­è¨€æ¨¡å‹çœŸçš„èƒ½åƒç†è§£çº¯æ–‡æœ¬ä¸€æ ·ç†è§£å›¾åƒä¸­çš„æ–‡æœ¬å—ï¼Ÿ<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04802.html)**|Huchuan Lu Team|[2602.04802](http://arxiv.org/abs/2602.04802)|[HJFY](https://hjfy.top/arxiv/2602.04802v1)|
|**2026-02-04**|**Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases<br>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¯¹é½æ¼‚ç§»ï¼šå¯¹å…«ä¸ªæ¨¡å‹ç‰ˆæœ¬æœ‰å®³æ€§çš„ä¸¤é˜¶æ®µçºµå‘è¯„ä¼°<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04739.html)**|Emily Dix Team|[2602.04739](http://arxiv.org/abs/2602.04739)|[HJFY](https://hjfy.top/arxiv/2602.04739v1)|
|**2026-02-04**|**SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation<br>SAR-RAGï¼šé€šè¿‡è¯­ä¹‰æœç´¢ã€æ£€ç´¢ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è‡ªåŠ¨ç›®æ ‡è¯†åˆ«è§†è§‰é—®ç­”<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04712.html)**|Andreas Spanias Team|[2602.04712](http://arxiv.org/abs/2602.04712)|[HJFY](https://hjfy.top/arxiv/2602.04712v1)|
|**2026-02-04**|**Annotation Free Spacecraft Detection and Segmentation using Vision Language Models<br>åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ— æ ‡æ³¨èˆªå¤©å™¨æ£€æµ‹ä¸åˆ†å‰²<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04699.html)**|Djamila Aouada Team|[2602.04699](http://arxiv.org/abs/2602.04699)|[HJFY](https://hjfy.top/arxiv/2602.04699v1)|
|**2026-02-04**|**AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation<br>AGILEï¼šåŸºäºæ™ºèƒ½ä½“ç”Ÿæˆä»è§†é¢‘é‡å»ºæ‰‹-ç‰©äº¤äº’<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04672.html)**|Chunhua Shen Team|[2602.04672](http://arxiv.org/abs/2602.04672)|[HJFY](https://hjfy.top/arxiv/2602.04672v1)|
|**2026-02-04**|**PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective<br>PIO-FVLMï¼šä»æ¨ç†ç›®æ ‡è§†è§’é‡æ–°å®¡è§†ç”¨äºVLMåŠ é€Ÿçš„æ— è®­ç»ƒè§†è§‰ä»¤ç‰Œç¼©å‡<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04657.html)**|Chunhua Shen Team|[2602.04657](http://arxiv.org/abs/2602.04657)|[HJFY](https://hjfy.top/arxiv/2602.04657v1)|
|**2026-02-04**|**Relational Scene Graphs for Object Grounding of Natural Language Commands<br>é¢å‘è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­ç‰©ä½“å®šä½çš„å…³ç³»åœºæ™¯å›¾<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04635.html)**|Ville Kyrki Team|[2602.04635](http://arxiv.org/abs/2602.04635)|[HJFY](https://hjfy.top/arxiv/2602.04635v1)|
|**2026-02-04**|**LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation<br>LEADï¼šé¢å‘å¿ å®æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„å±‚çº§ä¸“å®¶å¯¹é½è§£ç <br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.04617.html)**|Yan Song Team|[2602.04617](http://arxiv.org/abs/2602.04617)|[HJFY](https://hjfy.top/arxiv/2602.04617v1)|
|**2026-02-02**|**Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts<br>Avenir-Webï¼šåŸºäºæ··åˆå®šä½ä¸“å®¶çš„äººç±»ç»éªŒæ¨¡ä»¿å¼å¤šæ¨¡æ€ç½‘ç»œä»£ç†<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02468.html)**|Mengdi Wang Team|[2602.02468](http://arxiv.org/abs/2602.02468)|[HJFY](https://hjfy.top/arxiv/2602.02468v1)|
|**2026-02-02**|**MentisOculi: Revealing the Limits of Reasoning with Mental Imagery<br>MentisOculiï¼šæ­ç¤ºå¿ƒæ™ºæ„è±¡æ¨ç†çš„å±€é™æ€§<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02465.html)**|Wieland Brendel Team|[2602.02465](http://arxiv.org/abs/2602.02465)|[HJFY](https://hjfy.top/arxiv/2602.02465v1)|
|**2026-02-02**|**Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning<br>é¢å‘ä»»åŠ¡æ¨ç†çš„å…³ç³»æ„ŸçŸ¥åˆ†å±‚ä¸‰ç»´åœºæ™¯å›¾<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02456.html)**|Kostas Alexis Team|[2602.02456](http://arxiv.org/abs/2602.02456)|[HJFY](https://hjfy.top/arxiv/2602.02456v1)|
|**2026-02-02**|**World-Gymnast: Training Robots with Reinforcement Learning in a World Model<br>ä¸–ç•Œä½“æ“å®¶ï¼šåœ¨ä¸–ç•Œæ¨¡å‹ä¸­é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœºå™¨äºº<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02454.html)**|Sherry Yang Team|[2602.02454](http://arxiv.org/abs/2602.02454)|[HJFY](https://hjfy.top/arxiv/2602.02454v1)|
|**2026-02-02**|**ReasonEdit: Editing Vision-Language Models using Human Reasoning<br>ReasonEditï¼šåŸºäºäººç±»æ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ç¼–è¾‘<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02408.html)**|Thomas Hartvigsen Team|[2602.02408](http://arxiv.org/abs/2602.02408)|[HJFY](https://hjfy.top/arxiv/2602.02408v1)|
|**2026-02-02**|**LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization<br>LongVPOï¼šä»é”šå®šçº¿ç´¢åˆ°è‡ªæˆ‘æ¨ç†çš„é•¿è§†é¢‘åå¥½ä¼˜åŒ–<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02341.html)**|Limin Wang Team|[2602.02341](http://arxiv.org/abs/2602.02341)|[HJFY](https://hjfy.top/arxiv/2602.02341v1)|
|**2026-02-02**|**Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models<br>Vision-DeepResearchåŸºå‡†ï¼šé‡æ–°æ€è€ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ä¸æ–‡æœ¬æœç´¢èƒ½åŠ›<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02185.html)**|Shaosheng Cao Team|[2602.02185](http://arxiv.org/abs/2602.02185)|[HJFY](https://hjfy.top/arxiv/2602.02185v1)|
|**2026-02-02**|**See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers<br>See2Refineï¼šè§†è§‰-è¯­è¨€åé¦ˆæå‡åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„eHMIè¡Œä¸ºè®¾è®¡èƒ½åŠ›<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02063.html)**|Takeo Igarashi Team|[2602.02063](http://arxiv.org/abs/2602.02063)|[HJFY](https://hjfy.top/arxiv/2602.02063v1)|
|**2026-02-02**|**Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models<br>Auto-Compï¼šé¢å‘å¯¹æ¯”å¼è§†è§‰è¯­è¨€æ¨¡å‹å¯æ‰©å±•ç»„åˆæ€§æ¢æµ‹çš„è‡ªåŠ¨åŒ–æµç¨‹<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02043.html)**|Toshihiko Yamasaki Team|[2602.02043](http://arxiv.org/abs/2602.02043)|[HJFY](https://hjfy.top/arxiv/2602.02043v1)|
|**2026-02-02**|**One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation<br>ä¸€å›¾å¤šé…ï¼šåœ¨å¤§è§„æ¨¡å¹¿å‘Šå›¾åƒç”Ÿæˆä¸­åè°ƒå¤šæ ·åŒ–çš„ç¾¤ä½“ç‚¹å‡»åå¥½<br>[æ‘˜è¦](https://20bytes.github.io/vlm-arxiv-daily/abstracts/2602.02033.html)**|Jian Liang Team|[2602.02033](http://arxiv.org/abs/2602.02033)|[HJFY](https://hjfy.top/arxiv/2602.02033v1)|
|**2026-01-30**|**User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments**|Junfeng Lin et.al.|[2601.23281](http://arxiv.org/abs/2601.23281)|
|**2026-01-30**|**Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models**|Yi Zhang et.al.|[2601.23253](http://arxiv.org/abs/2601.23253)|
|**2026-01-30**|**Structured Over Scale: Learning Spatial Reasoning from Educational Video**|Bishoy Galoaa et.al.|[2601.23251](http://arxiv.org/abs/2601.23251)|
|**2026-01-30**|**Video-o3: Native Interleaved Clue Seeking for Long Video Multi-Hop Reasoning**|Xiangyu Zeng et.al.|[2601.23224](http://arxiv.org/abs/2601.23224)|
|**2026-01-30**|**Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training**|Anglin Liu et.al.|[2601.23220](http://arxiv.org/abs/2601.23220)|
|**2026-01-30**|**Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization**|Hui Lu et.al.|[2601.23179](http://arxiv.org/abs/2601.23179)|
|**2026-01-30**|**Hearing is Believing? Evaluating and Analyzing Audio Language Model Sycophancy with SYAUDIO**|Junchi Yao et.al.|[2601.23149](http://arxiv.org/abs/2601.23149)|
|**2026-01-30**|**One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs**|Youxu Shi et.al.|[2601.23041](http://arxiv.org/abs/2601.23041)|
|**2026-01-30**|**Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models**|Anmin Wang et.al.|[2601.22959](http://arxiv.org/abs/2601.22959)|
|**2026-01-30**|**Alignment among Language, Vision and Action Representations**|Nicola Milano et.al.|[2601.22948](http://arxiv.org/abs/2601.22948)|
|**2026-01-29**|**UEval: A Benchmark for Unified Multimodal Generation**|Bo Li et.al.|[2601.22155](http://arxiv.org/abs/2601.22155)|
|**2026-01-29**|**Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions**|Xiaoxiao Sun et.al.|[2601.22150](http://arxiv.org/abs/2601.22150)|
|**2026-01-29**|**SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence**|Saoud Aldowaish et.al.|[2601.22114](http://arxiv.org/abs/2601.22114)|
|**2026-01-29**|**VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning**|Yibo Wang et.al.|[2601.22069](http://arxiv.org/abs/2601.22069)|
|**2026-01-29**|**Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models**|Wenxuan Huang et.al.|[2601.22060](http://arxiv.org/abs/2601.22060)|
|**2026-01-29**|**MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources**|Baorui Ma et.al.|[2601.22054](http://arxiv.org/abs/2601.22054)|
|**2026-01-29**|**Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning**|Chengyi Cai et.al.|[2601.22020](http://arxiv.org/abs/2601.22020)|
|**2026-01-29**|**Causal World Modeling for Robot Control**|Lin Li et.al.|[2601.21998](http://arxiv.org/abs/2601.21998)|
|**2026-01-29**|**Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models**|Konstantinos P. Panousis et.al.|[2601.21944](http://arxiv.org/abs/2601.21944)|
|**2026-01-29**|**VideoAesBench: Benchmarking the Video Aesthetics Perception Capabilities of Large Multimodal Models**|Yunhao Li et.al.|[2601.21915](http://arxiv.org/abs/2601.21915)|

<p align=right>(<a href=#updated-on-20260222>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/20bytes/vlm-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/20bytes/vlm-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/20bytes/vlm-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/20bytes/vlm-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/20bytes/vlm-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/20bytes/vlm-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/20bytes/vlm-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/20bytes/vlm-arxiv-daily/issues

