<p align="center">
  <h1 align="center">ğŸ¤– VLM-Arxiv-Daily</h1>
  <p align="center">
    <img src="https://img.shields.io/badge/Robotics-VLA-orange?style=for-the-badge&logo=robotframework" alt="Robotics">
    <img src="https://img.shields.io/badge/Navigation-VLN-blue?style=for-the-badge&logo=googlemaps" alt="Navigation">
    <img src="https://img.shields.io/badge/Vision--Language-VLM-green?style=for-the-badge&logo=openai" alt="VLM">
  </p>
</p>

> ğŸš€ æ¯æ—¥è‡ªåŠ¨è¿½è¸ª **Vision-Language-Action (VLA)**, **Vision-Language Navigation (VLN)** å’Œ **Vision-Language Models (VLM)** çš„æœ€æ–° Arxiv è®ºæ–‡ã€‚

## ğŸ“… Updated on 2026.02.03
<details>
  <summary>ç‚¹å‡»æŸ¥çœ‹ç›®å½• (Table of Contents)</summary>
  <ol>
    <li><a href=#vla>VLA</a></li>
    <li><a href=#vln>VLN</a></li>
    <li><a href=#vlm>VLM</a></li>
  </ol>
</details>

### ğŸ“Œ VLA

|Publish Date (YYYY-MM-DD)|Title|Authors|PDF|HJFY|
|---|---|---|---|---|
|**2026-02-02**|**TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments<br>TIC-VLAï¼šä¸€ç§ç”¨äºåŠ¨æ€ç¯å¢ƒä¸­æœºå™¨äººå¯¼èˆªçš„æ€æ§ä¸€ä½“åŒ–è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br>[æ‘˜è¦](abstracts/2602.02459.html)**|Jiaqi Ma Team|[2602.02459](http://arxiv.org/abs/2602.02459)|[HJFY](https://hjfy.top/arxiv/2602.02459v1)|
|**2026-02-02**|**World-Gymnast: Training Robots with Reinforcement Learning in a World Model<br>ä¸–ç•Œä½“æ“å®¶ï¼šåœ¨ä¸–ç•Œæ¨¡å‹ä¸­é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœºå™¨äºº<br>[æ‘˜è¦](abstracts/2602.02454.html)**|Sherry Yang Team|[2602.02454](http://arxiv.org/abs/2602.02454)|[HJFY](https://hjfy.top/arxiv/2602.02454v1)|
|**2026-02-02**|**SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation<br>SoMAï¼šé¢å‘æœºå™¨äººè½¯ä½“æ“ä½œçš„çœŸå®åˆ°ä»¿çœŸç¥ç»æ¨¡æ‹Ÿå™¨<br>[æ‘˜è¦](abstracts/2602.02402.html)**|Jiangmiao Pang Team|[2602.02402](http://arxiv.org/abs/2602.02402)|[HJFY](https://hjfy.top/arxiv/2602.02402v1)|
|**2026-02-02**|**MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models<br>MAIN-VLAï¼šä¸ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å»ºæ¨¡æ„å›¾ä¸ç¯å¢ƒçš„æŠ½è±¡<br>[æ‘˜è¦](abstracts/2602.02212.html)**|Lemiao Qiu Team|[2602.02212](http://arxiv.org/abs/2602.02212)|[HJFY](https://hjfy.top/arxiv/2602.02212v1)|
|**2026-02-02**|**FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation<br>FD-VLAï¼šç”¨äºæ¥è§¦ä¸°å¯Œæ“ä½œçš„åŠ›è’¸é¦è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br>[æ‘˜è¦](abstracts/2602.02142.html)**|Haiyue Zhu Team|[2602.02142](http://arxiv.org/abs/2602.02142)|[HJFY](https://hjfy.top/arxiv/2602.02142v1)|
|**2026-02-02**|**See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers<br>See2Refineï¼šè§†è§‰-è¯­è¨€åé¦ˆæå‡åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„eHMIè¡Œä¸ºè®¾è®¡èƒ½åŠ›<br>[æ‘˜è¦](abstracts/2602.02063.html)**|Takeo Igarashi Team|[2602.02063](http://arxiv.org/abs/2602.02063)|[HJFY](https://hjfy.top/arxiv/2602.02063v1)|
|**2026-02-02**|**Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models<br>é¢å‘è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹æ¨ç†æ—¶å®‰å…¨æ€§çš„æ¦‚å¿µè¯å…¸å­¦ä¹ æ–¹æ³•<br>[æ‘˜è¦](abstracts/2602.01834.html)**|Di Wang Team|[2602.01834](http://arxiv.org/abs/2602.01834)|[HJFY](https://hjfy.top/arxiv/2602.01834v1)|
|**2026-02-02**|**From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models<br>ä»ç²¾ç¡®è®¤çŸ¥åˆ°ç²¾å‡†æ‰§è¡Œï¼šé¢å‘è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹çš„é€šç”¨è‡ªæ ¡æ­£ä¸ç»ˆæ­¢æ¡†æ¶<br>[æ‘˜è¦](abstracts/2602.01811.html)**|Jianzong Wang Team|[2602.01811](http://arxiv.org/abs/2602.01811)|[HJFY](https://hjfy.top/arxiv/2602.01811v1)|
|**2026-02-02**|**AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act<br>AgenticLabï¼šä¸€ä¸ªèƒ½å¤Ÿè§‚å¯Ÿã€æ€è€ƒä¸è¡ŒåŠ¨çš„çœŸå®ä¸–ç•Œæœºå™¨äººæ™ºèƒ½ä½“å¹³å°<br>[æ‘˜è¦](abstracts/2602.01662.html)**|Yu She Team|[2602.01662](http://arxiv.org/abs/2602.01662)|[HJFY](https://hjfy.top/arxiv/2602.01662v1)|
|**2026-02-02**|**From Perception to Action: Spatial AI Agents and World Models<br>ä»æ„ŸçŸ¥åˆ°è¡ŒåŠ¨ï¼šç©ºé—´äººå·¥æ™ºèƒ½ä»£ç†ä¸ä¸–ç•Œæ¨¡å‹<br>[æ‘˜è¦](abstracts/2602.01644.html)**|Esteban Rojas Team|[2602.01644](http://arxiv.org/abs/2602.01644)|[HJFY](https://hjfy.top/arxiv/2602.01644v1)|
|**2026-01-30**|**Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation**|Wu Songwei et.al.|[2601.23087](http://arxiv.org/abs/2601.23087)|
|**2026-01-30**|**EAG-PT: Emission-Aware Gaussians and Path Tracing for Indoor Scene Reconstruction and Editing**|Xijie Yang et.al.|[2601.23065](http://arxiv.org/abs/2601.23065)|
|**2026-01-30**|**Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation**|Di Zhang et.al.|[2601.22988](http://arxiv.org/abs/2601.22988)|
|**2026-01-30**|**Alignment among Language, Vision and Action Representations**|Nicola Milano et.al.|[2601.22948](http://arxiv.org/abs/2601.22948)|
|**2026-01-30**|**When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection**|Shashank Mishra et.al.|[2601.22868](http://arxiv.org/abs/2601.22868)|
|**2026-01-30**|**Vision-Language Models Unlock Task-Centric Latent Actions**|Alexander Nikulin et.al.|[2601.22714](http://arxiv.org/abs/2601.22714)|
|**2026-01-30**|**Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference**|Emilien BirÃ© et.al.|[2601.22701](http://arxiv.org/abs/2601.22701)|
|**2026-01-30**|**CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control**|Jiaqi Shi et.al.|[2601.22467](http://arxiv.org/abs/2601.22467)|
|**2026-01-29**|**PoSafeNet: Safe Learning with Poset-Structured Neural Nets**|Kiwan Wong et.al.|[2601.22356](http://arxiv.org/abs/2601.22356)|
|**2026-01-29**|**DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation**|Haozhe Xie et.al.|[2601.22153](http://arxiv.org/abs/2601.22153)|
|**2026-01-29**|**PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction**|Changjian Jiang et.al.|[2601.22046](http://arxiv.org/abs/2601.22046)|
|**2026-01-29**|**PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy**|Jinhao Zhang et.al.|[2601.22018](http://arxiv.org/abs/2601.22018)|
|**2026-01-29**|**Causal World Modeling for Robot Control**|Lin Li et.al.|[2601.21998](http://arxiv.org/abs/2601.21998)|
|**2026-01-29**|**MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts**|Lorenzo Mazza et.al.|[2601.21971](http://arxiv.org/abs/2601.21971)|
|**2026-01-29**|**Information Filtering via Variational Regularization for Robot Manipulation**|Jinhao Zhang et.al.|[2601.21926](http://arxiv.org/abs/2601.21926)|
|**2026-01-29**|**Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation**|Jiankun Peng et.al.|[2601.21751](http://arxiv.org/abs/2601.21751)|
|**2026-01-29**|**CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation**|Xuanran Zhai et.al.|[2601.21712](http://arxiv.org/abs/2601.21712)|
|**2026-01-29**|**AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation**|Jianli Sun et.al.|[2601.21602](http://arxiv.org/abs/2601.21602)|
|**2026-01-29**|**EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots**|Zixing Lei et.al.|[2601.21570](http://arxiv.org/abs/2601.21570)|

<p align=right>(<a href=#updated-on-20260203>back to top</a>)</p>

### ğŸ“Œ VLN

|Publish Date (YYYY-MM-DD)|Title|Authors|PDF|HJFY|
|---|---|---|---|---|
|**2026-02-02**|**LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation<br>LangMapï¼šé¢å‘å¼€æ”¾è¯æ±‡ç›®æ ‡å¯¼èˆªçš„åˆ†å±‚åŸºå‡†<br>[æ‘˜è¦](abstracts/2602.02220.html)**|Anton van den Hengel Team|[2602.02220](http://arxiv.org/abs/2602.02220)|[HJFY](https://hjfy.top/arxiv/2602.02220v1)|
|**2026-01-31**|**APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation<br>APEXï¼šä¸€ç§ç”¨äºå¼‚æ­¥ç©ºä¸­ç›®æ ‡å¯¼èˆªçš„è§£è€¦è®°å¿†å‹æ¢ç´¢å™¨<br>[æ‘˜è¦](abstracts/2602.00551.html)**|Shuo Yang Team|[2602.00551](http://arxiv.org/abs/2602.00551)|[HJFY](https://hjfy.top/arxiv/2602.00551v1)|
|**2026-01-30**|**MapDream: Task-Driven Map Learning for Vision-Language Navigation<br>MapDreamï¼šé¢å‘è§†è§‰è¯­è¨€å¯¼èˆªçš„ä»»åŠ¡é©±åŠ¨åœ°å›¾å­¦ä¹ <br>[æ‘˜è¦](abstracts/2602.00222.html)**|Zhaoxin Fan Team|[2602.00222](http://arxiv.org/abs/2602.00222)|[HJFY](https://hjfy.top/arxiv/2602.00222v1)|
|**2026-01-29**|**Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation<br>åŠ¨æ€æ‹“æ‰‘æ„ŸçŸ¥ï¼šæ‰“ç ´è§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„ç²’åº¦åƒµåŒ–<br>[æ‘˜è¦](abstracts/2601.21751.html)**|Xiaoming Wang Team|[2601.21751](http://arxiv.org/abs/2601.21751)|[HJFY](https://hjfy.top/arxiv/2601.21751v1)|
|**2026-01-26**|**DV-VLN: Dual Verification for Reliable LLM-Based Vision-and-Language Navigation<br>DV-VLNï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ä¸è¯­è¨€å¯¼èˆªåŒé‡éªŒè¯å¯é æ¡†æ¶<br>[æ‘˜è¦](abstracts/2601.18492.html)**|Shoujun Zhou Team|[2601.18492](http://arxiv.org/abs/2601.18492)|[HJFY](https://hjfy.top/arxiv/2601.18492v1)|
|**2026-01-26**|**\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation<br>NaVIDAï¼šåŸºäºé€†åŠ¨åŠ›å­¦å¢å¼ºçš„è§†è§‰è¯­è¨€å¯¼èˆª<br>[æ‘˜è¦](abstracts/2601.18188.html)**|Feng Zheng Team|[2601.18188](http://arxiv.org/abs/2601.18188)|[HJFY](https://hjfy.top/arxiv/2601.18188v1)|
|**2026-01-22**|**AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning<br>AIONï¼šåŸºäºåŒç­–ç•¥å¼ºåŒ–å­¦ä¹ çš„ç©ºä¸­å®¤å†…ç›®æ ‡å¯¼èˆªç³»ç»Ÿ<br>[æ‘˜è¦](abstracts/2601.15614.html)**|Lin Zhao Team|[2601.15614](http://arxiv.org/abs/2601.15614)|[HJFY](https://hjfy.top/arxiv/2601.15614v1)|
|**2026-01-23**|**FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation<br>FantasyVLNï¼šé¢å‘è§†è§‰è¯­è¨€å¯¼èˆªçš„ç»Ÿä¸€å¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†æ¡†æ¶<br>[æ‘˜è¦](abstracts/2601.13976.html)**|Yonggang Qi Team|[2601.13976](http://arxiv.org/abs/2601.13976)|[HJFY](https://hjfy.top/arxiv/2601.13976v2)|
|**2026-01-19**|**Spatial-VLN: Zero-Shot Vision-and-Language Navigation With Explicit Spatial Perception and Exploration<br>Spatial-VLNï¼šå…·å¤‡æ˜¾å¼ç©ºé—´æ„ŸçŸ¥ä¸æ¢ç´¢èƒ½åŠ›çš„é›¶æ ·æœ¬è§†è§‰è¯­è¨€å¯¼èˆª<br>[æ‘˜è¦](abstracts/2601.12766.html)**|Feitian Zhang Team|[2601.12766](http://arxiv.org/abs/2601.12766)|[HJFY](https://hjfy.top/arxiv/2601.12766v1)|
|**2026-01-14**|**Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning<br>è¿ˆå‘å¼€æ”¾ç¯å¢ƒä¸æŒ‡ä»¤ï¼šåŸºäºå¿«æ…¢äº¤äº’æ¨ç†çš„é€šç”¨è§†è§‰è¯­è¨€å¯¼èˆª<br>[æ‘˜è¦](abstracts/2601.09111.html)**|Yahong Han Team|[2601.09111](http://arxiv.org/abs/2601.09111)|[HJFY](https://hjfy.top/arxiv/2601.09111v1)|
|**2026-01-11**|**Residual Cross-Modal Fusion Networks for Audio-Visual Navigation**|Yi Wang et.al.|[2601.08868](http://arxiv.org/abs/2601.08868)|
|**2026-01-13**|**VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory**|Shaoan Wang et.al.|[2601.08665](http://arxiv.org/abs/2601.08665)|
|**2026-01-12**|**GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap**|Farzad Shami et.al.|[2601.07375](http://arxiv.org/abs/2601.07375)|

<p align=right>(<a href=#updated-on-20260203>back to top</a>)</p>

### ğŸ“Œ VLM

|Publish Date (YYYY-MM-DD)|Title|Authors|PDF|HJFY|
|---|---|---|---|---|
|**2026-02-02**|**Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts<br>Avenir-Webï¼šåŸºäºæ··åˆå®šä½ä¸“å®¶çš„äººç±»ç»éªŒæ¨¡ä»¿å¼å¤šæ¨¡æ€ç½‘ç»œä»£ç†<br>[æ‘˜è¦](abstracts/2602.02468.html)**|Mengdi Wang Team|[2602.02468](http://arxiv.org/abs/2602.02468)|[HJFY](https://hjfy.top/arxiv/2602.02468v1)|
|**2026-02-02**|**MentisOculi: Revealing the Limits of Reasoning with Mental Imagery<br>MentisOculiï¼šæ­ç¤ºå¿ƒæ™ºæ„è±¡æ¨ç†çš„å±€é™æ€§<br>[æ‘˜è¦](abstracts/2602.02465.html)**|Wieland Brendel Team|[2602.02465](http://arxiv.org/abs/2602.02465)|[HJFY](https://hjfy.top/arxiv/2602.02465v1)|
|**2026-02-02**|**Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning<br>é¢å‘ä»»åŠ¡æ¨ç†çš„å…³ç³»æ„ŸçŸ¥åˆ†å±‚ä¸‰ç»´åœºæ™¯å›¾<br>[æ‘˜è¦](abstracts/2602.02456.html)**|Kostas Alexis Team|[2602.02456](http://arxiv.org/abs/2602.02456)|[HJFY](https://hjfy.top/arxiv/2602.02456v1)|
|**2026-02-02**|**World-Gymnast: Training Robots with Reinforcement Learning in a World Model<br>ä¸–ç•Œä½“æ“å®¶ï¼šåœ¨ä¸–ç•Œæ¨¡å‹ä¸­é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœºå™¨äºº<br>[æ‘˜è¦](abstracts/2602.02454.html)**|Sherry Yang Team|[2602.02454](http://arxiv.org/abs/2602.02454)|[HJFY](https://hjfy.top/arxiv/2602.02454v1)|
|**2026-02-02**|**ReasonEdit: Editing Vision-Language Models using Human Reasoning<br>ReasonEditï¼šåŸºäºäººç±»æ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ç¼–è¾‘<br>[æ‘˜è¦](abstracts/2602.02408.html)**|Thomas Hartvigsen Team|[2602.02408](http://arxiv.org/abs/2602.02408)|[HJFY](https://hjfy.top/arxiv/2602.02408v1)|
|**2026-02-02**|**LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization<br>LongVPOï¼šä»é”šå®šçº¿ç´¢åˆ°è‡ªæˆ‘æ¨ç†çš„é•¿è§†é¢‘åå¥½ä¼˜åŒ–<br>[æ‘˜è¦](abstracts/2602.02341.html)**|Limin Wang Team|[2602.02341](http://arxiv.org/abs/2602.02341)|[HJFY](https://hjfy.top/arxiv/2602.02341v1)|
|**2026-02-02**|**Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models<br>Vision-DeepResearchåŸºå‡†ï¼šé‡æ–°æ€è€ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ä¸æ–‡æœ¬æœç´¢èƒ½åŠ›<br>[æ‘˜è¦](abstracts/2602.02185.html)**|Shaosheng Cao Team|[2602.02185](http://arxiv.org/abs/2602.02185)|[HJFY](https://hjfy.top/arxiv/2602.02185v1)|
|**2026-02-02**|**See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers<br>See2Refineï¼šè§†è§‰-è¯­è¨€åé¦ˆæå‡åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„eHMIè¡Œä¸ºè®¾è®¡èƒ½åŠ›<br>[æ‘˜è¦](abstracts/2602.02063.html)**|Takeo Igarashi Team|[2602.02063](http://arxiv.org/abs/2602.02063)|[HJFY](https://hjfy.top/arxiv/2602.02063v1)|
|**2026-02-02**|**Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models<br>Auto-Compï¼šé¢å‘å¯¹æ¯”å¼è§†è§‰è¯­è¨€æ¨¡å‹å¯æ‰©å±•ç»„åˆæ€§æ¢æµ‹çš„è‡ªåŠ¨åŒ–æµç¨‹<br>[æ‘˜è¦](abstracts/2602.02043.html)**|Toshihiko Yamasaki Team|[2602.02043](http://arxiv.org/abs/2602.02043)|[HJFY](https://hjfy.top/arxiv/2602.02043v1)|
|**2026-02-02**|**One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation<br>ä¸€å›¾å¤šé…ï¼šåœ¨å¤§è§„æ¨¡å¹¿å‘Šå›¾åƒç”Ÿæˆä¸­åè°ƒå¤šæ ·åŒ–çš„ç¾¤ä½“ç‚¹å‡»åå¥½<br>[æ‘˜è¦](abstracts/2602.02033.html)**|Jian Liang Team|[2602.02033](http://arxiv.org/abs/2602.02033)|[HJFY](https://hjfy.top/arxiv/2602.02033v1)|
|**2026-01-30**|**User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments**|Junfeng Lin et.al.|[2601.23281](http://arxiv.org/abs/2601.23281)|
|**2026-01-30**|**Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models**|Yi Zhang et.al.|[2601.23253](http://arxiv.org/abs/2601.23253)|
|**2026-01-30**|**Structured Over Scale: Learning Spatial Reasoning from Educational Video**|Bishoy Galoaa et.al.|[2601.23251](http://arxiv.org/abs/2601.23251)|
|**2026-01-30**|**Video-o3: Native Interleaved Clue Seeking for Long Video Multi-Hop Reasoning**|Xiangyu Zeng et.al.|[2601.23224](http://arxiv.org/abs/2601.23224)|
|**2026-01-30**|**Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training**|Anglin Liu et.al.|[2601.23220](http://arxiv.org/abs/2601.23220)|
|**2026-01-30**|**Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization**|Hui Lu et.al.|[2601.23179](http://arxiv.org/abs/2601.23179)|
|**2026-01-30**|**Hearing is Believing? Evaluating and Analyzing Audio Language Model Sycophancy with SYAUDIO**|Junchi Yao et.al.|[2601.23149](http://arxiv.org/abs/2601.23149)|
|**2026-01-30**|**One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs**|Youxu Shi et.al.|[2601.23041](http://arxiv.org/abs/2601.23041)|
|**2026-01-30**|**Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models**|Anmin Wang et.al.|[2601.22959](http://arxiv.org/abs/2601.22959)|
|**2026-01-30**|**Alignment among Language, Vision and Action Representations**|Nicola Milano et.al.|[2601.22948](http://arxiv.org/abs/2601.22948)|
|**2026-01-29**|**UEval: A Benchmark for Unified Multimodal Generation**|Bo Li et.al.|[2601.22155](http://arxiv.org/abs/2601.22155)|
|**2026-01-29**|**Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions**|Xiaoxiao Sun et.al.|[2601.22150](http://arxiv.org/abs/2601.22150)|
|**2026-01-29**|**SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence**|Saoud Aldowaish et.al.|[2601.22114](http://arxiv.org/abs/2601.22114)|
|**2026-01-29**|**VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning**|Yibo Wang et.al.|[2601.22069](http://arxiv.org/abs/2601.22069)|
|**2026-01-29**|**Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models**|Wenxuan Huang et.al.|[2601.22060](http://arxiv.org/abs/2601.22060)|
|**2026-01-29**|**MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources**|Baorui Ma et.al.|[2601.22054](http://arxiv.org/abs/2601.22054)|
|**2026-01-29**|**Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning**|Chengyi Cai et.al.|[2601.22020](http://arxiv.org/abs/2601.22020)|
|**2026-01-29**|**Causal World Modeling for Robot Control**|Lin Li et.al.|[2601.21998](http://arxiv.org/abs/2601.21998)|
|**2026-01-29**|**Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models**|Konstantinos P. Panousis et.al.|[2601.21944](http://arxiv.org/abs/2601.21944)|
|**2026-01-29**|**VideoAesBench: Benchmarking the Video Aesthetics Perception Capabilities of Large Multimodal Models**|Yunhao Li et.al.|[2601.21915](http://arxiv.org/abs/2601.21915)|

<p align=right>(<a href=#updated-on-20260203>back to top</a>)</p>

[contributors-shield]: https://img.shields.io/github/contributors/20bytes/vlm-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/20bytes/vlm-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/20bytes/vlm-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/20bytes/vlm-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/20bytes/vlm-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/20bytes/vlm-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/20bytes/vlm-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/20bytes/vlm-arxiv-daily/issues

