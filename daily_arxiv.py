import os
import re
import json
import arxiv
import yaml
import logging
import argparse
import html
import datetime
import requests

logging.basicConfig(format='[%(asctime)s %(levelname)s] %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)

github_url = "https://api.github.com/search/repositories"
arxiv_url = "http://arxiv.org/"
hjfy_url = "https://hjfy.top/arxiv/"

def load_translation_cache(cache_path: str) -> dict:
    if not cache_path:
        return {}
    if not os.path.exists(cache_path):
        return {}
    try:
        with open(cache_path, "r", encoding="utf-8") as f:
            return json.load(f) or {}
    except Exception as exc:
        logging.warning(f"Failed to load translation cache: {exc}")
        return {}

def save_translation_cache(cache_path: str, cache: dict) -> None:
    if not cache_path:
        return
    os.makedirs(os.path.dirname(cache_path) or ".", exist_ok=True)
    with open(cache_path, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)

def translate_title_and_abstract_deepseek(
    title: str, abstract: str, api_key: str, base_url: str, model: str
) -> dict:
    if not title or not api_key:
        return {}
    url = base_url.rstrip("/") + "/chat/completions"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}",
    }
    payload = {
        "model": model,
        "messages": [
            {
                "role": "system",
                "content": (
                    "Translate the paper title and abstract into Chinese. "
                    "Return only JSON with keys: title_zh, abstract_zh."
                ),
            },
            {
                "role": "user",
                "content": f"Title:\n{title}\n\nAbstract:\n{abstract}",
            },
        ],
        "stream": False,
    }
    try:
        resp = requests.post(url, headers=headers, json=payload, timeout=60)
        resp.raise_for_status()
        data = resp.json()
        content = (data.get("choices", [{}])[0].get("message", {}).get("content", "") or "").strip()
        if not content:
            return {}
        # try to parse JSON from response
        try:
            return json.loads(content)
        except Exception:
            match = re.search(r"\{[\s\S]*\}", content)
            if match:
                try:
                    return json.loads(match.group(0))
                except Exception:
                    return {}
        return {}
    except Exception as exc:
        logging.warning(f"DeepSeek translation failed: {exc}")
        return {}

def load_config(config_file:str) -> dict:
    '''
    config_file: input config file path
    return: a dict of configuration
    '''
    # make filters pretty
    def pretty_filters(**config) -> dict:
        keywords = dict()
        EXCAPE = '\"'
        QUOTA = '' # NO-USE
        OR = ' OR ' # TODO
        def parse_filters(filters:list):
            ret = ''
            for idx in range(0,len(filters)):
                filter = filters[idx]
                if len(filter.split()) > 1:
                    ret += (EXCAPE + filter + EXCAPE)
                else:
                    ret += (QUOTA + filter + QUOTA)
                if idx != len(filters) - 1:
                    ret += OR
            return ret
        for k,v in config['keywords'].items():
            keywords[k] = parse_filters(v['filters'])
        return keywords
    with open(config_file,'r') as f:
        config = yaml.load(f,Loader=yaml.FullLoader)
        config['kv'] = pretty_filters(**config)
        logging.info(f'config = {config}')
    return config

def get_authors(authors, first_author = False, last_author = False):
    output = str()
    if first_author:
        output = authors[0]
    elif last_author:
        output = authors[-1]
    else:
        output = ", ".join(str(author) for author in authors)
    return output
def sort_papers(papers):
    output = dict()
    keys = list(papers.keys())
    keys.sort(reverse=True)
    for key in keys:
        output[key] = papers[key]
    return output
import requests

def get_code_link(qword:str) -> str:
    """
    This short function was auto-generated by ChatGPT.
    I only renamed some params and added some comments.
    @param qword: query string, eg. arxiv ids and paper titles
    @return paper_code in github: string, if not found, return None
    """
    # query = f"arxiv:{arxiv_id}"
    query = f"{qword}"
    params = {
        "q": query,
        "sort": "stars",
        "order": "desc"
    }
    r = requests.get(github_url, params=params)
    results = r.json()
    code_link = None
    if results["total_count"] > 0:
        code_link = results["items"][0]["html_url"]
    return code_link

def get_daily_papers(topic,query="slam", max_results=2, translate_opts=None):
    """
    @param topic: str
    @param query: str
    @return paper_with_code: dict
    """
    # output
    content = dict()
    content_to_web = dict()
    search_engine = arxiv.Search(
        query = query,
        max_results = max_results,
        sort_by = arxiv.SortCriterion.SubmittedDate
    )

    for result in search_engine.results():

        paper_id            = result.get_short_id()
        paper_title         = result.title
        paper_url           = result.entry_id
        paper_abstract      = result.summary.replace("\n"," ")
        paper_authors       = get_authors(result.authors)
        paper_first_author  = get_authors(result.authors,first_author = True)
        paper_last_author   = get_authors(result.authors,last_author = True)
        primary_category    = result.primary_category
        publish_time        = result.published.date()
        update_time         = result.updated.date()
        comments            = result.comment

        logging.info(f"Time = {update_time} title = {paper_title} author = {paper_last_author}")

        # eg: 2108.09112v1 -> 2108.09112
        ver_pos = paper_id.find('v')
        if ver_pos == -1:
            paper_key = paper_id
        else:
            paper_key = paper_id[0:ver_pos]
        paper_url = arxiv_url + 'abs/' + paper_key
        hjfy_link = hjfy_url + paper_id

        # optional title translation
        translated_title = ""
        translated_abstract = ""
        abstract_en = paper_abstract
        if translate_opts is not None:
            cache = translate_opts["cache"]
            cached = cache.get(paper_key)
            if isinstance(cached, str):
                cached = {"title_zh": cached, "abstract_zh": "", "abstract_en": ""}
            if isinstance(cached, dict):
                translated_title = cached.get("title_zh", "")
                translated_abstract = cached.get("abstract_zh", "")
                if cached.get("abstract_en"):
                    abstract_en = cached.get("abstract_en", "")

            need_abstract = translate_opts.get("translate_abstract", False)
            if not translated_title or (need_abstract and not translated_abstract):
                abstract_for_translation = paper_abstract if need_abstract else ""
                translated = translate_title_and_abstract_deepseek(
                    paper_title,
                    abstract_for_translation,
                    translate_opts["api_key"],
                    translate_opts["base_url"],
                    translate_opts["model"],
                )
                if translated:
                    translated_title = translated.get("title_zh", "").strip()
                    translated_abstract = translated.get("abstract_zh", "").strip()
                    cache[paper_key] = {
                        "title_zh": translated_title,
                        "abstract_zh": translated_abstract,
                        "abstract_en": paper_abstract,
                    }
                    translate_opts["dirty"] = True

        def sanitize_cell(s: str) -> str:
            return s.replace("|", "&#124;").strip()

        title_cell = sanitize_cell(paper_title)
        if translated_title:
            title_cell = f"{title_cell}<br>{sanitize_cell(translated_title)}"
        if translated_abstract and translate_opts and translate_opts.get("translate_abstract"):
            abstract_link = f"abstracts/{paper_key}.html"
            title_cell = f"{title_cell}<br>[ÊëòË¶Å]({abstract_link})"

        # Since PapersWithCode API is deprecated, we no longer fetch code links
        # Papers will be listed without code links
        content[paper_key] = "|**{}**|**{}**|{} Team|[{}]({})|[HJFY]({})|\n".format(
               update_time,title_cell,paper_last_author,paper_key,paper_url,hjfy_link)
        content_to_web[paper_key] = "- {}, **{}**, {} Team, Paper: [{}]({}), HJFY: [{}]({})".format(
               update_time,paper_title,paper_last_author,paper_url,paper_url,hjfy_link,hjfy_link)

        # TODO: select useful comments
        comments = None
        if comments != None:
            content_to_web[paper_key] += f", {comments}\n"
        else:
            content_to_web[paper_key] += f"\n"

    data = {topic:content}
    data_web = {topic:content_to_web}
    return data,data_web

def update_paper_links(filename):
    '''
    weekly update paper links in json file
    '''
    def parse_arxiv_string(s):
        parts = s.split("|")
        date = parts[1].strip()
        title = parts[2].strip()
        authors = parts[3].strip()
        arxiv_id = parts[4].strip()
        hjfy_link = parts[5].strip()
        arxiv_id = re.sub(r'v\d+', '', arxiv_id)
        return date,title,authors,arxiv_id,hjfy_link

    with open(filename,"r") as f:
        content = f.read()
        if not content:
            m = {}
        else:
            m = json.loads(content)

        json_data = m.copy()

        for keywords,v in json_data.items():
            logging.info(f'keywords = {keywords}')
            for paper_id,contents in v.items():
                contents = str(contents)

                update_time, paper_title, paper_first_author, paper_url, hjfy_link = parse_arxiv_string(contents)

                contents = "|{}|{}|{}|{}|{}|\n".format(update_time,paper_title,paper_first_author,paper_url,hjfy_link)
                json_data[keywords][paper_id] = str(contents)
                logging.info(f'paper_id = {paper_id}, contents = {contents}')

                # PapersWithCode API is deprecated, skip code link updates
                # Papers will keep their existing null code links
                logging.info(f'Skipping code link update for paper_id = {paper_id} (PapersWithCode API deprecated)')
        # dump to json file
        with open(filename,"w") as f:
            json.dump(json_data,f)

def update_json_file(filename,data_dict):
    '''
    daily update json file using data_dict
    '''
    with open(filename,"r") as f:
        content = f.read()
        if not content:
            m = {}
        else:
            m = json.loads(content)

    json_data = m.copy()

    # update papers in each keywords
    for data in data_dict:
        for keyword in data.keys():
            papers = data[keyword]

            if keyword in json_data.keys():
                json_data[keyword].update(papers)
            else:
                json_data[keyword] = papers

    with open(filename,"w") as f:
        json.dump(json_data,f)

def json_to_md(filename,md_filename,
               task = '',
               to_web = False,
               use_title = True,
               use_tc = True,
               show_badge = True,
               use_b2t = True):
    """
    @param filename: str
    @param md_filename: str
    @return None
    """
    def pretty_math(s:str) -> str:
        ret = ''
        match = re.search(r"\$.*\$", s)
        if match == None:
            return s
        math_start,math_end = match.span()
        space_trail = space_leading = ''
        if s[:math_start][-1] != ' ' and '*' != s[:math_start][-1]: space_trail = ' '
        if s[math_end:][0] != ' ' and '*' != s[math_end:][0]: space_leading = ' '
        ret += s[:math_start]
        ret += f'{space_trail}${match.group()[1:-1].strip()}${space_leading}'
        ret += s[math_end:]
        return ret

    DateNow = datetime.date.today()
    DateNow = str(DateNow)
    DateNow = DateNow.replace('-','.')

    with open(filename,"r") as f:
        content = f.read()
        if not content:
            data = {}
        else:
            data = json.loads(content)

    # clean README.md if daily already exist else create it
    with open(md_filename,"w+") as f:
        pass

    # write data into README.md
    with open(md_filename,"a+") as f:

        if (use_title == True) and (to_web == True):
            f.write("---\n" + "layout: default\n" + "---\n\n")

        if use_title == True:
            if to_web == False:
                f.write("<p align=\"center\">\n")
                f.write("  <h1 align=\"center\">ü§ñ VLM-Arxiv-Daily</h1>\n")
                f.write("  <p align=\"center\">\n")
                f.write("    <img src=\"https://img.shields.io/badge/Robotics-VLA-orange?style=for-the-badge&logo=robotframework\" alt=\"Robotics\">\n")
                f.write("    <img src=\"https://img.shields.io/badge/Navigation-VLN-blue?style=for-the-badge&logo=googlemaps\" alt=\"Navigation\">\n")
                f.write("    <img src=\"https://img.shields.io/badge/Vision--Language-VLM-green?style=for-the-badge&logo=openai\" alt=\"VLM\">\n")
                f.write("  </p>\n")
                f.write("</p>\n\n")
                f.write("> üöÄ ÊØèÊó•Ëá™Âä®ËøΩË∏™ **Vision-Language-Action (VLA)**, **Vision-Language Navigation (VLN)** Âíå **Vision-Language Models (VLM)** ÁöÑÊúÄÊñ∞ Arxiv ËÆ∫Êñá„ÄÇ\n\n")
            f.write("## üìÖ Updated on " + DateNow + "\n")
        else:
            f.write("> Updated on " + DateNow + "\n")

        #Add: table of contents
        if use_tc == True:
            if to_web:
                f.write("\n**Table of Contents**\n\n")
                for keyword in data.keys():
                    day_content = data[keyword]
                    if not day_content:
                        continue
                    kw = keyword.replace(' ','-')
                    f.write(f"- [{keyword}](#{kw.lower()})\n")
                f.write("\n")
            else:
                f.write("<details>\n")
                f.write("  <summary>ÁÇπÂáªÊü•ÁúãÁõÆÂΩï (Table of Contents)</summary>\n")
                f.write("  <ol>\n")
                for keyword in data.keys():
                    day_content = data[keyword]
                    if not day_content:
                        continue
                    kw = keyword.replace(' ','-')
                    f.write(f"    <li><a href=#{kw.lower()}>{keyword}</a></li>\n")
                f.write("  </ol>\n")
                f.write("</details>\n\n")

        for keyword in data.keys():
            day_content = data[keyword]
            if not day_content:
                continue
            # the head of each part
            f.write(f"### üìå {keyword}\n\n")

            if use_title == True :
                if to_web == False:
                    f.write("|Publish Date (YYYY-MM-DD)|Title|Authors|PDF|HJFY|\n" + "|---|---|---|---|---|\n")
                else:
                    f.write("| Publish Date (YYYY-MM-DD) | Title | Authors | PDF | HJFY |\n")
                    f.write("|:---------|:-----------------------|:---------|:------|:------|\n")

            # sort papers by date
            day_content = sort_papers(day_content)

            for _,v in day_content.items():
                if v is not None:
                    # ÂéªÊéâË°®Ê†º‰∏≠ÁöÑ Code ÂàóÔºàÂ¶ÇÊûúÊúâÁöÑËØùÔºâ
                    v_clean = v.replace("|null|", "|")
                    f.write(pretty_math(v_clean)) # make latex pretty

            f.write(f"\n")

            #Add: back to top
            if use_b2t:
                top_info = f"#Updated on {DateNow}"
                top_info = top_info.replace(' ','-').replace('.','')
                if to_web:
                    f.write(f"[back to top]({top_info.lower()})\n\n")
                else:
                    f.write(f"<p align=right>(<a href={top_info.lower()}>back to top</a>)</p>\n\n")

        if show_badge == True:
            # we don't like long string, break it!
            f.write((f"[contributors-shield]: https://img.shields.io/github/"
                     f"contributors/20bytes/vlm-arxiv-daily.svg?style=for-the-badge\n"))
            f.write((f"[contributors-url]: https://github.com/20bytes/"
                     f"vlm-arxiv-daily/graphs/contributors\n"))
            f.write((f"[forks-shield]: https://img.shields.io/github/forks/20bytes/"
                     f"vlm-arxiv-daily.svg?style=for-the-badge\n"))
            f.write((f"[forks-url]: https://github.com/20bytes/"
                     f"vlm-arxiv-daily/network/members\n"))
            f.write((f"[stars-shield]: https://img.shields.io/github/stars/20bytes/"
                     f"vlm-arxiv-daily.svg?style=for-the-badge\n"))
            f.write((f"[stars-url]: https://github.com/20bytes/"
                     f"vlm-arxiv-daily/stargazers\n"))
            f.write((f"[issues-shield]: https://img.shields.io/github/issues/20bytes/"
                     f"vlm-arxiv-daily.svg?style=for-the-badge\n"))
            f.write((f"[issues-url]: https://github.com/20bytes/"
                     f"vlm-arxiv-daily/issues\n\n"))

    logging.info(f"{task} finished")

def json_to_html(filename, html_filename, task = '', translation_cache_path = ''):
    """
    Generate an interactive HTML page for GitHub Pages.
    Evaluation states are stored in localStorage per arXiv ID.
    """
    def parse_md_link(md: str):
        match = re.match(r"\[([^\]]+)\]\(([^)]+)\)", md)
        if match:
            return match.group(1).strip(), match.group(2).strip()
        return md.strip(), ""

    DateNow = datetime.date.today()
    DateNow = str(DateNow).replace('-','.')

    with open(filename,"r") as f:
        content = f.read()
        if not content:
            data = {}
        else:
            data = json.loads(content)

    translation_cache = load_translation_cache(translation_cache_path) if translation_cache_path else {}
    abstracts_dir = os.path.join(os.path.dirname(html_filename), "abstracts")
    os.makedirs(abstracts_dir, exist_ok=True)

    def render_abstract_page(arxiv_id: str, title: str, abstract_en: str, abstract_zh: str) -> None:
        page_path = os.path.join(abstracts_dir, f"{arxiv_id}.html")
        with open(page_path, "w", encoding="utf-8") as af:
            af.write("<!DOCTYPE html>\n")
            af.write("<html lang=\"zh-CN\">\n")
            af.write("<head>\n")
            af.write("  <meta charset=\"UTF-8\" />\n")
            af.write("  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n")
            af.write("  <title>Abstract</title>\n")
            af.write("  <style>\n")
            af.write("    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }\n")
            af.write("    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }\n")
            af.write("    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }\n")
            af.write("    h1 { font-size: 20px; margin: 0 0 12px; }\n")
            af.write("    h2 { font-size: 16px; margin: 20px 0 8px; }\n")
            af.write("    p { line-height: 1.6; white-space: pre-wrap; }\n")
            af.write("    a { color: #2563eb; }\n")
            af.write("  </style>\n")
            af.write("</head>\n")
            af.write("<body>\n")
            af.write("  <div class=\"wrap\">\n")
            af.write("    <div class=\"card\">\n")
            af.write(f"      <h1>{html.escape(title)}</h1>\n")
            af.write("      <h2>Abstract (EN)</h2>\n")
            af.write(f"      <p>{html.escape(abstract_en)}</p>\n")
            af.write("      <h2>ÊëòË¶Å (ZH)</h2>\n")
            af.write(f"      <p>{html.escape(abstract_zh)}</p>\n")
            af.write("      <p><a href=\"../index.html\">‚Üê Back</a></p>\n")
            af.write("    </div>\n")
            af.write("  </div>\n")
            af.write("</body>\n")
            af.write("</html>\n")

    with open(html_filename,"w") as f:
        f.write("<!DOCTYPE html>\n")
        f.write("<html lang=\"zh-CN\">\n")
        f.write("<head>\n")
        f.write("  <meta charset=\"UTF-8\" />\n")
        f.write("  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n")
        f.write("  <title>VLM-Arxiv-Daily</title>\n")
        f.write("  <style>\n")
        f.write("    :root { --bg: #f7f7f5; --card: #ffffff; --text: #1f2937; --muted: #6b7280; --line: #e5e7eb; --accent: #0f766e; }\n")
        f.write("    * { box-sizing: border-box; }\n")
        f.write("    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; color: var(--text); background: var(--bg); }\n")
        f.write("    .wrap { max-width: 1120px; margin: 0 auto; padding: 28px 20px 60px; }\n")
        f.write("    header { background: var(--card); border: 1px solid var(--line); border-radius: 12px; padding: 20px 24px; }\n")
        f.write("    h1 { margin: 0 0 8px; font-size: 28px; }\n")
        f.write("    .sub { color: var(--muted); margin: 0 0 8px; }\n")
        f.write("    .updated { font-weight: 600; }\n")
        f.write("    .toc { margin: 16px 0 0; padding: 0; list-style: none; display: flex; flex-wrap: wrap; gap: 10px; }\n")
        f.write("    .toc a { text-decoration: none; color: var(--accent); background: #e7f3f1; padding: 4px 10px; border-radius: 999px; font-size: 13px; }\n")
        f.write("    section { margin-top: 24px; }\n")
        f.write("    table { width: 100%; border-collapse: collapse; background: var(--card); border: 1px solid var(--line); border-radius: 12px; overflow: hidden; }\n")
        f.write("    th, td { padding: 10px 12px; border-bottom: 1px solid var(--line); vertical-align: top; font-size: 14px; }\n")
        f.write("    th { background: #f3f4f6; text-align: left; white-space: nowrap; }\n")
        f.write("    tr:last-child td { border-bottom: none; }\n")
        f.write("    .eval { display: inline-flex; gap: 8px; }\n")
        f.write("    .eval button { border: 1px solid var(--line); background: #fff; padding: 2px 8px; border-radius: 6px; cursor: pointer; font-size: 14px; }\n")
        f.write("    .eval button.active { border-color: var(--accent); background: #e7f3f1; }\n")
        f.write("    .legend { color: var(--muted); font-size: 13px; margin: 8px 0 0; }\n")
        f.write("    a { color: #2563eb; }\n")
        f.write("  </style>\n")
        f.write("</head>\n")
        f.write("<body>\n")
        f.write("  <div class=\"wrap\">\n")
        f.write("    <header>\n")
        f.write("      <h1>ü§ñ VLM-Arxiv-Daily</h1>\n")
        f.write("      <p class=\"sub\">ÊØèÊó•Ëá™Âä®ËøΩË∏™ Vision-Language-Action (VLA)„ÄÅVision-Language Navigation (VLN) Âíå Vision-Language Models (VLM) ÁöÑÊúÄÊñ∞ arXiv ËÆ∫Êñá„ÄÇ</p>\n")
        f.write(f"      <p class=\"updated\">Updated on {DateNow}</p>\n")
        f.write("      <ul class=\"toc\">\n")
        for keyword in data.keys():
            day_content = data[keyword]
            if not day_content:
                continue
            kw = keyword.replace(' ','-').lower()
            f.write(f"        <li><a href=\"#{kw}\">{html.escape(keyword)}</a></li>\n")
        f.write("      </ul>\n")
        f.write("    </header>\n")

        for keyword in data.keys():
            day_content = data[keyword]
            if not day_content:
                continue

            f.write(f"    <section id=\"{keyword.replace(' ','-').lower()}\">\n")
            f.write(f"      <h2>üìå {html.escape(keyword)}</h2>\n")
            f.write("      <table>\n")
            f.write("        <thead>\n")
            f.write("          <tr>\n")
            f.write("            <th>Publish Date (YYYY-MM-DD)</th>\n")
            f.write("            <th>Title</th>\n")
            f.write("            <th>Authors</th>\n")
            f.write("            <th>PDF</th>\n")
            f.write("            <th>HJFY</th>\n")
            f.write("            <th>ËØÑ‰º∞</th>\n")
            f.write("          </tr>\n")
            f.write("        </thead>\n")
            f.write("        <tbody>\n")

            day_content = sort_papers(day_content)
            for _, v in day_content.items():
                if v is None:
                    continue
                parts = [p for p in str(v).strip().split("|") if p != ""]
                if len(parts) < 5:
                    continue
                date = parts[0].replace("**","").strip()
                title = parts[1].replace("**","").strip()
                authors = parts[2].strip()
                pdf_text, pdf_url = parse_md_link(parts[3].strip())
                hjfy_text, hjfy_link = parse_md_link(parts[4].strip())
                arxiv_id = pdf_text or pdf_url.rsplit("/", 1)[-1]
                arxiv_id = re.sub(r'v\\d+$', '', arxiv_id)

                cached = translation_cache.get(arxiv_id) or {}
                if isinstance(cached, dict) and cached.get("abstract_en") and cached.get("abstract_zh"):
                    render_abstract_page(
                        arxiv_id,
                        title,
                        cached.get("abstract_en", ""),
                        cached.get("abstract_zh", ""),
                    )

                f.write(f"          <tr data-arxiv-id=\"{html.escape(arxiv_id)}\">\n")
                f.write(f"            <td>{html.escape(date)}</td>\n")
                safe_title = html.escape(title)
                safe_title = safe_title.replace("&lt;br&gt;", "<br>").replace("&lt;br/&gt;", "<br>")
                # convert markdown links inside title cell to HTML anchors
                def replace_md_link(match):
                    text = match.group(1)
                    url = match.group(2)
                    return f'<a href="{html.escape(url)}">{html.escape(text)}</a>'
                safe_title = re.sub(r"\[([^\]]+)\]\(([^)]+)\)", replace_md_link, safe_title)
                f.write(f"            <td>{safe_title}</td>\n")
                f.write(f"            <td>{html.escape(authors)}</td>\n")
                f.write(f"            <td><a href=\"{html.escape(pdf_url)}\">{html.escape(pdf_text)}</a></td>\n")
                f.write(f"            <td><a href=\"{html.escape(hjfy_link)}\">{html.escape(hjfy_text or 'HJFY')}</a></td>\n")
                f.write("            <td>\n")
                f.write("              <div class=\"eval\">\n")
                f.write("                <button type=\"button\" data-value=\"read\">‚úÖ</button>\n")
                f.write("                <button type=\"button\" data-value=\"skip\">‚ùå</button>\n")
                f.write("                <button type=\"button\" data-value=\"star\">‚≠ê</button>\n")
                f.write("              </div>\n")
                f.write("            </td>\n")
                f.write("          </tr>\n")

            f.write("        </tbody>\n")
            f.write("      </table>\n")
            f.write("      <p class=\"legend\">ËØÑ‰º∞Áä∂ÊÄÅ‰øùÂ≠òÂú®ÊµèËßàÂô®Êú¨Âú∞ÔºàlocalStorageÔºâÔºåÊç¢ËÆæÂ§á/ÊµèËßàÂô®‰∏ç‰ºöÂêåÊ≠•„ÄÇ</p>\n")
            f.write("    </section>\n")

        f.write("  </div>\n")
        f.write("  <script>\n")
        f.write("    const storageKey = (id) => `vlm_arxiv_daily_eval:${id}`;\n")
        f.write("    const rows = document.querySelectorAll('tr[data-arxiv-id]');\n")
        f.write("    rows.forEach((row) => {\n")
        f.write("      const id = row.getAttribute('data-arxiv-id');\n")
        f.write("      const buttons = row.querySelectorAll('button[data-value]');\n")
        f.write("      const saved = localStorage.getItem(storageKey(id));\n")
        f.write("      if (saved) {\n")
        f.write("        buttons.forEach((btn) => {\n")
        f.write("          if (btn.dataset.value === saved) btn.classList.add('active');\n")
        f.write("        });\n")
        f.write("      }\n")
        f.write("      buttons.forEach((btn) => {\n")
        f.write("        btn.addEventListener('click', () => {\n")
        f.write("          const val = btn.dataset.value;\n")
        f.write("          localStorage.setItem(storageKey(id), val);\n")
        f.write("          buttons.forEach((b) => b.classList.remove('active'));\n")
        f.write("          btn.classList.add('active');\n")
        f.write("        });\n")
        f.write("      });\n")
        f.write("    });\n")
        f.write("  </script>\n")
        f.write("</body>\n")
        f.write("</html>\n")

    logging.info(f"{task} finished")

def demo(**config):
    # TODO: use config
    data_collector = []
    data_collector_web= []

    keywords = config['kv']
    max_results = config['max_results']
    publish_readme = config['publish_readme']
    publish_gitpage = config['publish_gitpage']
    publish_wechat = config['publish_wechat']
    show_badge = config['show_badge']

    translate_opts = None
    if config.get('translate_title', False):
        api_key = os.getenv("DEEPSEEK_API_KEY", "").strip()
        if not api_key:
            logging.warning("DEEPSEEK_API_KEY is not set; skip title translation.")
        else:
            translate_opts = {
                "api_key": api_key,
                "base_url": config.get("deepseek_base_url", "https://api.deepseek.com"),
                "model": config.get("deepseek_model", "deepseek-chat"),
                "cache_path": config.get("translation_cache_path", "./docs/title_translations.json"),
                "cache": {},
                "dirty": False,
                "translate_abstract": config.get("translate_abstract", False),
            }
            translate_opts["cache"] = load_translation_cache(translate_opts["cache_path"])

    b_update = config['update_paper_links']
    logging.info(f'Update Paper Link = {b_update}')
    if config['update_paper_links'] == False:
        logging.info(f"GET daily papers begin")
        for topic, keyword in keywords.items():
            logging.info(f"Keyword: {topic}")
            data, data_web = get_daily_papers(topic, query = keyword,
                                            max_results = max_results,
                                            translate_opts = translate_opts)
            data_collector.append(data)
            data_collector_web.append(data_web)
            print("\n")
        logging.info(f"GET daily papers end")
        if translate_opts and translate_opts["dirty"]:
            save_translation_cache(translate_opts["cache_path"], translate_opts["cache"])

    # 1. update README.md file
    if publish_readme:
        json_file = config['json_readme_path']
        md_file   = config['md_readme_path']
        # update paper links
        if config['update_paper_links']:
            update_paper_links(json_file)
        else:
            # update json data
            update_json_file(json_file,data_collector)
        # json data to markdown
        json_to_md(json_file,md_file, task ='Update Readme', \
            show_badge = show_badge)

    # 2. update docs/index.md file (to gitpage)
    if publish_gitpage:
        json_file = config['json_gitpage_path']
        md_file   = config['md_gitpage_path']
        # TODO: duplicated update paper links!!!
        if config['update_paper_links']:
            update_paper_links(json_file)
        else:
            update_json_file(json_file,data_collector)
        if md_file.endswith(".html"):
            json_to_html(
                json_file,
                md_file,
                task ='Update GitPage',
                translation_cache_path=config.get("translation_cache_path", "./docs/title_translations.json"),
            )
        else:
            json_to_md(json_file, md_file, task ='Update GitPage', \
                to_web = True, show_badge = show_badge, \
                use_tc=False, use_b2t=False)

    # 3. Update docs/wechat.md file
    if publish_wechat:
        json_file = config['json_wechat_path']
        md_file   = config['md_wechat_path']
        # TODO: duplicated update paper links!!!
        if config['update_paper_links']:
            update_paper_links(json_file)
        else:
            update_json_file(json_file, data_collector_web)
        json_to_md(json_file, md_file, task ='Update Wechat', \
            to_web=False, use_title= False, show_badge = show_badge)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--config_path',type=str, default='config.yaml',
                            help='configuration file path')
    parser.add_argument('--update_paper_links', default=False,
                        action="store_true",help='whether to update paper links etc.')
    args = parser.parse_args()
    config = load_config(args.config_path)
    config = {**config, 'update_paper_links':args.update_paper_links}
    demo(**config)
