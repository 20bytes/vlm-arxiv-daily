{
  "2602.02459": {
    "title_zh": "TIC-VLA：一种用于动态环境中机器人导航的思控一体化视觉-语言-动作模型",
    "abstract_zh": "在动态、以人为中心的环境中，机器人必须遵循语言指令，同时保持实时反应控制。视觉-语言-动作（VLA）模型提供了一个有前景的框架，但它们假设推理与控制在时间上对齐，尽管语义推理本质上相对于实时动作存在延迟。我们提出了思控一体化（TIC）-VLA，这是一个延迟感知框架，在动作生成过程中显式建模延迟的语义推理。TIC-VLA定义了一个延迟的语义-控制接口，除了当前观测外，还将动作生成条件化于延迟的视觉-语言语义状态和显式延迟元数据，使策略能够补偿异步推理。我们进一步提出了一种延迟一致性训练流程，在模仿学习和在线强化学习中注入推理延迟，使训练与异步部署保持一致。为了支持真实评估，我们推出了DynaNav，这是一个物理精确、照片级逼真的仿真套件，用于动态环境中的语言引导导航。在仿真和真实机器人上的大量实验表明，TIC-VLA在数秒推理延迟下持续优于先前的VLA模型，同时保持稳健的实时控制。项目网站：https://ucla-mobility.github.io/TIC-VLA/",
    "abstract_en": "Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/"
  },
  "2602.02454": {
    "title_zh": "世界体操家：在世界模型中通过强化学习训练机器人",
    "abstract_zh": "机器人通过与物理世界交互进行学习，从根本上受到物理交互成本的制约。两种替代方案——基于专家演示的监督微调（SFT）和基于软件模拟器的强化学习（RL）——分别受限于可用专家数据的数量以及操作任务中的仿真到现实差距。随着近期从真实世界视频-动作数据中学习的世界模型的出现，我们提出一个问题：在世界模型中训练策略是否比监督学习或软件仿真更能有效提升真实机器人的性能。我们提出了World-Gymnast方法，该方法通过在动作条件化的视频世界模型中展开策略，并利用视觉语言模型（VLM）对展开过程进行奖励，从而对视觉语言动作（VLA）策略进行强化学习微调。在Bridge机器人实验平台上，World-Gymnast的性能比SFT最高提升18倍，比软件模拟器最高提升2倍。更重要的是，World-Gymnast展示了基于世界模型的强化学习的引人注目的能力，包括在世界模型中对多样化语言指令和新场景进行训练、在新场景中进行测试时训练，以及在线迭代优化世界模型和策略。我们的结果表明，学习世界模型并在云端训练机器人策略，可能是弥合仅能在演示中工作的机器人与能在任何家庭中工作的机器人之间差距的关键。",
    "abstract_en": "Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household."
  },
  "2602.02402": {
    "title_zh": "SoMA：面向机器人软体操作的真实到仿真神经模拟器",
    "abstract_zh": "在丰富交互下模拟可变形物体对于真实到仿真的机器人操作仍是一个根本性挑战，其动力学由环境效应与机器人动作共同驱动。现有模拟器依赖于预定义物理或数据驱动的动力学，缺乏机器人条件控制，限制了准确性、稳定性和泛化能力。本文提出SoMA，一种用于软体操作的三维高斯泼溅模拟器。SoMA将可变形动力学、环境力和机器人关节动作耦合于统一的潜在神经空间中，实现端到端的真实到仿真模拟。通过在学习的高斯泼溅上建模交互，SoMA实现了可控、稳定的长时程操作，并能在无预定义物理模型的情况下泛化至未观测轨迹。SoMA将真实世界机器人操作的再模拟准确性与泛化能力提升了20%，能够稳定模拟复杂任务，如长时程布料折叠。",
    "abstract_en": "Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding."
  },
  "2602.02212": {
    "title_zh": "MAIN-VLA：为视觉-语言-动作模型建模意图与环境的抽象",
    "abstract_zh": "尽管视觉-语言-动作（VLA）模型已取得显著进展，但在涉及实时不可预测交互的高度复杂动态环境（如3D开放世界和大型PvP游戏）中，现有方法仍难以从冗余的传感器流中高效提取动作关键信号。为此，我们提出MAIN-VLA框架，通过显式建模意图与环境的抽象，将决策制定建立在深层语义对齐而非浅层模式匹配的基础上。具体而言，我们的意图抽象（IA）将冗长的语言指令及其相关推理提炼为紧凑、显式的语义基元，而环境语义抽象（ESA）则将海量视觉流映射为结构化的拓扑可供性表示。此外，对齐这两种抽象模态会引发一种自发的注意力集中效应，从而支持无需参数的令牌剪枝策略，在保持性能的同时过滤感知冗余。在开放世界《我的世界》及大规模PvP环境（《和平精英》与《无畏契约》）中的大量实验表明，MAIN-VLA实现了新的技术突破，具备更优的决策质量、更强的泛化能力以及顶尖的推理效率。",
    "abstract_en": "Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency."
  },
  "2602.02142": {
    "title_zh": "FD-VLA：用于接触丰富操作的力蒸馏视觉-语言-动作模型",
    "abstract_zh": "力感知是视觉-语言-动作（VLA）框架中的关键模态，因为它能够在接触丰富的任务中实现细粒度感知和灵巧操作。我们提出了力蒸馏VLA（FD-VLA），这是一种新颖的框架，可在不依赖物理力传感器的情况下，将力感知集成到接触丰富的操作中。我们方法的核心是力蒸馏模块（FDM），它通过将可学习的查询令牌（以视觉观察和机器人状态为条件）映射到与真实力信号潜在表示对齐的预测力令牌中，从而蒸馏出力信息。在推理过程中，这个蒸馏出的力令牌被注入到预训练的VLM中，使其能够进行力感知推理，同时保持其视觉-语言语义的完整性。这种设计带来两个关键优势：首先，它允许在缺乏昂贵或易受力-扭矩传感器的广泛机器人上进行实际部署，从而降低硬件成本和复杂性；其次，FDM在VLM之前引入了额外的力-视觉-状态融合先验，这改善了跨模态对齐，并增强了接触丰富场景下的感知-动作鲁棒性。令人惊讶的是，我们的物理实验表明，蒸馏出的力令牌优于直接传感器力测量以及其他基线方法，这突显了这种力蒸馏VLA方法的有效性。",
    "abstract_en": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
  },
  "2602.02063": {
    "title_zh": "See2Refine：视觉-语言反馈提升基于大语言模型的eHMI行为设计能力",
    "abstract_zh": "自动驾驶车辆缺乏与其他道路使用者的自然沟通渠道，因此外部人机界面（eHMI）对于在共享环境中传达意图和维持信任至关重要。然而，大多数eHMI研究依赖于开发者手工设计的消息-行为配对，难以适应多样且动态的交通场景。一种有前景的替代方案是利用大语言模型（LLM）作为行为设计器，生成基于上下文的eHMI行为，但此类设计器缺乏感知验证，通常依赖固定提示或昂贵的人工标注反馈进行改进。我们提出了See2Refine，一种无需人工干预的闭环框架，利用视觉-语言模型（VLM）的感知评估作为自动化视觉反馈，以优化基于LLM的eHMI行为设计器。给定驾驶场景和候选eHMI行为，VLM评估该行为的感知适宜性，并利用此反馈迭代修正设计器的输出，从而实现无需人工监督的系统性优化。我们在三种eHMI模态（光条、眼睛和手臂）及多种LLM模型规模下评估了该框架。在所有设置中，我们的框架在基于VLM的指标和人类受试者评估中均持续优于仅使用提示的LLM设计器及手动指定的基线方法。结果进一步表明，改进效果在不同模态间具有泛化性，且VLM评估与人类偏好高度一致，这支持了See2Refine在可扩展行为设计中的鲁棒性和有效性。",
    "abstract_en": "Automated vehicles lack natural communication channels with other road users, making external Human-Machine Interfaces (eHMIs) essential for conveying intent and maintaining trust in shared environments. However, most eHMI studies rely on developer-crafted message-action pairs, which are difficult to adapt to diverse and dynamic traffic contexts. A promising alternative is to use Large Language Models (LLMs) as action designers that generate context-conditioned eHMI actions, yet such designers lack perceptual verification and typically depend on fixed prompts or costly human-annotated feedback for improvement. We present See2Refine, a human-free, closed-loop framework that uses vision-language model (VLM) perceptual evaluation as automated visual feedback to improve an LLM-based eHMI action designer. Given a driving context and a candidate eHMI action, the VLM evaluates the perceived appropriateness of the action, and this feedback is used to iteratively revise the designer's outputs, enabling systematic refinement without human supervision. We evaluate our framework across three eHMI modalities (lightbar, eyes, and arm) and multiple LLM model sizes. Across settings, our framework consistently outperforms prompt-only LLM designers and manually specified baselines in both VLM-based metrics and human-subject evaluations. Results further indicate that the improvements generalize across modalities and that VLM evaluations are well aligned with human preferences, supporting the robustness and effectiveness of See2Refine for scalable action design."
  },
  "2602.01834": {
    "title_zh": "面向视觉语言动作模型推理时安全性的概念词典学习方法",
    "abstract_zh": "视觉语言动作（VLA）模型通过将多模态指令转化为可执行行为，实现了感知-动作闭环，但这一能力也放大了安全风险：在大型语言模型中仅产生有害文本的越狱攻击，可能在具身系统中触发不安全的物理行为。现有防御方法（如对齐、过滤或提示强化）干预过晚或针对错误模态，导致融合后的表征仍可被利用。我们提出了一种基于概念的词典学习框架，用于推理时的安全控制。该方法通过从隐藏层激活中构建稀疏、可解释的词典，识别有害概念方向，并应用基于阈值的干预来抑制或阻断不安全激活。在Libero-Harm、BadRobot、RoboPair和IS-Bench上的实验表明，我们的方法实现了最先进的防御性能，将攻击成功率降低超过70%，同时保持任务成功率。关键的是，该框架为即插即用且模型无关，无需重新训练，并能与多种VLA模型无缝集成。据我们所知，这是首个面向具身系统的推理时基于概念的安全方法，推动了VLA模型的可解释性与安全部署。",
    "abstract_en": "Vision Language Action (VLA) models close the perception action loop by translating multimodal instructions into executable behaviors, but this very capability magnifies safety risks: jailbreaks that merely yield toxic text in LLMs can trigger unsafe physical actions in embodied systems. Existing defenses alignment, filtering, or prompt hardening intervene too late or at the wrong modality, leaving fused representations exploitable. We introduce a concept-based dictionary learning framework for inference-time safety control. By constructing sparse, interpretable dictionaries from hidden activations, our method identifies harmful concept directions and applies threshold-based interventions to suppress or block unsafe activations. Experiments on Libero-Harm, BadRobot, RoboPair, and IS-Bench show that our approach achieves state-of-the-art defense performance, cutting attack success rates by over 70\\% while maintaining task success. Crucially, the framework is plug-in and model-agnostic, requiring no retraining and integrating seamlessly with diverse VLAs. To our knowledge, this is the first inference-time concept-based safety method for embodied systems, advancing both interpretability and safe deployment of VLA models."
  },
  "2602.01811": {
    "title_zh": "从精确认知到精准执行：面向视觉语言动作模型的通用自校正与终止框架",
    "abstract_zh": "尽管面向具身智能体的视觉语言动作（VLA）模型集成了感知、推理与控制能力，但仍受限于两大关键弱点：其一，在抓取任务中，语言模型生成的动作标记常与目标对象存在细微的空间偏差，导致抓取失败；其二，模型缺乏可靠的任务完成识别能力，引发冗余动作及频繁的超时错误。为应对这些挑战并提升鲁棒性，我们提出了一种轻量级、无需训练的框架VLA-SCT。该框架作为自校正控制循环运行，结合了数据驱动的动作优化与基于条件的终止逻辑。因此，相较于基线方法，我们的方案在LIBERO基准测试的所有数据集中均实现了稳定提升，显著提高了精细操作任务的成功率，并确保任务准确完成，从而推动更可靠的VLA智能体在复杂非结构化环境中的部署。",
    "abstract_en": "While vision-language-action (VLA) models for embodied agents integrate perception, reasoning, and control, they remain constrained by two critical weaknesses: first, during grasping tasks, the action tokens generated by the language model often exhibit subtle spatial deviations from the target object, resulting in grasp failures; second, they lack the ability to reliably recognize task completion, which leads to redundant actions and frequent timeout errors. To address these challenges and enhance robustness, we propose a lightweight, training-free framework, VLA-SCT. This framework operates as a self-correcting control loop, combining data-driven action refinement with conditional logic for termination. Consequently, compared to baseline approaches, our method achieves consistent improvements across all datasets in the LIBERO benchmark, significantly increasing the success rate of fine manipulation tasks and ensuring accurate task completion, thereby promoting the deployment of more reliable VLA agents in complex, unstructured environments."
  },
  "2602.01662": {
    "title_zh": "AgenticLab：一个能够观察、思考与行动的真实世界机器人智能体平台",
    "abstract_zh": "近年来，大型视觉语言模型（VLMs）在泛化性开放词汇感知与推理方面取得了显著进展，然而其在非结构化、真实世界环境中执行长时程、闭环操作的实际机器人操控能力仍不明确。现有的基于VLM的操控流程难以在不同研究团队的实验设置间进行比较，且多数评估依赖于仿真环境、特权状态或专门设计的实验条件。本文提出AgenticLab，一个模型无关的机器人智能体平台与开放世界操控基准。AgenticLab提供了一套闭环智能体流程，涵盖感知、任务分解、在线验证与重规划。借助AgenticLab，我们在非结构化环境中的真实机器人任务上对当前最先进的基于VLM的智能体进行了基准测试。我们的基准揭示了一系列离线视觉语言测试（如视觉问答与静态图像理解）未能捕捉的故障模式，包括多步语义落地一致性失效、遮挡与场景变化下的物体定位困难，以及空间推理能力不足以支持可靠操控等问题。我们将发布完整的硬件与软件栈，以支持可复现的评估，并加速通用机器人智能体的研究进程。",
    "abstract_en": "Recent advances in large vision-language models (VLMs) have demonstrated generalizable open-vocabulary perception and reasoning, yet their real-robot manipulation capability remains unclear for long-horizon, closed-loop execution in unstructured, in-the-wild environments. Prior VLM-based manipulation pipelines are difficult to compare across different research groups' setups, and many evaluations rely on simulation, privileged state, or specially designed setups. We present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. AgenticLab provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using AgenticLab, we benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments. Our benchmark reveals several failure modes that offline vision-language tests (e.g., VQA and static image understanding) fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation. We will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents."
  },
  "2602.01644": {
    "title_zh": "从感知到行动：空间人工智能代理与世界模型",
    "abstract_zh": "尽管大语言模型已成为代理推理与规划的主流方法，但其在符号领域的成功并未能直接迁移到物理世界。空间智能——即感知三维结构、推理物体关系并在物理约束下行动的能力——是一种正交的能力，对具身代理至关重要。现有综述往往孤立地讨论代理架构或空间领域，缺乏将这两种互补能力统一起来的框架。本文旨在弥合这一鸿沟。通过对2000余篇论文的系统梳理（其中引用顶级学术会议的742篇文献），我们提出了一个统一的三轴分类法，将代理能力与跨尺度的空间任务相连接。关键之处在于，我们区分了空间基础（对几何与物理的度量理解）与符号基础（将图像与文本关联），并论证仅凭感知无法赋予代理能力。我们的分析揭示了映射至这三个轴线的三项核心发现：（1）分层记忆系统（能力轴）对长时程空间任务至关重要；（2）图神经网络与大语言模型融合（任务轴）是结构化空间推理的有效途径；（3）世界模型（尺度轴）对于在微观到宏观空间尺度上实现安全部署不可或缺。最后，我们指出了六大挑战并展望未来研究方向，包括建立统一评估框架以标准化跨领域评估。该分类法为整合碎片化的研究奠定了基础，有望推动机器人、自动驾驶与地理空间智能等领域中新一代空间感知自主系统的发展。",
    "abstract_en": "While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence."
  },
  "2602.02220": {
    "title_zh": "LangMap：面向开放词汇目标导航的分层基准",
    "abstract_zh": "物体与语言之间的关系对于人类与人工智能之间的有意义交流以及实际有用的具身智能至关重要。我们引入了HieraNav，一个多粒度、开放词汇的目标导航任务，其中智能体通过解析自然语言指令，在四个语义层级上到达目标：场景、房间、区域和实例。为此，我们提出了Language as a Map（LangMap），这是一个基于真实世界3D室内扫描的大规模基准，包含全面的人工验证标注和覆盖这些层级的任务。LangMap提供区域标签、区分性区域描述、涵盖414个对象类别的区分性实例描述，以及超过18K个导航任务。每个目标都配有简洁和详细的描述，支持不同指令风格的评估。LangMap实现了卓越的标注质量，在区分性准确率上比GOAT-Bench高出23.8%，同时使用的词汇量减少了四倍。在LangMap上对零样本和监督模型的综合评估表明，更丰富的上下文和记忆能提高成功率，而长尾、小型、上下文依赖和远距离目标，以及多目标完成，仍然是挑战。HieraNav和LangMap为推进语言驱动的具身导航建立了一个严谨的测试平台。项目地址：https://bo-miao.github.io/LangMap",
    "abstract_en": "The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap"
  },
  "2602.00551": {
    "title_zh": "APEX：一种用于异步空中目标导航的解耦记忆型探索器",
    "abstract_zh": "空中目标导航是具身人工智能领域的一个前沿挑战，要求无人机代理仅通过视觉感知和语言描述，自主探索、推理并识别特定目标。然而，现有方法在记忆复杂空中环境的空间表示、实现可靠且可解释的动作决策，以及高效探索和信息收集方面存在困难。为解决这些挑战，我们提出了APEX（空中并行探索器），这是一种新颖的分层代理，专为复杂空中环境中的高效探索和目标获取而设计。APEX基于模块化的三部分架构构建：1）动态空间语义映射记忆，利用视觉语言模型的零样本能力，动态构建高分辨率的三维吸引力、探索和障碍物地图，作为可解释的记忆机制；2）动作决策模块，通过强化学习训练，将丰富的空间理解转化为细粒度且鲁棒的控制策略；3）目标定位模块，采用开放词汇检测器实现确定性和泛化性的目标识别。所有这些组件被集成到一个分层、异步且并行的框架中，有效规避了视觉语言模型的推理延迟，并提升了代理在探索中的主动性。大量实验表明，在具有挑战性的UAV-ON基准测试中，APEX在成功率上超越了先前最佳方法4.2%，在最短路径长度指标上提升了2.8%，证明了其卓越的效率和分层异步设计的有效性。我们的源代码已在GitHub上提供。",
    "abstract_en": "Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \\textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM's inference latency and boosting the agent's proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\\% SR and +2.8\\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \\href{https://github.com/4amGodvzx/apex}{GitHub}"
  },
  "2602.00222": {
    "title_zh": "MapDream：面向视觉语言导航的任务驱动地图学习",
    "abstract_zh": "视觉语言导航要求智能体在部分可观测的三维环境中遵循自然语言指令，这促使需要构建能够聚合超越局部感知的空间上下文的地图表示。然而，现有方法大多依赖于独立于导航策略构建的手工地图。我们认为，地图应是直接由导航目标塑造的学习表示，而非详尽的重建。基于这一见解，我们提出了MapDream，一个地图闭环框架，将地图构建形式化为自回归的鸟瞰图图像合成。该框架联合学习地图生成与动作预测，将环境上下文提炼为紧凑的三通道鸟瞰图地图，仅保留对导航至关重要的可操作信息。通过监督预训练引导出可靠的映射到控制接口，而自回归设计则支持通过强化微调实现端到端的联合优化。在R2R-CE和RxR-CE数据集上的实验达到了单目视觉导航的先进水平，验证了任务驱动的生成式地图学习的有效性。",
    "abstract_en": "Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning."
  },
  "2601.21751": {
    "title_zh": "动态拓扑感知：打破视觉语言导航中的粒度僵化",
    "abstract_zh": "连续环境下的视觉语言导航（VLN-CE）面临一个核心挑战：将高层语言指令落实到精确、安全且长距离的空间行动中。显式拓扑图已被证明是为此类任务提供鲁棒空间记忆的关键方案。然而，现有拓扑规划方法存在“粒度僵化”问题。具体而言，这些方法通常依赖固定的几何阈值来采样节点，无法适应多变的环境复杂性。这种僵化导致严重的不匹配：模型在简单区域倾向于过度采样，造成计算冗余；而在高不确定性区域则采样不足，增加碰撞风险并损害导航精度。为解决此问题，我们提出了DGNav——一个动态拓扑导航框架，引入上下文感知机制以实时调整地图密度与连通性。我们的方法包含两项核心创新：（1）场景感知自适应策略：基于预测路径点的离散度动态调整图构建阈值，实现在复杂环境中“按需加密”；（2）动态图变换器：通过融合视觉、语言与几何线索为动态边权重，重构图连通性，使智能体能够滤除拓扑噪声并增强指令遵循能力。在R2R-CE和RxR-CE基准上的大量实验表明，DGNav展现出卓越的导航性能和强大的泛化能力。此外，消融研究证实该框架在导航效率与安全探索间实现了最优平衡。代码已开源：https://github.com/shannanshouyin/DGNav。",
    "abstract_en": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a \"Granularity Rigidity\" problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling \"densification on demand\" in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at https://github.com/shannanshouyin/DGNav."
  },
  "2601.18492": {
    "title_zh": "DV-VLN：基于大语言模型的视觉与语言导航双重验证可靠框架",
    "abstract_zh": "视觉与语言导航任务要求具身智能体根据自然语言指令在复杂三维环境中进行导航。近期大语言模型的发展提升了语言驱动导航的可解释性。然而，大多数基于大语言模型的智能体仍依赖单次动作决策机制，即模型必须从带有噪声的多视角文本化观察结果中选择一个选项。由于局部信息不匹配及中间推理过程的不完善，此类决策极易偏离正确路径，导致误差累积并在未知环境中降低可靠性。本文提出DV-VLN——一种遵循“生成-验证”范式的新型视觉与语言导航框架。该框架首先对开源LLaMA-2主干网络进行参数高效的领域内适配，以生成结构化的导航思维链，随后通过两个互补通道验证候选动作：真伪验证与掩码实体验证。DV-VLN通过聚合多个样本的验证成功次数来选择动作，并生成可解释的分数进行重排序。在R2R、RxR（英文子集）和REVERIE数据集上的实验表明，DV-VLN相较于直接预测和纯采样基线方法均取得稳定提升，在纯语言视觉与语言导航智能体中达到竞争性性能，与多种跨模态系统相比亦展现出有前景的结果。代码已开源：https://github.com/PlumJun/DV-VLN。",
    "abstract_en": "Vision-and-Language Navigation (VLN) requires an embodied agent to navigate in a complex 3D environment according to natural language instructions. Recent progress in large language models (LLMs) has enabled language-driven navigation with improved interpretability. However, most LLM-based agents still rely on single-shot action decisions, where the model must choose one option from noisy, textualized multi-perspective observations. Due to local mismatches and imperfect intermediate reasoning, such decisions can easily deviate from the correct path, leading to error accumulation and reduced reliability in unseen environments. In this paper, we propose DV-VLN, a new VLN framework that follows a generate-then-verify paradigm. DV-VLN first performs parameter-efficient in-domain adaptation of an open-source LLaMA-2 backbone to produce a structured navigational chain-of-thought, and then verifies candidate actions with two complementary channels: True-False Verification (TFV) and Masked-Entity Verification (MEV). DV-VLN selects actions by aggregating verification successes across multiple samples, yielding interpretable scores for reranking. Experiments on R2R, RxR (English subset), and REVERIE show that DV-VLN consistently improves over direct prediction and sampling-only baselines, achieving competitive performance among language-only VLN agents and promising results compared with several cross-modal systems.Code is available at https://github.com/PlumJun/DV-VLN."
  },
  "2601.18188": {
    "title_zh": "NaVIDA：基于逆动力学增强的视觉语言导航",
    "abstract_zh": "视觉语言导航（VLN）要求智能体能够理解自然语言指令，并在视觉丰富的环境中连贯地执行动作。然而，现有方法大多依赖于反应式的状态-动作映射，未能显式建模动作如何因果性地改变后续视觉观察。由于缺乏这种视觉-动作因果关系，智能体无法预测自身动作引发的视觉变化，导致行为不稳定、泛化能力弱以及轨迹上的累积误差。为解决这些问题，我们提出了NaVIDA（基于逆动力学增强的导航），这是一个统一的VLN框架，将策略学习与动作驱动的视觉动态建模及自适应执行相结合。NaVIDA通过基于动作块的逆动力学监督增强训练，以学习视觉变化与对应动作之间的因果关系。为构建这种监督并扩展有效规划范围，NaVIDA采用分层概率动作块化（HPAC）方法，将轨迹组织为多步动作块，并提供具有区分性的长程视觉变化线索。为进一步抑制推理过程中的误差累积并稳定行为，一种基于熵的引导机制自适应地设置动作块的执行范围。大量实验表明，与现有最先进方法相比，NaVIDA以更少的参数量（30亿 vs. 80亿）实现了更优的导航性能。真实世界机器人评估进一步验证了本方法的实际可行性和有效性。代码与数据将在论文录用后公开。",
    "abstract_en": "Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \\textsc{NaVIDA} (\\textbf{Nav}igation with \\textbf{I}nverse \\textbf{D}ynamics \\textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \\textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \\textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \\textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance."
  },
  "2601.15614": {
    "title_zh": "AION：基于双策略强化学习的空中室内目标导航系统",
    "abstract_zh": "目标导航任务要求智能体在未知环境中自主探索，并导航至由语义标签指定的目标物体。先前的研究主要集中于二维移动场景下的零样本目标导航，而将其扩展至具备三维移动能力的空中平台仍鲜有探索。空中机器人虽具备卓越的机动性与搜索效率，但也带来了空间感知、动态控制及安全保障方面的新挑战。本文提出AION，一种无需依赖外部定位或全局地图、基于视觉的空中目标导航方法。AION采用端到端的双策略强化学习框架，将探索行为与目标抵达行为解耦为两个专用策略。我们在AI2-THOR基准测试中评估了AION，并进一步使用高保真无人机模型在IsaacSim中验证了其实时性能。实验结果表明，AION在探索能力、导航效率及安全性等综合评估指标上均表现出优越性能。演示视频可访问：https://youtu.be/TgsUm6bb7zg。",
    "abstract_en": "Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at https://youtu.be/TgsUm6bb7zg."
  },
  "2601.13976": {
    "title_zh": "FantasyVLN：面向视觉语言导航的统一多模态思维链推理框架",
    "abstract_zh": "在视觉语言导航（VLN）中实现人类水平的表现，要求具身智能体能够同时理解多模态指令与视觉空间上下文，并对长序列动作进行推理。近期研究，如NavCoT与NavGPT-2，展示了思维链（CoT）推理在提升可解释性与长程规划能力方面的潜力。此外，OctoNav-R1和CoT-VLA等多模态扩展进一步验证了CoT作为实现类人导航推理的有效路径。然而，现有方法存在明显缺陷：纯文本CoT缺乏空间基础，易因稀疏标注的推理步骤而过拟合；而多模态CoT通过生成想象的视觉观测导致严重的令牌膨胀，使得实时导航难以实现。本文提出FantasyVLN，一个统一的隐式推理框架，在保留CoT推理优势的同时避免了显式的令牌开销。具体而言，在CoT推理训练中，我们使用预训练的视觉自回归模型（VAR）将想象的视觉令牌编码至紧凑的潜在空间，并通过统一的多CoT策略，使模型能够从文本、视觉及多模态CoT模式中联合学习。在推理阶段，我们的模型直接执行从指令到动作的映射，同时仍受益于推理感知的表征。在LH-VLN数据集上的大量实验表明，该方法实现了兼具推理感知与实时性的导航，不仅提升了成功率与效率，而且相比显式CoT方法，推理延迟降低了一个数量级。",
    "abstract_en": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods."
  },
  "2601.12766": {
    "title_zh": "Spatial-VLN：具备显式空间感知与探索能力的零样本视觉语言导航",
    "abstract_zh": "利用大型语言模型（LLM）的零样本视觉语言导航（VLN）代理在泛化方面表现出色，但存在空间感知不足的问题。针对复杂连续环境，我们将关键感知瓶颈归纳为三类空间挑战：门交互、多房间导航和模糊指令执行，现有方法在这些方面持续面临高失败率。我们提出了Spatial-VLN，一种感知引导的探索框架，旨在克服这些挑战。该框架包含两个核心模块：空间感知增强（SPE）模块通过全景过滤结合专门的门与区域专家，生成空间连贯、跨视图一致的感知表征；在此基础上，探索式多专家推理（EMR）模块利用并行LLM专家处理路径点级语义和区域级空间转换。当专家预测出现分歧时，查询-探索机制被激活，引导代理主动探测关键区域以解决感知模糊性。在VLN-CE上的实验表明，Spatial-VLN仅使用低成本LLM即实现了最先进的性能。此外，为验证实际应用性，我们引入了一种基于价值的路径点采样策略，有效弥合了仿真到现实的差距。大量真实环境评估证实，该框架在复杂环境中具有卓越的泛化能力和鲁棒性。代码与演示视频发布于https://yueluhhxx.github.io/Spatial-VLN-web/。",
    "abstract_en": "Zero-shot Vision-and-Language Navigation (VLN) agents leveraging Large Language Models (LLMs) excel in generalization but suffer from insufficient spatial perception. Focusing on complex continuous environments, we categorize key perceptual bottlenecks into three spatial challenges: door interaction,multi-room navigation, and ambiguous instruction execution, where existing methods consistently suffer high failure rates. We present Spatial-VLN, a perception-guided exploration framework designed to overcome these challenges. The framework consists of two main modules. The Spatial Perception Enhancement (SPE) module integrates panoramic filtering with specialized door and region experts to produce spatially coherent, cross-view consistent perceptual representations. Building on this foundation, our Explored Multi-expert Reasoning (EMR) module uses parallel LLM experts to address waypoint-level semantics and region-level spatial transitions. When discrepancies arise between expert predictions, a query-and-explore mechanism is activated, prompting the agent to actively probe critical areas and resolve perceptual ambiguities. Experiments on VLN-CE demonstrate that Spatial VLN achieves state-of-the-art performance using only low-cost LLMs. Furthermore, to validate real-world applicability, we introduce a value-based waypoint sampling strategy that effectively bridges the Sim2Real gap. Extensive real-world evaluations confirm that our framework delivers superior generalization and robustness in complex environments. Our codes and videos are available at https://yueluhhxx.github.io/Spatial-VLN-web/."
  },
  "2601.09111": {
    "title_zh": "迈向开放环境与指令：基于快慢交互推理的通用视觉语言导航",
    "abstract_zh": "视觉语言导航旨在使智能体能够根据语言指令导航至目标位置。传统的视觉语言导航通常遵循封闭集假设，即训练与测试数据共享相同风格的输入图像和指令。然而，现实世界是开放的，充满了各种未见过的环境，这对封闭集方法构成了巨大挑战。为此，我们聚焦于通用场景适应任务，旨在通过引入多样化的环境和不一致的指令来学习泛化的导航能力。面对这一任务，当遭遇未见环境和指令时，主要挑战在于如何使智能体在导航过程中动态生成泛化策略。近期研究表明，人类通过快慢认知系统能够生成稳定的策略，从而增强对开放世界的适应能力。受此启发，我们提出了慢促快视觉语言导航方法，构建了一个动态交互的快慢推理框架。其中，快速推理模块作为一个端到端的策略网络，通过实时输入输出动作，并在历史存储库中积累执行记录以构建记忆。慢速推理模块则分析快速推理模块生成的记忆，通过深度反思提取能够增强决策泛化能力的经验。这些经验被结构化存储，并用于持续优化快速推理模块。与将快慢推理视为独立机制的传统方法不同，我们的框架实现了快慢交互。通过利用慢速推理产生的经验，这种交互使系统能够持续适应，并在面对未见场景时高效执行导航任务。",
    "abstract_en": "Vision-Language Navigation aims to enable agents to navigate to a target location based on language instructions. Traditional VLN often follows a close-set assumption, i.e., training and test data share the same style of the input images and instructions. However, the real world is open and filled with various unseen environments, posing enormous difficulties for close-set methods. To this end, we focus on the General Scene Adaptation (GSA-VLN) task, aiming to learn generalized navigation ability by introducing diverse environments and inconsistent intructions.Towards this task, when facing unseen environments and instructions, the challenge mainly lies in how to enable the agent to dynamically produce generalized strategies during the navigation process. Recent research indicates that by means of fast and slow cognition systems, human beings could generate stable policies, which strengthen their adaptation for open world. Inspired by this idea, we propose the slow4fast-VLN, establishing a dynamic interactive fast-slow reasoning framework. The fast-reasoning module, an end-to-end strategy network, outputs actions via real-time input. It accumulates execution records in a history repository to build memory. The slow-reasoning module analyze the memories generated by the fast-reasoning module. Through deep reflection, it extracts experiences that enhance the generalization ability of decision-making. These experiences are structurally stored and used to continuously optimize the fast-reasoning module. Unlike traditional methods that treat fast-slow reasoning as independent mechanisms, our framework enables fast-slow interaction. By leveraging the experiences from slow reasoning. This interaction allows the system to continuously adapt and efficiently execute navigation tasks when facing unseen scenarios."
  },
  "2602.02468": {
    "title_zh": "Avenir-Web：基于混合定位专家的人类经验模仿式多模态网络代理",
    "abstract_zh": "尽管多模态大语言模型取得了进展，但自主网络代理在执行复杂动态网页界面的长时程任务时仍难以保证可靠性。现有代理常面临元素定位不准确、缺乏站点特定程序性知识，以及长期任务跟踪与记忆不稳定等问题，尤其在处理复杂的文档对象模型结构时更为突出。为应对这些局限，我们提出了Avenir-Web，一种在网络代理中实现开源新标杆的代理，在真实世界部署的Online-Mind2Web基准测试中表现卓越。Avenir-Web采用混合定位专家机制，通过经验模仿规划融入程序性先验知识，并结合任务跟踪清单与自适应记忆，以实现跨多样用户界面范式的稳健无缝交互。我们在Online-Mind2Web这一严格评估实时用户中心网络任务的基准上对Avenir-Web进行了测试。结果表明，Avenir-Web显著超越了先前的开源代理，并与顶尖专有模型达到性能持平，从而为实时网站上的可靠网络代理确立了新的开源标杆。",
    "abstract_en": "Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites."
  },
  "2602.02465": {
    "title_zh": "MentisOculi：揭示心智意象推理的局限性",
    "abstract_zh": "前沿模型正从仅能接收视觉信息的多模态大语言模型（MLLMs）向能够原生交错生成内容的统一多模态模型（UMMs）转变。这一转变激发了人们利用中间可视化作为推理辅助的兴趣，类似于人类的心智意象。该理念的核心在于以目标为导向形成、维持和操纵视觉表征的能力。为了评估和探究这一能力，我们开发了MentisOculi——一套程序化、分层化的多步推理问题集，适用于视觉化解决方案，并针对前沿模型的挑战进行了优化。通过评估从潜在标记到显式生成图像等多种视觉策略，我们发现它们通常未能提升性能。对UMMs的具体分析揭示了一个关键局限：尽管它们具备解决任务的文本推理能力，有时也能生成正确的视觉内容，但它们受到生成误差累积的影响，甚至无法有效利用真实的可视化信息。我们的研究结果表明，尽管视觉思维具有内在吸引力，但目前尚未对模型推理产生助益。MentisOculi为分析和弥合不同模型家族间的这一差距奠定了必要基础。",
    "abstract_en": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families."
  },
  "2602.02456": {
    "title_zh": "面向任务推理的关系感知分层三维场景图",
    "abstract_zh": "以结构化方式表示和理解三维环境对于自主智能体导航和推理其周围环境至关重要。传统的同步定位与建图（SLAM）方法虽能生成度量重建并可扩展为度量-语义建图，但缺乏更高层次的抽象和关系推理能力。为弥补这一不足，三维场景图作为一种能够捕捉层次结构和物体关系的强大表示方法应运而生。本研究提出了一种增强型分层三维场景图，它在多个抽象层次上整合了开放词汇特征，并支持物体关系推理。我们的方法利用视觉语言模型（VLM）来推断语义关系。特别地，我们引入了一个任务推理模块，该模块结合大型语言模型（LLM）和视觉语言模型（VLM）来解析场景图的语义与关系信息，使智能体能够更智能地进行任务推理并与环境交互。通过在四足机器人上部署该方法于多种环境和任务中，我们验证了其推理能力。",
    "abstract_en": "Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them."
  },
  "2602.02408": {
    "title_zh": "ReasonEdit：基于人类推理的视觉语言模型编辑",
    "abstract_zh": "模型编辑旨在修正大型预训练模型中的错误，同时不影响无关行为。尽管近期已有研究尝试编辑视觉语言模型（VLMs），但尚无现有编辑器能够处理需要人类与模型对图像进行复杂推理的任务。为此，我们提出了ReasonEdit，这是首个允许用户在编辑过程中解释其推理的VLM编辑器，引入了一种新颖且实用的模型编辑框架。ReasonEdit持续将人类推理存储于代码本中，并在推理时通过一种受网络科学启发的新型拓扑平衡多模态嵌入方法，仅检索相关事实。在多个基于推理的视觉问答数据集上对四种VLMs进行测试，ReasonEdit实现了最先进的编辑性能，最终证明在编辑过程中融入人类推理能显著提升编辑的泛化能力。",
    "abstract_en": "Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization."
  },
  "2602.02341": {
    "title_zh": "LongVPO：从锚定线索到自我推理的长视频偏好优化",
    "abstract_zh": "我们提出了LongVPO，一种新颖的两阶段直接偏好优化框架，使短上下文视觉语言模型能够稳健地理解超长视频，无需任何长视频标注。在第一阶段，我们通过将问题锚定到单个短视频片段、与干扰项交错排列，并应用视觉相似性和问题特异性过滤来合成偏好三元组，以减轻位置偏差并确保明确的监督。我们还通过仅评估锚定片段来近似参考模型在长上下文中的评分，从而降低计算开销。在第二阶段，我们在长视频上采用递归字幕生成流程来生成场景级元数据，然后使用大型语言模型构建多片段推理查询和不受偏好的响应，通过多片段推理任务来对齐模型的偏好。仅使用16K个合成示例且无需昂贵的人工标注，LongVPO在多个长视频基准测试中超越了最先进的开源模型，同时保持了强大的短视频性能（例如在MVBench上），为高效的长视频理解提供了一个可扩展的范式。",
    "abstract_en": "We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding."
  },
  "2602.02185": {
    "title_zh": "Vision-DeepResearch基准：重新思考多模态大语言模型的视觉与文本搜索能力",
    "abstract_zh": "多模态大语言模型（MLLMs）已推动视觉问答（VQA）的发展，并支持利用搜索引擎进行复杂视觉-文本事实查找的Vision-DeepResearch系统。然而，评估这些视觉与文本搜索能力仍具挑战，现有基准存在两大局限。首先，现有基准并非以视觉搜索为核心：本需视觉搜索的答案常通过文本问题中的跨文本线索泄露，或可从当前MLLMs的先验世界知识中推断。其次，评估场景过于理想化：在图像搜索方面，所需信息常可通过与完整图像的近似精确匹配获取；而文本搜索则过于直接且挑战性不足。为解决这些问题，我们构建了包含2000个VQA实例的Vision-DeepResearch基准（VDR-Bench）。所有问题均通过细致多阶段筛选流程与严格专家评审创建，旨在评估Vision-DeepResearch系统在真实世界条件下的表现。此外，针对当前MLLMs视觉检索能力不足的问题，我们提出一种简单的多轮裁剪搜索工作流程。该策略被证明能有效提升模型在真实视觉检索场景中的性能。总体而言，我们的研究结果为未来多模态深度研究系统的设计提供了实用指导。代码将在https://github.com/Osilly/Vision-DeepResearch发布。",
    "abstract_en": "Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch."
  },
  "2602.02043": {
    "title_zh": "Auto-Comp：面向对比式视觉语言模型可扩展组合性探测的自动化流程",
    "abstract_zh": "现代视觉语言模型在组合推理方面存在关键缺陷，常将‘红色立方体和蓝色球体’与‘蓝色立方体和红色球体’混淆。解构这些失败的视觉与语言根源是鲁棒性评估的根本挑战。为实现细粒度、可控的分析，我们引入了Auto-Comp，一个用于生成可扩展基准测试的完全自动化合成流程。其可控特性是剖析和隔离不同推理技能的关键。Auto-Comp从最小化描述（如‘白色背景上一辆自行车左侧的显示器’）和LLM生成的上下文描述（如‘在明亮摄影棚中，显示器位于自行车左侧’）生成配对图像，通过受控的A/B测试来分离核心绑定能力与视觉语言复杂性。我们对20个视觉语言模型在颜色绑定和空间关系新基准上的评估显示，CLIP和SigLIP模型家族普遍存在组合性失败。关键的是，我们新颖的‘混淆基准’揭示了超越简单属性交换的更深层缺陷：模型极易受低熵干扰项（如重复物体或颜色）影响，表明其组合性失败超出了已知的词袋模型限制。我们揭示了一个令人惊讶的权衡：提供全局场景线索的视觉语言上下文虽有助于空间推理，但同时会因引入视觉杂乱而阻碍局部属性绑定。我们发布Auto-Comp流程以促进未来基准测试的创建，并附上所有生成的基准测试集（https://huggingface.co/AutoComp）。",
    "abstract_en": "Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing \"a red cube and a blue sphere\" with \"a blue cube and a red sphere\". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., \"a monitor to the left of a bicycle on a white background\") and LLM-generated Contextual captions (e.g., \"In a brightly lit photography studio, a monitor is positioned to the left of a bicycle\"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel \"Confusion Benchmark\" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp)."
  },
  "2602.02033": {
    "title_zh": "一图多配：在大规模广告图像生成中协调多样化的群体点击偏好",
    "abstract_zh": "广告图像生成日益关注点击率等在线指标，但现有方法采用“一刀切”策略，仅优化整体点击率，忽视了用户群体间的偏好多样性。这导致特定群体的表现欠佳，限制了定向营销的效果。为弥补这一差距，我们提出了《一图多配》统一框架，旨在协调大规模广告图像生成中多样化的群体点击偏好。该框架首先进行产品感知的自适应分组，根据用户属性和产品特征动态组织用户，并用丰富的集体偏好特征表示每个群体。基于这些分组，偏好条件图像生成采用群体感知多模态大语言模型，为每个群体生成定制化图像。该模型经过预训练，能同时理解群体特征并生成广告图像。随后，我们使用提出的群体偏好优化方法对模型进行微调，以协调群体偏好，有效提升各群体在生成图像上的点击率。为推进该领域发展，我们引入了分组广告图像偏好数据集，这是首个大规模公开的群体图像偏好数据集，包含基于4000万用户构建的约60万个群体。大量实验表明，我们的框架在离线和在线场景下均实现了最先进的性能。代码和数据集将在https://github.com/JD-GenX/OSMF发布。",
    "abstract_en": "Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all\" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specific groups, limiting targeted marketing effectiveness. To bridge this gap, we present \\textit{One Size, Many Fits} (OSMF), a unified framework that aligns diverse group-wise click preferences in large-scale advertising image generation. OSMF begins with product-aware adaptive grouping, which dynamically organizes users based on their attributes and product characteristics, representing each group with rich collective preference features. Building on these groups, preference-conditioned image generation employs a Group-aware Multimodal Large Language Model (G-MLLM) to generate tailored images for each group. The G-MLLM is pre-trained to simultaneously comprehend group features and generate advertising images. Subsequently, we fine-tune the G-MLLM using our proposed Group-DPO for group-wise preference alignment, which effectively enhances each group's CTR on the generated images. To further advance this field, we introduce the Grouped Advertising Image Preference Dataset (GAIP), the first large-scale public dataset of group-wise image preferences, including around 600K groups built from 40M users. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance in both offline and online settings. Our code and datasets will be released at https://github.com/JD-GenX/OSMF."
  },
  "2602.04880": {
    "title_zh": "捕捉视觉环境结构与控制性能的相关性",
    "abstract_zh": "视觉表示的选择是扩展通用机器人策略的关键。然而，即使是在仿真环境中，通过策略部署进行直接评估的成本也很高。现有的代理指标侧重于表示捕捉视觉世界狭窄方面的能力，如物体形状，这限制了跨环境的泛化能力。在本文中，我们采取分析视角：通过测量预训练视觉编码器从图像中解码环境状态（包括几何结构、物体结构和物理属性）的能力，来探究这些编码器。利用能够获取真实状态信息的仿真环境，我们证明了这种探究精度与跨不同环境和学习设置的下游策略性能高度相关，显著优于先前的指标，并实现了高效的表示选择。更广泛地说，我们的研究为支持可泛化操作的表示属性提供了见解，表明学习编码环境的潜在物理状态是实现控制的一个有前景的目标。",
    "abstract_en": "The choice of visual representation is key to scaling generalist robot policies. However, direct evaluation via policy rollouts is expensive, even in simulation. Existing proxy metrics focus on the representation's capacity to capture narrow aspects of the visual world, like object shape, limiting generalization across environments. In this paper, we take an analytical perspective: we probe pretrained visual encoders by measuring how well they support decoding of environment state -- including geometry, object structure, and physical attributes -- from images. Leveraging simulation environments with access to ground-truth state, we show that this probing accuracy strongly correlates with downstream policy performance across diverse environments and learning settings, significantly outperforming prior metrics and enabling efficient representation selection. More broadly, our study provides insight into the representational properties that support generalizable manipulation, suggesting that learning to encode the latent physical state of the environment is a promising objective for control."
  },
  "2602.04877": {
    "title_zh": "CoWTracker：通过变形而非相关性进行跟踪",
    "abstract_zh": "密集点跟踪是计算机视觉中的一个基础问题，其应用范围从视频分析到机器人操作。当前最先进的跟踪器通常依赖成本体积来跨帧匹配特征，但这种方法在空间分辨率上具有二次复杂度，限制了可扩展性和效率。本文提出了一种新颖的密集点跟踪器——CoWTracker，它摒弃了成本体积，转而采用变形方法。受光流领域最新进展的启发，我们的方法基于当前估计，通过将目标帧的特征变形到查询帧，迭代地优化跟踪估计。结合一个在所有轨迹上进行联合时空推理的Transformer架构，我们的设计无需计算特征相关性即可建立长距离对应关系。该模型结构简洁，在标准密集点跟踪基准测试（包括TAP-Vid-DAVIS、TAP-Vid-Kinetics和Robo-TAP）中达到了最先进的性能。值得注意的是，该模型在光流估计方面也表现出色，有时在Sintel、KITTI和Spring基准测试中甚至超越了专用方法。这些结果表明，基于变形的架构可以统一密集点跟踪和光流估计。",
    "abstract_en": "Dense point tracking is a fundamental problem in computer vision, with applications ranging from video analysis to robotic manipulation. State-of-the-art trackers typically rely on cost volumes to match features across frames, but this approach incurs quadratic complexity in spatial resolution, limiting scalability and efficiency. In this paper, we propose \\method, a novel dense point tracker that eschews cost volumes in favor of warping. Inspired by recent advances in optical flow, our approach iteratively refines track estimates by warping features from the target frame to the query frame based on the current estimate. Combined with a transformer architecture that performs joint spatiotemporal reasoning across all tracks, our design establishes long-range correspondences without computing feature correlations. Our model is simple and achieves state-of-the-art performance on standard dense point tracking benchmarks, including TAP-Vid-DAVIS, TAP-Vid-Kinetics, and Robo-TAP. Remarkably, the model also excels at optical flow, sometimes outperforming specialized methods on the Sintel, KITTI, and Spring benchmarks. These results suggest that warping-based architectures can unify dense point tracking and optical flow estimation."
  },
  "2602.04635": {
    "title_zh": "面向自然语言指令中物体定位的关系场景图",
    "abstract_zh": "随着机器人在人类环境中的应用日益广泛，自然的人机交互需求愈发迫切。然而，理解自然语言指令要求机器人推断预期任务、将其分解为可执行动作，并将这些动作基于机器人对环境（包括相关物体、智能体和位置）的认知进行定位。这一挑战可通过结合大语言模型（LLMs）理解自然语言的能力与三维场景图（3DSGs）在环境语义表征中定位推断动作的能力来解决。然而，许多3DSGs缺乏物体间的显式空间关系，尽管人类在描述环境时常常依赖这些关系。本文探讨了将开放或封闭词汇的空间关系融入3DSGs是否能提升LLMs解释自然语言指令的能力。为此，我们提出了一种基于LLM的管道，用于从开放词汇语言指令中定位目标物体，以及一种基于视觉语言模型（VLM）的管道，用于从建图过程中捕获的图像向3DSGs添加开放词汇空间边。最后，通过一项研究评估了两种LLMs在目标物体定位下游任务中的表现。我们的研究表明，显式空间关系能有效提升LLMs的物体定位能力。此外，基于VLM的开放词汇关系生成在机器人捕获图像中具有可行性，但其相较于封闭词汇关系的优势较为有限。",
    "abstract_en": "Robots are finding wider adoption in human environments, increasing the need for natural human-robot interaction. However, understanding a natural language command requires the robot to infer the intended task and how to decompose it into executable actions, and to ground those actions in the robot's knowledge of the environment, including relevant objects, agents, and locations. This challenge can be addressed by combining the capabilities of Large language models (LLMs) to understand natural language with 3D scene graphs (3DSGs) for grounding inferred actions in a semantic representation of the environment. However, many 3DSGs lack explicit spatial relations between objects, even though humans often rely on these relations to describe an environment. This paper investigates whether incorporating open- or closed-vocabulary spatial relations into 3DSGs can improve the ability of LLMs to interpret natural language commands. To address this, we propose an LLM-based pipeline for target object grounding from open-vocabulary language commands and a vision language model (VLM)-based pipeline to add open-vocabulary spatial edges to 3DSGs from images captured while mapping. Finally, two LLMs are evaluated in a study assessing their performance on the downstream task of target object grounding. Our study demonstrates that explicit spatial relations improve the ability of LLMs to ground objects. Moreover, open-vocabulary relation generation with VLMs proves feasible from robot-captured images, but their advantage over closed-vocabulary relations is found to be limited."
  },
  "2602.04600": {
    "title_zh": "行动、感知、再行动：从大规模第一人称人类数据中学习非马尔可夫主动感知策略",
    "abstract_zh": "在无约束环境中实现泛化性操作要求机器人能够主动解决信息不确定性，即具备主动感知能力。然而，现有方法通常局限于有限的感知行为类型，限制了其在复杂环境中的适用性。本研究将主动感知形式化为一个由信息增益和决策分支驱动的非马尔可夫过程，并提出了视觉主动感知范式的结构化分类。基于这一视角，我们提出了CoMe-VLA框架——一种融合认知与记忆的视觉-语言-动作框架，该框架利用大规模人类第一人称数据来学习多功能的探索与操作先验。我们的框架集成了一个用于自主子任务转换的认知辅助头模块，以及一个通过融合本体感觉与视觉时序上下文来维持自我与环境一致感知的双轨记忆系统。通过将人类与机器人的手眼协调行为对齐到统一的第一人称动作空间中，我们分三个阶段逐步训练模型。在轮式人形机器人上进行的广泛实验表明，该方法在跨越多种主动感知场景的多样化长时程任务中展现出强大的鲁棒性与适应性。",
    "abstract_en": "Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios."
  },
  "2602.04522": {
    "title_zh": "基于互补性的统一方法在刚体操作与运动预测中的应用",
    "abstract_zh": "在非结构化环境中的机器人操作要求规划器能够同时推理自由空间运动与环境中持续的摩擦接触。现有的（局部）规划与仿真框架通常将这些状态分离，或依赖于简化的接触表示，尤其是在建模非凸或分布式接触区域时。这种近似限制了接触模式转换的保真度，并阻碍了在实时条件下稳健执行富含接触行为的任务。本文提出了一种统一的离散时间建模框架（Unicomp），用于机器人操作，该框架在单一数学形式中一致地捕捉自由运动与摩擦接触。基于互补性的刚体动力学，我们将自由空间运动与接触交互建模为耦合的线性和非线性互补问题，从而在不强制固定接触假设的情况下实现接触模式之间的原则性转换。对于平面区域接触，我们从最大功率耗散原理推导出一个摩擦接触模型，其中可接受的接触力矩集合由椭球极限曲面表示。这种表示捕捉了耦合的力-力矩效应，包括扭转摩擦，同时不依赖于接触区域底层的压力分布。由此产生的公式产生了一个离散时间预测模型，该模型通过二次约束关联广义速度与接触力矩，适用于基于优化的实时规划。实验结果表明，所提出的方法能够在交互速度下实现稳定、物理一致的行为，覆盖从平面推动到富含接触的全身操作等多种任务。",
    "abstract_en": "Robotic manipulation in unstructured environments requires planners to reason jointly about free-space motion and sustained, frictional contact with the environment. Existing (local) planning and simulation frameworks typically separate these regimes or rely on simplified contact representations, particularly when modeling non-convex or distributed contact patches. Such approximations limit the fidelity of contact-mode transitions and hinder the robust execution of contact-rich behaviors in real time. This paper presents a unified discrete-time modeling framework for robotic manipulation that consistently captures both free motion and frictional contact within a single mathematical formalism (Unicomp). Building on complementarity-based rigid-body dynamics, we formulate free-space motion and contact interactions as coupled linear and nonlinear complementarity problems, enabling principled transitions between contact modes without enforcing fixed-contact assumptions. For planar patch contact, we derive a frictional contact model from the maximum power dissipation principle in which the set of admissible contact wrenches is represented by an ellipsoidal limit surface. This representation captures coupled force-moment effects, including torsional friction, while remaining agnostic to the underlying pressure distribution across the contact patch. The resulting formulation yields a discrete-time predictive model that relates generalized velocities and contact wrenches through quadratic constraints and is suitable for real-time optimization-based planning. Experimental results show that the proposed approach enables stable, physically consistent behavior at interactive speeds across tasks, from planar pushing to contact-rich whole-body maneuvers."
  },
  "2602.04515": {
    "title_zh": "EgoActor：通过视觉语言模型将任务规划落地为具身机器人的空间感知自我中心动作",
    "abstract_zh": "在现实世界中部署具身机器人面临根本性挑战，因为它需要在部分信息观测和动态变化的环境下，紧密整合感知、移动和操作能力，并能在不同类型子任务间稳健切换。为应对这些挑战，我们提出一项新颖任务——EgoActing，要求将高层指令直接转化为多样、精确且具备空间感知的机器人动作。我们进一步通过引入EgoActor来实例化该任务，这是一个统一且可扩展的视觉语言模型，能够实时协调感知与执行，预测移动基元（如行走、转向、侧移、高度调整）、头部运动、操作指令以及人机交互动作。我们利用来自真实世界演示的纯RGB自我中心视角数据、空间推理问答以及仿真环境演示的广泛监督，使EgoActor能够做出稳健的情境感知决策，并以8B和4B参数模型实现流畅的动作推断（耗时低于1秒）。在仿真和真实环境中的大量评估表明，EgoActor有效桥接了抽象任务规划与具体运动执行，并能泛化至多样化任务及未见过的环境。",
    "abstract_en": "Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments."
  },
  "2602.04411": {
    "title_zh": "自演化的具身人工智能",
    "abstract_zh": "具身人工智能是由智能体与其环境通过主动感知、具身认知与行动交互形成的智能系统。现有具身人工智能仍局限于人工设定的框架，其中智能体基于给定记忆进行训练，并为特定任务构建模型，使固定具身形态与相对静态环境交互。此类方法难以适应具有可变具身形态和动态开放环境的真实世界场景。本文提出自演化具身人工智能这一新范式，智能体能够根据自身状态与环境变化，通过记忆自我更新、任务自主切换、环境自我预测、具身自适应以及模型自演化实现自主运作，旨在达成持续自适应与自主进化的智能。具体而言，我们阐述了自演化具身人工智能的定义、框架、组件与机制，系统综述了已实现组件的先进研究成果，探讨了实际应用场景，并指出了未来研究方向。我们相信，自演化具身人工智能能使智能体以类人方式自主学习和与环境交互，并为实现通用人工智能提供全新视角。",
    "abstract_en": "Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence."
  },
  "2602.04315": {
    "title_zh": "GeneralVLA：具备知识引导轨迹规划的通用视觉-语言-动作模型",
    "abstract_zh": "大型基础模型已在视觉和语言领域展现出对复杂问题的强大开放世界泛化能力，但机器人学中尚未实现类似水平的泛化。一个根本性挑战在于这些模型表现出有限的零样本能力，这阻碍了它们有效泛化至未见场景。本研究提出GeneralVLA（具备知识引导轨迹规划的通用视觉-语言-动作模型），这是一种分层视觉-语言-动作模型，能更有效地利用基础模型的泛化能力，实现零样本操控并自动生成机器人学数据。具体而言，我们研究一类分层VLA模型：高层ASM（可供性分割模块）经微调后感知场景的图像关键点可供性；中层3DAgent执行任务理解、技能知识与轨迹规划，生成指示期望机器人末端执行器轨迹的三维路径。该中间三维路径预测随后作为低层三维感知控制策略的引导，实现精确操控。相较于其他方法，我们的方法无需真实世界机器人数据收集或人工示范，使其能更高效地扩展至多样化任务与视角。实验表明，GeneralVLA成功为14项任务生成轨迹，显著优于VoxPoser等先进方法。所生成的示范数据训练出的行为克隆策略，比基于人工示范或VoxPoser、Scaling-up、Code-As-Policies生成数据训练的策略更具鲁棒性。我们相信GeneralVLA可成为兼具机器人数据生成与零样本场景下解决新任务能力的可扩展方法。代码：https://github.com/AIGeeksGroup/GeneralVLA。项目网站：https://aigeeksgroup.github.io/GeneralVLA。",
    "abstract_en": "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: https://github.com/AIGeeksGroup/GeneralVLA. Website: https://aigeeksgroup.github.io/GeneralVLA."
  },
  "2602.04243": {
    "title_zh": "视角至关重要：利用掩码自编码器动态优化视觉操控的视角",
    "abstract_zh": "机器人操控仍面临挑战，而模仿学习（IL）使机器人能够从专家演示中学习任务。当前的IL方法通常依赖于固定的相机设置，即相机被手动放置在静态位置，这极大地限制了系统的适应性和覆盖范围。受人类主动感知的启发——人类会动态调整视角以捕捉最相关且噪声最少的信息，我们提出了MAE-Select，一种用于单相机机器人系统中主动视角选择的新颖框架。MAE-Select充分利用了预训练的多视角掩码自编码器表示，并在每个时间块动态选择下一个最具信息量的视角，无需标注视角数据。大量实验表明，MAE-Select提升了单相机系统的能力，在某些情况下甚至超越了多相机设置。项目将在https://mae-select.github.io上公开。",
    "abstract_en": "Robotic manipulation continues to be a challenge, and imitation learning (IL) enables robots to learn tasks from expert demonstrations. Current IL methods typically rely on fixed camera setups, where cameras are manually positioned in static locations, imposing significant limitations on adaptability and coverage. Inspired by human active perception, where humans dynamically adjust their viewpoint to capture the most relevant and least noisy information, we propose MAE-Select, a novel framework for active viewpoint selection in single-camera robotic systems. MAE-Select fully leverages pre-trained multi-view masked autoencoder representations and dynamically selects the next most informative viewpoint at each time chunk without requiring labeled viewpoints. Extensive experiments demonstrate that MAE-Select improves the capabilities of single-camera systems and, in some cases, even surpasses multi-camera setups. The project will be available at https://mae-select.github.io."
  },
  "2602.04231": {
    "title_zh": "GeoLanG：基于统一RGB-D多模态学习的几何感知语言引导抓取",
    "abstract_zh": "语言引导抓取作为一种通过自然语言指令使机器人识别和操作目标对象的有前景范式，但在杂乱或遮挡场景中仍面临巨大挑战。现有方法通常依赖将物体感知与抓取分离的多阶段流程，导致跨模态融合有限、计算冗余，且在杂乱、遮挡或低纹理场景中泛化能力差。为应对这些局限，我们提出了GeoLanG，这是一个基于CLIP架构构建的端到端多任务框架，它将视觉和语言输入统一到共享表示空间中，以实现鲁棒的语义对齐和增强的泛化能力。为提升遮挡和低纹理条件下的目标辨别力，我们通过深度引导几何模块（DGGM）探索了更有效的深度信息利用方式，该模块将深度转换为显式几何先验，并在不增加额外计算开销的情况下将其注入注意力机制。此外，我们提出了自适应密集通道集成方法，可自适应平衡多层特征的贡献，以生成更具区分性和泛化能力的视觉表示。在OCID-VLG数据集以及仿真和真实硬件上的大量实验表明，GeoLanG能够在复杂、杂乱的环境中实现精确且鲁棒的语言引导抓取，为在真实世界以人为中心的环境中实现更可靠的多模态机器人操作铺平了道路。",
    "abstract_en": "Language-guided grasping has emerged as a promising paradigm for enabling robots to identify and manipulate target objects through natural language instructions, yet it remains highly challenging in cluttered or occluded scenes. Existing methods often rely on multi-stage pipelines that separate object perception and grasping, which leads to limited cross-modal fusion, redundant computation, and poor generalization in cluttered, occluded, or low-texture scenes. To address these limitations, we propose GeoLanG, an end-to-end multi-task framework built upon the CLIP architecture that unifies visual and linguistic inputs into a shared representation space for robust semantic alignment and improved generalization. To enhance target discrimination under occlusion and low-texture conditions, we explore a more effective use of depth information through the Depth-guided Geometric Module (DGGM), which converts depth into explicit geometric priors and injects them into the attention mechanism without additional computational overhead. In addition, we propose Adaptive Dense Channel Integration, which adaptively balances the contributions of multi-layer features to produce more discriminative and generalizable visual representations. Extensive experiments on the OCID-VLG dataset, as well as in both simulation and real-world hardware, demonstrate that GeoLanG enables precise and robust language-guided grasping in complex, cluttered environments, paving the way toward more reliable multimodal robotic manipulation in real-world human-centric settings."
  },
  "2602.04864": {
    "title_zh": "当LLaVA遇见物体：视觉语言模型的令牌组合",
    "abstract_zh": "当前的自回归视觉语言模型通常依赖大量视觉令牌来表示图像，导致在推理时尤其需要更多计算资源。为解决这一问题，我们提出了Mask-LLaVA框架，该框架利用不同层级的视觉特征，为自回归视觉语言模型创建紧凑且信息丰富的视觉表示。具体而言，我们将基于掩码的物体表示与全局令牌和局部补丁令牌相结合。尽管训练时使用所有令牌，但结果表明，所得模型在测试时能够灵活地减少特别是基于掩码的物体令牌数量，从而允许在推理过程中调整令牌数量，而无需重新训练模型且性能不会显著下降。我们在标准基准测试套件上评估了所提方法，结果显示其与当前令牌高效方法竞争激烈，且仅使用一小部分视觉令牌即可达到与原始LLaVA基线相当的性能。我们的分析表明，结合多层级特征能够以更少的令牌实现高效学习，同时允许在测试时动态选择令牌以保持良好的性能。",
    "abstract_en": "Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance."
  },
  "2602.04849": {
    "title_zh": "结构智能体：一种人工智能分子编辑器",
    "abstract_zh": "我们介绍了结构智能体（El Agente Estructural），这是一种多模态、自然语言驱动的几何生成与操作智能体，专为自主化学与分子建模而设计。与通过生成模型进行分子生成或编辑不同，结构智能体通过整合一套全面的领域知识工具和视觉语言模型，模拟人类专家在三维空间中直接操作分子系统的方式。这种设计使得无需重建庞大的核心分子框架，即可实现对原子或官能团替换、原子连接性以及立体化学的精确控制。通过一系列代表性案例研究，我们展示了结构智能体能够在广泛的现实场景中实现具有化学意义的几何操作。这些场景包括位点选择性功能化、配体结合、配体交换、立体化学控制的结构构建、异构体互变、片段级结构分析、基于图像从示意反应机理生成结构，以及机理驱动的几何生成与修改。这些示例说明了多模态推理与专业几何感知工具相结合，如何支持超越结构生成的交互式、情境感知的分子建模。展望未来，将结构智能体集成到自主多智能体量子化学平台El Agente Quntur中，通过添加用于三维结构生成和编辑的复杂工具，进一步增强了其能力。",
    "abstract_en": "We present El Agente Estructural, a multimodal, natural-language-driven geometry-generation and manipulation agent for autonomous chemistry and molecular modelling. Unlike molecular generation or editing via generative models, Estructural mimics how human experts directly manipulate molecular systems in three dimensions by integrating a comprehensive set of domain-informed tools and vision-language models. This design enables precise control over atomic or functional group replacements, atomic connectivity, and stereochemistry without the need to rebuild extensive core molecular frameworks. Through a series of representative case studies, we demonstrate that Estructural enables chemically meaningful geometry manipulation across a wide range of real-world scenarios. These include site-selective functionalization, ligand binding, ligand exchange, stereochemically controlled structure construction, isomer interconversion, fragment-level structural analysis, image-guided generation of structures from schematic reaction mechanisms, and mechanism-driven geometry generation and modification. These examples illustrate how multimodal reasoning, when combined with specialized geometry-aware tools, supports interactive and context-aware molecular modelling beyond structure generation. Looking forward, the integration of Estructural into El Agente Quntur, an autonomous multi-agent quantum chemistry platform, enhances its capabilities by adding sophisticated tools for the generation and editing of three-dimensional structures."
  },
  "2602.04802": {
    "title_zh": "VISTA-Bench：视觉语言模型真的能像理解纯文本一样理解图像中的文本吗？",
    "abstract_zh": "视觉语言模型（VLMs）在跨文本与视觉输入的多模态理解方面取得了显著成就，但现有基准测试主要关注纯文本查询。在现实场景中，语言也常以图像中嵌入的可视化文本形式出现，这引发了一个问题：当前的VLMs是否能同等处理这类输入请求。我们推出了VISTA-Bench，这是一个从多模态感知、推理到单模态理解领域的系统性基准测试。它通过在受控渲染条件下对比纯文本与可视化文本问题，评估模型对可视化文本的理解能力。对超过20个代表性VLMs的广泛评估揭示了一个显著的模态差距：在纯文本查询上表现良好的模型，当相同语义内容以可视化文本呈现时，性能往往大幅下降。这种差距随着感知难度的增加而进一步放大，突显了模型对渲染变化的敏感性，尽管语义内容保持不变。总体而言，VISTA-Bench提供了一个原则性的评估框架，用于诊断这一局限性，并指导在标记化文本与像素之间实现更统一语言表征的进展。源数据集可在https://github.com/QingAnLiu/VISTA-Bench获取。",
    "abstract_en": "Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench."
  },
  "2602.04739": {
    "title_zh": "多模态大语言模型中的对齐漂移：对八个模型版本有害性的两阶段纵向评估",
    "abstract_zh": "多模态大语言模型（MLLMs）正越来越多地部署于实际系统中，但其在对抗性提示下的安全性仍缺乏深入探究。我们采用一个由26名专业红队人员编写的固定基准测试集（包含726个对抗性提示），对MLLMs的有害性进行了两阶段评估。第一阶段评估了GPT-4o、Claude Sonnet 3.5、Pixtral 12B和Qwen VL Plus；第二阶段评估了它们的后续版本（GPT-5、Claude Sonnet 4.5、Pixtral Large和Qwen Omni），共获得82,256个人类有害性评分。不同模型系列间存在显著且持续的差异：Pixtral模型始终最为脆弱，而Claude模型因高拒绝率显得最安全。攻击成功率（ASR）显示出明显的对齐漂移现象：GPT和Claude模型在代际更新中ASR有所上升，而Pixtral和Qwen则呈现小幅下降。模态效应也随时间变化：第一阶段中纯文本提示更为有效，而第二阶段则出现模型特定的模式，GPT-5和Claude 4.5在不同模态下表现出近乎同等的脆弱性。这些发现表明，MLLMs的有害性在模型更新中既非一致也非稳定，凸显了需要建立纵向、多模态的基准测试来追踪其安全行为的演变。",
    "abstract_en": "Multimodal large language models (MLLMs) are increasingly deployed in real-world systems, yet their safety under adversarial prompting remains underexplored. We present a two-phase evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red teamers. Phase 1 assessed GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus; Phase 2 evaluated their successors (GPT-5, Claude Sonnet 4.5, Pixtral Large, and Qwen Omni) yielding 82,256 human harm ratings. Large, persistent differences emerged across model families: Pixtral models were consistently the most vulnerable, whereas Claude models appeared safest due to high refusal rates. Attack success rates (ASR) showed clear alignment drift: GPT and Claude models exhibited increased ASR across generations, while Pixtral and Qwen showed modest decreases. Modality effects also shifted over time: text-only prompts were more effective in Phase 1, whereas Phase 2 produced model-specific patterns, with GPT-5 and Claude 4.5 showing near-equivalent vulnerability across modalities. These findings demonstrate that MLLM harmlessness is neither uniform nor stable across updates, underscoring the need for longitudinal, multimodal benchmarks to track evolving safety behaviour."
  },
  "2602.04712": {
    "title_zh": "SAR-RAG：通过语义搜索、检索与多模态大语言模型生成的自动目标识别视觉问答",
    "abstract_zh": "本文提出了一种视觉上下文图像检索增强生成（ImageRAG）辅助的AI智能体，用于合成孔径雷达（SAR）的自动目标识别（ATR）。SAR是一种应用于国防与安全领域的遥感方法，用于检测和监控军事车辆的位置，这些车辆在图像中可能难以区分。研究人员已广泛研究SAR ATR，以提升对车辆类型、特征及尺寸的区分与识别能力。测试样本可与已知车辆目标类型进行比较，从而改进识别任务。新方法增强了神经网络、Transformer注意力机制及多模态大语言模型的能力。我们开发了一种智能AI方法，利用一组定义的工具，例如在相似样本库中进行搜索。我们提出的方法——SAR检索增强生成（SAR-RAG），将多模态大语言模型（MLLM）与语义嵌入向量数据库相结合，支持对具有已知特性的图像范例进行上下文搜索。通过检索具有已知真实目标类型的过往图像样本，SAR-RAG系统能够比较相似的车辆类别，从而提升ATR预测精度。我们通过搜索与检索指标、分类准确率以及车辆尺寸的数值回归进行评估。这些指标均表明，将SAR-RAG作为附加的ATR记忆库集成到MLLM基线方法中后，性能得到全面提升。",
    "abstract_en": "We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank."
  },
  "2602.04699": {
    "title_zh": "基于视觉语言模型的无标注航天器检测与分割",
    "abstract_zh": "视觉语言模型在开放世界零样本视觉识别任务中展现出卓越性能，但其在航天领域的应用潜力尚未得到充分探索。在航天场景中，由于低可见度、光照变化及目标与行星背景融合等因素，精确的人工标注尤为困难。因此，开发无需大量人工标注即可检测与分割航天器及轨道目标的方法至关重要。本研究提出一种基于视觉语言模型的无标注空间目标检测与分割流程。该方法首先利用预训练的视觉语言模型自动为少量未标注真实数据生成伪标签，随后通过师生标签蒸馏框架，利用这些伪标签训练轻量化模型。尽管伪标签本身存在固有噪声，但蒸馏过程相较于直接零样本视觉语言模型推理仍带来显著的性能提升。在SPARK-2024、SPEED+和TANGO数据集上的分割任务实验评估表明，平均精度最高可提升10个百分点。代码与模型已发布于https://github.com/giddyyupp/annotation-free-spacecraft-segmentation。",
    "abstract_en": "Vision Language Models (VLMs) have demonstrated remarkable performance in open-world zero-shot visual recognition. However, their potential in space-related applications remains largely unexplored. In the space domain, accurate manual annotation is particularly challenging due to factors such as low visibility, illumination variations, and object blending with planetary backgrounds. Developing methods that can detect and segment spacecraft and orbital targets without requiring extensive manual labeling is therefore of critical importance. In this work, we propose an annotation-free detection and segmentation pipeline for space targets using VLMs. Our approach begins by automatically generating pseudo-labels for a small subset of unlabeled real data with a pre-trained VLM. These pseudo-labels are then leveraged in a teacher-student label distillation framework to train lightweight models. Despite the inherent noise in the pseudo-labels, the distillation process leads to substantial performance gains over direct zero-shot VLM inference. Experimental evaluations on the SPARK-2024, SPEED+, and TANGO datasets on segmentation tasks demonstrate consistent improvements in average precision (AP) by up to 10 points. Code and models are available at https://github.com/giddyyupp/annotation-free-spacecraft-segmentation."
  },
  "2602.04672": {
    "title_zh": "AGILE：基于智能体生成从视频重建手-物交互",
    "abstract_zh": "从单目视频重建动态的手-物交互对于灵巧操作数据收集以及为机器人与虚拟现实创建逼真的数字孪生至关重要。然而，现有方法面临两大障碍：(1) 依赖神经渲染常导致在严重遮挡下产生碎片化、无法直接用于仿真的几何体；(2) 依赖脆弱的运动恢复结构初始化，导致对野外拍摄视频频繁失败。为克服这些局限，我们提出了AGILE，一个将交互学习范式从重建转向智能体生成的鲁棒框架。首先，我们采用智能体流程，通过视觉语言模型引导生成模型合成完整、封闭的高保真纹理物体网格，不受视频遮挡影响。其次，完全绕过脆弱的运动恢复结构，我们提出一种鲁棒的锚定-跟踪策略：利用基础模型在单个交互起始帧初始化物体姿态，并通过生成资产与视频观测间的强视觉相似性进行时序传播。最后，通过接触感知优化整合语义、几何与交互稳定性约束，确保物理合理性。在HO3D、DexYCB及野外视频上的大量实验表明，AGILE在全局几何精度上超越基线方法，并在现有方法常失效的挑战性序列中展现出卓越的鲁棒性。通过优先保证物理有效性，我们的方法可生成可直接用于仿真的资产，并已通过真实到仿真的重定向在机器人应用中验证。",
    "abstract_en": "Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications."
  },
  "2602.04657": {
    "title_zh": "PIO-FVLM：从推理目标视角重新审视用于VLM加速的无训练视觉令牌缩减",
    "abstract_zh": "近年来，通过减少视觉-语言模型（VLMs）中的冗余视觉令牌以加速VLM推理已成为热门话题。然而，现有方法大多依赖于基于视觉令牌间相似性或跨模态视觉-文本相似性构建的启发式规则，这导致其在压缩性能和实际部署中存在一定局限性。相比之下，我们从推理目标的角度出发，提出了PIO-FVLM，将视觉令牌压缩转化为保持输出结果不变性的问题，并依据令牌对此目标的重要性进行筛选。具体而言，我们通过设计的层局部代理损失（一种从当前层到最终结果的粗略约束）生成令牌级梯度显著性，并以此指导视觉令牌重新排序。随后，遵循非极大值抑制（NMS）原则选取最有价值的视觉令牌。所提出的PIO-FVLM无需训练，且与FlashAttention兼容，便于实际应用与部署。它可作为无编码器方法独立部署，也可与VisionZip等编码器压缩方法结合，作为编码器参与的方法使用。在LLaVA-Next-7B模型上，PIO-FVLM仅保留11.1%的视觉令牌，却能维持97.2%的原始性能，同时实现预填充速度提升2.67倍、推理速度提升2.11倍、计算量（FLOPs）降低6.22倍，并减少6.05倍的KV缓存开销。代码已开源：https://github.com/ocy1/PIO-FVLM。",
    "abstract_en": "Recently, reducing redundant visual tokens in vision-language models (VLMs) to accelerate VLM inference has emerged as a hot topic. However, most existing methods rely on heuristics constructed based on inter-visual-token similarity or cross-modal visual-text similarity, which gives rise to certain limitations in compression performance and practical deployment. In contrast, we propose PIO-FVLM from the perspective of inference objectives, which transforms visual token compression into preserving output result invariance and selects tokens primarily by their importance to this goal. Specially, vision tokens are reordered with the guidance of token-level gradient saliency generated by our designed layer-local proxy loss, a coarse constraint from the current layer to the final result. Then the most valuable vision tokens are selected following the non-maximum suppression (NMS) principle. The proposed PIO-FVLM is training-free and compatible with FlashAttention, friendly to practical application and deployment. It can be deployed independently as an encoder-free method, or combined with encoder compression approaches like VisionZip for use as an encoder-involved method. On LLaVA-Next-7B, PIO-FVLM retains just 11.1% of visual tokens but maintains 97.2% of the original performance, with a 2.67$\\times$ prefill speedup, 2.11$\\times$ inference speedup, 6.22$\\times$ lower FLOPs, and 6.05$\\times$ reduced KV Cache overhead. Our code is available at https://github.com/ocy1/PIO-FVLM."
  },
  "2602.04617": {
    "title_zh": "LEAD：面向忠实放射学报告生成的层级专家对齐解码",
    "abstract_zh": "放射学报告生成（RRG）旨在从医学图像中生成准确且连贯的诊断结果。尽管大型视觉语言模型（LVLM）提升了报告的流畅性与准确性，但它们存在幻觉问题，会生成看似合理但缺乏图像依据的病理细节。现有方法主要依赖外部知识引导来促进生成文本与视觉信息的对齐，然而这些方法往往忽视了预训练模型固有的解码先验和视觉-语言对齐偏差，且因依赖构建的引导而缺乏鲁棒性。本文提出层级专家对齐解码（LEAD），一种新颖的方法，旨在从根本上修改LVLM的解码轨迹。我们设计了一个多专家模块，用于提取不同的病理特征，并通过门控机制将其整合到每个解码器层中。这种层级架构使LLM能够在每个推理步骤中通过学习的门控函数咨询专家特征，从而动态纠正解码偏差，并引导生成过程朝向事实一致性。在多个公共数据集上进行的实验表明，LEAD方法在临床准确性指标上实现了有效提升，缓解了幻觉问题，同时保持了高质量生成。",
    "abstract_en": "Radiology Report Generation (RRG) aims to produce accurate and coherent diagnostics from medical images. Although large vision language models (LVLM) improve report fluency and accuracy, they exhibit hallucinations, generating plausible yet image-ungrounded pathological details. Existing methods primarily rely on external knowledge guidance to facilitate the alignment between generated text and visual information. However, these approaches often ignore the inherent decoding priors and vision-language alignment biases in pretrained models and lack robustness due to reliance on constructed guidance. In this paper, we propose Layer-wise Expert-aligned Decoding (LEAD), a novel method to inherently modify the LVLM decoding trajectory. A multiple experts module is designed for extracting distinct pathological features which are integrated into each decoder layer via a gating mechanism. This layer-wise architecture enables the LLM to consult expert features at every inference step via a learned gating function, thereby dynamically rectifying decoding biases and steering the generation toward factual consistency. Experiments conducted on multiple public datasets demonstrate that the LEAD method yields effective improvements in clinical accuracy metrics and mitigates hallucinations while preserving high generation quality."
  }
}