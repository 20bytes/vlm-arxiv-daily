{
  "2602.02459": {
    "title_zh": "TIC-VLA：一种用于动态环境中机器人导航的思控一体化视觉-语言-动作模型",
    "abstract_zh": "在动态、以人为中心的环境中，机器人必须遵循语言指令，同时保持实时反应控制。视觉-语言-动作（VLA）模型提供了一个有前景的框架，但它们假设推理与控制在时间上对齐，尽管语义推理本质上相对于实时动作存在延迟。我们提出了思控一体化（TIC）-VLA，这是一个延迟感知框架，在动作生成过程中显式建模延迟的语义推理。TIC-VLA定义了一个延迟的语义-控制接口，除了当前观测外，还将动作生成条件化于延迟的视觉-语言语义状态和显式延迟元数据，使策略能够补偿异步推理。我们进一步提出了一种延迟一致性训练流程，在模仿学习和在线强化学习中注入推理延迟，使训练与异步部署保持一致。为了支持真实评估，我们推出了DynaNav，这是一个物理精确、照片级逼真的仿真套件，用于动态环境中的语言引导导航。在仿真和真实机器人上的大量实验表明，TIC-VLA在数秒推理延迟下持续优于先前的VLA模型，同时保持稳健的实时控制。项目网站：https://ucla-mobility.github.io/TIC-VLA/",
    "abstract_en": "Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/"
  },
  "2602.02454": {
    "title_zh": "世界体操家：在世界模型中通过强化学习训练机器人",
    "abstract_zh": "机器人通过与物理世界交互进行学习，从根本上受到物理交互成本的制约。两种替代方案——基于专家演示的监督微调（SFT）和基于软件模拟器的强化学习（RL）——分别受限于可用专家数据的数量以及操作任务中的仿真到现实差距。随着近期从真实世界视频-动作数据中学习的世界模型的出现，我们提出一个问题：在世界模型中训练策略是否比监督学习或软件仿真更能有效提升真实机器人的性能。我们提出了World-Gymnast方法，该方法通过在动作条件化的视频世界模型中展开策略，并利用视觉语言模型（VLM）对展开过程进行奖励，从而对视觉语言动作（VLA）策略进行强化学习微调。在Bridge机器人实验平台上，World-Gymnast的性能比SFT最高提升18倍，比软件模拟器最高提升2倍。更重要的是，World-Gymnast展示了基于世界模型的强化学习的引人注目的能力，包括在世界模型中对多样化语言指令和新场景进行训练、在新场景中进行测试时训练，以及在线迭代优化世界模型和策略。我们的结果表明，学习世界模型并在云端训练机器人策略，可能是弥合仅能在演示中工作的机器人与能在任何家庭中工作的机器人之间差距的关键。",
    "abstract_en": "Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household."
  },
  "2602.02402": {
    "title_zh": "SoMA：面向机器人软体操作的真实到仿真神经模拟器",
    "abstract_zh": "在丰富交互下模拟可变形物体对于真实到仿真的机器人操作仍是一个根本性挑战，其动力学由环境效应与机器人动作共同驱动。现有模拟器依赖于预定义物理或数据驱动的动力学，缺乏机器人条件控制，限制了准确性、稳定性和泛化能力。本文提出SoMA，一种用于软体操作的三维高斯泼溅模拟器。SoMA将可变形动力学、环境力和机器人关节动作耦合于统一的潜在神经空间中，实现端到端的真实到仿真模拟。通过在学习的高斯泼溅上建模交互，SoMA实现了可控、稳定的长时程操作，并能在无预定义物理模型的情况下泛化至未观测轨迹。SoMA将真实世界机器人操作的再模拟准确性与泛化能力提升了20%，能够稳定模拟复杂任务，如长时程布料折叠。",
    "abstract_en": "Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding."
  },
  "2602.02212": {
    "title_zh": "MAIN-VLA：为视觉-语言-动作模型建模意图与环境的抽象",
    "abstract_zh": "尽管视觉-语言-动作（VLA）模型已取得显著进展，但在涉及实时不可预测交互的高度复杂动态环境（如3D开放世界和大型PvP游戏）中，现有方法仍难以从冗余的传感器流中高效提取动作关键信号。为此，我们提出MAIN-VLA框架，通过显式建模意图与环境的抽象，将决策制定建立在深层语义对齐而非浅层模式匹配的基础上。具体而言，我们的意图抽象（IA）将冗长的语言指令及其相关推理提炼为紧凑、显式的语义基元，而环境语义抽象（ESA）则将海量视觉流映射为结构化的拓扑可供性表示。此外，对齐这两种抽象模态会引发一种自发的注意力集中效应，从而支持无需参数的令牌剪枝策略，在保持性能的同时过滤感知冗余。在开放世界《我的世界》及大规模PvP环境（《和平精英》与《无畏契约》）中的大量实验表明，MAIN-VLA实现了新的技术突破，具备更优的决策质量、更强的泛化能力以及顶尖的推理效率。",
    "abstract_en": "Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency."
  },
  "2602.02142": {
    "title_zh": "FD-VLA：用于接触丰富操作的力蒸馏视觉-语言-动作模型",
    "abstract_zh": "力感知是视觉-语言-动作（VLA）框架中的关键模态，因为它能够在接触丰富的任务中实现细粒度感知和灵巧操作。我们提出了力蒸馏VLA（FD-VLA），这是一种新颖的框架，可在不依赖物理力传感器的情况下，将力感知集成到接触丰富的操作中。我们方法的核心是力蒸馏模块（FDM），它通过将可学习的查询令牌（以视觉观察和机器人状态为条件）映射到与真实力信号潜在表示对齐的预测力令牌中，从而蒸馏出力信息。在推理过程中，这个蒸馏出的力令牌被注入到预训练的VLM中，使其能够进行力感知推理，同时保持其视觉-语言语义的完整性。这种设计带来两个关键优势：首先，它允许在缺乏昂贵或易受力-扭矩传感器的广泛机器人上进行实际部署，从而降低硬件成本和复杂性；其次，FDM在VLM之前引入了额外的力-视觉-状态融合先验，这改善了跨模态对齐，并增强了接触丰富场景下的感知-动作鲁棒性。令人惊讶的是，我们的物理实验表明，蒸馏出的力令牌优于直接传感器力测量以及其他基线方法，这突显了这种力蒸馏VLA方法的有效性。",
    "abstract_en": "Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach."
  },
  "2602.02063": {
    "title_zh": "See2Refine：视觉-语言反馈提升基于大语言模型的eHMI行为设计能力",
    "abstract_zh": "自动驾驶车辆缺乏与其他道路使用者的自然沟通渠道，因此外部人机界面（eHMI）对于在共享环境中传达意图和维持信任至关重要。然而，大多数eHMI研究依赖于开发者手工设计的消息-行为配对，难以适应多样且动态的交通场景。一种有前景的替代方案是利用大语言模型（LLM）作为行为设计器，生成基于上下文的eHMI行为，但此类设计器缺乏感知验证，通常依赖固定提示或昂贵的人工标注反馈进行改进。我们提出了See2Refine，一种无需人工干预的闭环框架，利用视觉-语言模型（VLM）的感知评估作为自动化视觉反馈，以优化基于LLM的eHMI行为设计器。给定驾驶场景和候选eHMI行为，VLM评估该行为的感知适宜性，并利用此反馈迭代修正设计器的输出，从而实现无需人工监督的系统性优化。我们在三种eHMI模态（光条、眼睛和手臂）及多种LLM模型规模下评估了该框架。在所有设置中，我们的框架在基于VLM的指标和人类受试者评估中均持续优于仅使用提示的LLM设计器及手动指定的基线方法。结果进一步表明，改进效果在不同模态间具有泛化性，且VLM评估与人类偏好高度一致，这支持了See2Refine在可扩展行为设计中的鲁棒性和有效性。",
    "abstract_en": "Automated vehicles lack natural communication channels with other road users, making external Human-Machine Interfaces (eHMIs) essential for conveying intent and maintaining trust in shared environments. However, most eHMI studies rely on developer-crafted message-action pairs, which are difficult to adapt to diverse and dynamic traffic contexts. A promising alternative is to use Large Language Models (LLMs) as action designers that generate context-conditioned eHMI actions, yet such designers lack perceptual verification and typically depend on fixed prompts or costly human-annotated feedback for improvement. We present See2Refine, a human-free, closed-loop framework that uses vision-language model (VLM) perceptual evaluation as automated visual feedback to improve an LLM-based eHMI action designer. Given a driving context and a candidate eHMI action, the VLM evaluates the perceived appropriateness of the action, and this feedback is used to iteratively revise the designer's outputs, enabling systematic refinement without human supervision. We evaluate our framework across three eHMI modalities (lightbar, eyes, and arm) and multiple LLM model sizes. Across settings, our framework consistently outperforms prompt-only LLM designers and manually specified baselines in both VLM-based metrics and human-subject evaluations. Results further indicate that the improvements generalize across modalities and that VLM evaluations are well aligned with human preferences, supporting the robustness and effectiveness of See2Refine for scalable action design."
  },
  "2602.01834": {
    "title_zh": "面向视觉语言动作模型推理时安全性的概念词典学习方法",
    "abstract_zh": "视觉语言动作（VLA）模型通过将多模态指令转化为可执行行为，实现了感知-动作闭环，但这一能力也放大了安全风险：在大型语言模型中仅产生有害文本的越狱攻击，可能在具身系统中触发不安全的物理行为。现有防御方法（如对齐、过滤或提示强化）干预过晚或针对错误模态，导致融合后的表征仍可被利用。我们提出了一种基于概念的词典学习框架，用于推理时的安全控制。该方法通过从隐藏层激活中构建稀疏、可解释的词典，识别有害概念方向，并应用基于阈值的干预来抑制或阻断不安全激活。在Libero-Harm、BadRobot、RoboPair和IS-Bench上的实验表明，我们的方法实现了最先进的防御性能，将攻击成功率降低超过70%，同时保持任务成功率。关键的是，该框架为即插即用且模型无关，无需重新训练，并能与多种VLA模型无缝集成。据我们所知，这是首个面向具身系统的推理时基于概念的安全方法，推动了VLA模型的可解释性与安全部署。",
    "abstract_en": "Vision Language Action (VLA) models close the perception action loop by translating multimodal instructions into executable behaviors, but this very capability magnifies safety risks: jailbreaks that merely yield toxic text in LLMs can trigger unsafe physical actions in embodied systems. Existing defenses alignment, filtering, or prompt hardening intervene too late or at the wrong modality, leaving fused representations exploitable. We introduce a concept-based dictionary learning framework for inference-time safety control. By constructing sparse, interpretable dictionaries from hidden activations, our method identifies harmful concept directions and applies threshold-based interventions to suppress or block unsafe activations. Experiments on Libero-Harm, BadRobot, RoboPair, and IS-Bench show that our approach achieves state-of-the-art defense performance, cutting attack success rates by over 70\\% while maintaining task success. Crucially, the framework is plug-in and model-agnostic, requiring no retraining and integrating seamlessly with diverse VLAs. To our knowledge, this is the first inference-time concept-based safety method for embodied systems, advancing both interpretability and safe deployment of VLA models."
  },
  "2602.01811": {
    "title_zh": "从精确认知到精准执行：面向视觉语言动作模型的通用自校正与终止框架",
    "abstract_zh": "尽管面向具身智能体的视觉语言动作（VLA）模型集成了感知、推理与控制能力，但仍受限于两大关键弱点：其一，在抓取任务中，语言模型生成的动作标记常与目标对象存在细微的空间偏差，导致抓取失败；其二，模型缺乏可靠的任务完成识别能力，引发冗余动作及频繁的超时错误。为应对这些挑战并提升鲁棒性，我们提出了一种轻量级、无需训练的框架VLA-SCT。该框架作为自校正控制循环运行，结合了数据驱动的动作优化与基于条件的终止逻辑。因此，相较于基线方法，我们的方案在LIBERO基准测试的所有数据集中均实现了稳定提升，显著提高了精细操作任务的成功率，并确保任务准确完成，从而推动更可靠的VLA智能体在复杂非结构化环境中的部署。",
    "abstract_en": "While vision-language-action (VLA) models for embodied agents integrate perception, reasoning, and control, they remain constrained by two critical weaknesses: first, during grasping tasks, the action tokens generated by the language model often exhibit subtle spatial deviations from the target object, resulting in grasp failures; second, they lack the ability to reliably recognize task completion, which leads to redundant actions and frequent timeout errors. To address these challenges and enhance robustness, we propose a lightweight, training-free framework, VLA-SCT. This framework operates as a self-correcting control loop, combining data-driven action refinement with conditional logic for termination. Consequently, compared to baseline approaches, our method achieves consistent improvements across all datasets in the LIBERO benchmark, significantly increasing the success rate of fine manipulation tasks and ensuring accurate task completion, thereby promoting the deployment of more reliable VLA agents in complex, unstructured environments."
  },
  "2602.01662": {
    "title_zh": "AgenticLab：一个能够观察、思考与行动的真实世界机器人智能体平台",
    "abstract_zh": "近年来，大型视觉语言模型（VLMs）在泛化性开放词汇感知与推理方面取得了显著进展，然而其在非结构化、真实世界环境中执行长时程、闭环操作的实际机器人操控能力仍不明确。现有的基于VLM的操控流程难以在不同研究团队的实验设置间进行比较，且多数评估依赖于仿真环境、特权状态或专门设计的实验条件。本文提出AgenticLab，一个模型无关的机器人智能体平台与开放世界操控基准。AgenticLab提供了一套闭环智能体流程，涵盖感知、任务分解、在线验证与重规划。借助AgenticLab，我们在非结构化环境中的真实机器人任务上对当前最先进的基于VLM的智能体进行了基准测试。我们的基准揭示了一系列离线视觉语言测试（如视觉问答与静态图像理解）未能捕捉的故障模式，包括多步语义落地一致性失效、遮挡与场景变化下的物体定位困难，以及空间推理能力不足以支持可靠操控等问题。我们将发布完整的硬件与软件栈，以支持可复现的评估，并加速通用机器人智能体的研究进程。",
    "abstract_en": "Recent advances in large vision-language models (VLMs) have demonstrated generalizable open-vocabulary perception and reasoning, yet their real-robot manipulation capability remains unclear for long-horizon, closed-loop execution in unstructured, in-the-wild environments. Prior VLM-based manipulation pipelines are difficult to compare across different research groups' setups, and many evaluations rely on simulation, privileged state, or specially designed setups. We present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. AgenticLab provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using AgenticLab, we benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments. Our benchmark reveals several failure modes that offline vision-language tests (e.g., VQA and static image understanding) fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation. We will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents."
  },
  "2602.01644": {
    "title_zh": "从感知到行动：空间人工智能代理与世界模型",
    "abstract_zh": "尽管大语言模型已成为代理推理与规划的主流方法，但其在符号领域的成功并未能直接迁移到物理世界。空间智能——即感知三维结构、推理物体关系并在物理约束下行动的能力——是一种正交的能力，对具身代理至关重要。现有综述往往孤立地讨论代理架构或空间领域，缺乏将这两种互补能力统一起来的框架。本文旨在弥合这一鸿沟。通过对2000余篇论文的系统梳理（其中引用顶级学术会议的742篇文献），我们提出了一个统一的三轴分类法，将代理能力与跨尺度的空间任务相连接。关键之处在于，我们区分了空间基础（对几何与物理的度量理解）与符号基础（将图像与文本关联），并论证仅凭感知无法赋予代理能力。我们的分析揭示了映射至这三个轴线的三项核心发现：（1）分层记忆系统（能力轴）对长时程空间任务至关重要；（2）图神经网络与大语言模型融合（任务轴）是结构化空间推理的有效途径；（3）世界模型（尺度轴）对于在微观到宏观空间尺度上实现安全部署不可或缺。最后，我们指出了六大挑战并展望未来研究方向，包括建立统一评估框架以标准化跨领域评估。该分类法为整合碎片化的研究奠定了基础，有望推动机器人、自动驾驶与地理空间智能等领域中新一代空间感知自主系统的发展。",
    "abstract_en": "While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence."
  },
  "2602.02220": {
    "title_zh": "LangMap：面向开放词汇目标导航的分层基准",
    "abstract_zh": "物体与语言之间的关系对于人类与人工智能之间的有意义交流以及实际有用的具身智能至关重要。我们引入了HieraNav，一个多粒度、开放词汇的目标导航任务，其中智能体通过解析自然语言指令，在四个语义层级上到达目标：场景、房间、区域和实例。为此，我们提出了Language as a Map（LangMap），这是一个基于真实世界3D室内扫描的大规模基准，包含全面的人工验证标注和覆盖这些层级的任务。LangMap提供区域标签、区分性区域描述、涵盖414个对象类别的区分性实例描述，以及超过18K个导航任务。每个目标都配有简洁和详细的描述，支持不同指令风格的评估。LangMap实现了卓越的标注质量，在区分性准确率上比GOAT-Bench高出23.8%，同时使用的词汇量减少了四倍。在LangMap上对零样本和监督模型的综合评估表明，更丰富的上下文和记忆能提高成功率，而长尾、小型、上下文依赖和远距离目标，以及多目标完成，仍然是挑战。HieraNav和LangMap为推进语言驱动的具身导航建立了一个严谨的测试平台。项目地址：https://bo-miao.github.io/LangMap",
    "abstract_en": "The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap"
  },
  "2602.00551": {
    "title_zh": "APEX：一种用于异步空中目标导航的解耦记忆型探索器",
    "abstract_zh": "空中目标导航是具身人工智能领域的一个前沿挑战，要求无人机代理仅通过视觉感知和语言描述，自主探索、推理并识别特定目标。然而，现有方法在记忆复杂空中环境的空间表示、实现可靠且可解释的动作决策，以及高效探索和信息收集方面存在困难。为解决这些挑战，我们提出了APEX（空中并行探索器），这是一种新颖的分层代理，专为复杂空中环境中的高效探索和目标获取而设计。APEX基于模块化的三部分架构构建：1）动态空间语义映射记忆，利用视觉语言模型的零样本能力，动态构建高分辨率的三维吸引力、探索和障碍物地图，作为可解释的记忆机制；2）动作决策模块，通过强化学习训练，将丰富的空间理解转化为细粒度且鲁棒的控制策略；3）目标定位模块，采用开放词汇检测器实现确定性和泛化性的目标识别。所有这些组件被集成到一个分层、异步且并行的框架中，有效规避了视觉语言模型的推理延迟，并提升了代理在探索中的主动性。大量实验表明，在具有挑战性的UAV-ON基准测试中，APEX在成功率上超越了先前最佳方法4.2%，在最短路径长度指标上提升了2.8%，证明了其卓越的效率和分层异步设计的有效性。我们的源代码已在GitHub上提供。",
    "abstract_en": "Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \\textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM's inference latency and boosting the agent's proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\\% SR and +2.8\\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \\href{https://github.com/4amGodvzx/apex}{GitHub}"
  },
  "2602.00222": {
    "title_zh": "MapDream：面向视觉语言导航的任务驱动地图学习",
    "abstract_zh": "视觉语言导航要求智能体在部分可观测的三维环境中遵循自然语言指令，这促使需要构建能够聚合超越局部感知的空间上下文的地图表示。然而，现有方法大多依赖于独立于导航策略构建的手工地图。我们认为，地图应是直接由导航目标塑造的学习表示，而非详尽的重建。基于这一见解，我们提出了MapDream，一个地图闭环框架，将地图构建形式化为自回归的鸟瞰图图像合成。该框架联合学习地图生成与动作预测，将环境上下文提炼为紧凑的三通道鸟瞰图地图，仅保留对导航至关重要的可操作信息。通过监督预训练引导出可靠的映射到控制接口，而自回归设计则支持通过强化微调实现端到端的联合优化。在R2R-CE和RxR-CE数据集上的实验达到了单目视觉导航的先进水平，验证了任务驱动的生成式地图学习的有效性。",
    "abstract_en": "Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning."
  },
  "2601.21751": {
    "title_zh": "动态拓扑感知：打破视觉语言导航中的粒度僵化",
    "abstract_zh": "连续环境下的视觉语言导航（VLN-CE）面临一个核心挑战：将高层语言指令落实到精确、安全且长距离的空间行动中。显式拓扑图已被证明是为此类任务提供鲁棒空间记忆的关键方案。然而，现有拓扑规划方法存在“粒度僵化”问题。具体而言，这些方法通常依赖固定的几何阈值来采样节点，无法适应多变的环境复杂性。这种僵化导致严重的不匹配：模型在简单区域倾向于过度采样，造成计算冗余；而在高不确定性区域则采样不足，增加碰撞风险并损害导航精度。为解决此问题，我们提出了DGNav——一个动态拓扑导航框架，引入上下文感知机制以实时调整地图密度与连通性。我们的方法包含两项核心创新：（1）场景感知自适应策略：基于预测路径点的离散度动态调整图构建阈值，实现在复杂环境中“按需加密”；（2）动态图变换器：通过融合视觉、语言与几何线索为动态边权重，重构图连通性，使智能体能够滤除拓扑噪声并增强指令遵循能力。在R2R-CE和RxR-CE基准上的大量实验表明，DGNav展现出卓越的导航性能和强大的泛化能力。此外，消融研究证实该框架在导航效率与安全探索间实现了最优平衡。代码已开源：https://github.com/shannanshouyin/DGNav。",
    "abstract_en": "Vision-Language Navigation in Continuous Environments (VLN-CE) presents a core challenge: grounding high-level linguistic instructions into precise, safe, and long-horizon spatial actions. Explicit topological maps have proven to be a vital solution for providing robust spatial memory in such tasks. However, existing topological planning methods suffer from a \"Granularity Rigidity\" problem. Specifically, these methods typically rely on fixed geometric thresholds to sample nodes, which fails to adapt to varying environmental complexities. This rigidity leads to a critical mismatch: the model tends to over-sample in simple areas, causing computational redundancy, while under-sampling in high-uncertainty regions, increasing collision risks and compromising precision. To address this, we propose DGNav, a framework for Dynamic Topological Navigation, introducing a context-aware mechanism to modulate map density and connectivity on-the-fly. Our approach comprises two core innovations: (1) A Scene-Aware Adaptive Strategy that dynamically modulates graph construction thresholds based on the dispersion of predicted waypoints, enabling \"densification on demand\" in challenging environments; (2) A Dynamic Graph Transformer that reconstructs graph connectivity by fusing visual, linguistic, and geometric cues into dynamic edge weights, enabling the agent to filter out topological noise and enhancing instruction adherence. Extensive experiments on the R2R-CE and RxR-CE benchmarks demonstrate DGNav exhibits superior navigation performance and strong generalization capabilities. Furthermore, ablation studies confirm that our framework achieves an optimal trade-off between navigation efficiency and safe exploration. The code is available at https://github.com/shannanshouyin/DGNav."
  },
  "2601.18492": {
    "title_zh": "DV-VLN：基于大语言模型的视觉与语言导航双重验证可靠框架",
    "abstract_zh": "视觉与语言导航任务要求具身智能体根据自然语言指令在复杂三维环境中进行导航。近期大语言模型的发展提升了语言驱动导航的可解释性。然而，大多数基于大语言模型的智能体仍依赖单次动作决策机制，即模型必须从带有噪声的多视角文本化观察结果中选择一个选项。由于局部信息不匹配及中间推理过程的不完善，此类决策极易偏离正确路径，导致误差累积并在未知环境中降低可靠性。本文提出DV-VLN——一种遵循“生成-验证”范式的新型视觉与语言导航框架。该框架首先对开源LLaMA-2主干网络进行参数高效的领域内适配，以生成结构化的导航思维链，随后通过两个互补通道验证候选动作：真伪验证与掩码实体验证。DV-VLN通过聚合多个样本的验证成功次数来选择动作，并生成可解释的分数进行重排序。在R2R、RxR（英文子集）和REVERIE数据集上的实验表明，DV-VLN相较于直接预测和纯采样基线方法均取得稳定提升，在纯语言视觉与语言导航智能体中达到竞争性性能，与多种跨模态系统相比亦展现出有前景的结果。代码已开源：https://github.com/PlumJun/DV-VLN。",
    "abstract_en": "Vision-and-Language Navigation (VLN) requires an embodied agent to navigate in a complex 3D environment according to natural language instructions. Recent progress in large language models (LLMs) has enabled language-driven navigation with improved interpretability. However, most LLM-based agents still rely on single-shot action decisions, where the model must choose one option from noisy, textualized multi-perspective observations. Due to local mismatches and imperfect intermediate reasoning, such decisions can easily deviate from the correct path, leading to error accumulation and reduced reliability in unseen environments. In this paper, we propose DV-VLN, a new VLN framework that follows a generate-then-verify paradigm. DV-VLN first performs parameter-efficient in-domain adaptation of an open-source LLaMA-2 backbone to produce a structured navigational chain-of-thought, and then verifies candidate actions with two complementary channels: True-False Verification (TFV) and Masked-Entity Verification (MEV). DV-VLN selects actions by aggregating verification successes across multiple samples, yielding interpretable scores for reranking. Experiments on R2R, RxR (English subset), and REVERIE show that DV-VLN consistently improves over direct prediction and sampling-only baselines, achieving competitive performance among language-only VLN agents and promising results compared with several cross-modal systems.Code is available at https://github.com/PlumJun/DV-VLN."
  },
  "2601.18188": {
    "title_zh": "NaVIDA：基于逆动力学增强的视觉语言导航",
    "abstract_zh": "视觉语言导航（VLN）要求智能体能够理解自然语言指令，并在视觉丰富的环境中连贯地执行动作。然而，现有方法大多依赖于反应式的状态-动作映射，未能显式建模动作如何因果性地改变后续视觉观察。由于缺乏这种视觉-动作因果关系，智能体无法预测自身动作引发的视觉变化，导致行为不稳定、泛化能力弱以及轨迹上的累积误差。为解决这些问题，我们提出了NaVIDA（基于逆动力学增强的导航），这是一个统一的VLN框架，将策略学习与动作驱动的视觉动态建模及自适应执行相结合。NaVIDA通过基于动作块的逆动力学监督增强训练，以学习视觉变化与对应动作之间的因果关系。为构建这种监督并扩展有效规划范围，NaVIDA采用分层概率动作块化（HPAC）方法，将轨迹组织为多步动作块，并提供具有区分性的长程视觉变化线索。为进一步抑制推理过程中的误差累积并稳定行为，一种基于熵的引导机制自适应地设置动作块的执行范围。大量实验表明，与现有最先进方法相比，NaVIDA以更少的参数量（30亿 vs. 80亿）实现了更优的导航性能。真实世界机器人评估进一步验证了本方法的实际可行性和有效性。代码与数据将在论文录用后公开。",
    "abstract_en": "Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \\textsc{NaVIDA} (\\textbf{Nav}igation with \\textbf{I}nverse \\textbf{D}ynamics \\textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \\textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \\textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \\textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance."
  },
  "2601.15614": {
    "title_zh": "AION：基于双策略强化学习的空中室内目标导航系统",
    "abstract_zh": "目标导航任务要求智能体在未知环境中自主探索，并导航至由语义标签指定的目标物体。先前的研究主要集中于二维移动场景下的零样本目标导航，而将其扩展至具备三维移动能力的空中平台仍鲜有探索。空中机器人虽具备卓越的机动性与搜索效率，但也带来了空间感知、动态控制及安全保障方面的新挑战。本文提出AION，一种无需依赖外部定位或全局地图、基于视觉的空中目标导航方法。AION采用端到端的双策略强化学习框架，将探索行为与目标抵达行为解耦为两个专用策略。我们在AI2-THOR基准测试中评估了AION，并进一步使用高保真无人机模型在IsaacSim中验证了其实时性能。实验结果表明，AION在探索能力、导航效率及安全性等综合评估指标上均表现出优越性能。演示视频可访问：https://youtu.be/TgsUm6bb7zg。",
    "abstract_en": "Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at https://youtu.be/TgsUm6bb7zg."
  },
  "2601.13976": {
    "title_zh": "FantasyVLN：面向视觉语言导航的统一多模态思维链推理框架",
    "abstract_zh": "在视觉语言导航（VLN）中实现人类水平的表现，要求具身智能体能够同时理解多模态指令与视觉空间上下文，并对长序列动作进行推理。近期研究，如NavCoT与NavGPT-2，展示了思维链（CoT）推理在提升可解释性与长程规划能力方面的潜力。此外，OctoNav-R1和CoT-VLA等多模态扩展进一步验证了CoT作为实现类人导航推理的有效路径。然而，现有方法存在明显缺陷：纯文本CoT缺乏空间基础，易因稀疏标注的推理步骤而过拟合；而多模态CoT通过生成想象的视觉观测导致严重的令牌膨胀，使得实时导航难以实现。本文提出FantasyVLN，一个统一的隐式推理框架，在保留CoT推理优势的同时避免了显式的令牌开销。具体而言，在CoT推理训练中，我们使用预训练的视觉自回归模型（VAR）将想象的视觉令牌编码至紧凑的潜在空间，并通过统一的多CoT策略，使模型能够从文本、视觉及多模态CoT模式中联合学习。在推理阶段，我们的模型直接执行从指令到动作的映射，同时仍受益于推理感知的表征。在LH-VLN数据集上的大量实验表明，该方法实现了兼具推理感知与实时性的导航，不仅提升了成功率与效率，而且相比显式CoT方法，推理延迟降低了一个数量级。",
    "abstract_en": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods."
  },
  "2601.12766": {
    "title_zh": "Spatial-VLN：具备显式空间感知与探索能力的零样本视觉语言导航",
    "abstract_zh": "利用大型语言模型（LLM）的零样本视觉语言导航（VLN）代理在泛化方面表现出色，但存在空间感知不足的问题。针对复杂连续环境，我们将关键感知瓶颈归纳为三类空间挑战：门交互、多房间导航和模糊指令执行，现有方法在这些方面持续面临高失败率。我们提出了Spatial-VLN，一种感知引导的探索框架，旨在克服这些挑战。该框架包含两个核心模块：空间感知增强（SPE）模块通过全景过滤结合专门的门与区域专家，生成空间连贯、跨视图一致的感知表征；在此基础上，探索式多专家推理（EMR）模块利用并行LLM专家处理路径点级语义和区域级空间转换。当专家预测出现分歧时，查询-探索机制被激活，引导代理主动探测关键区域以解决感知模糊性。在VLN-CE上的实验表明，Spatial-VLN仅使用低成本LLM即实现了最先进的性能。此外，为验证实际应用性，我们引入了一种基于价值的路径点采样策略，有效弥合了仿真到现实的差距。大量真实环境评估证实，该框架在复杂环境中具有卓越的泛化能力和鲁棒性。代码与演示视频发布于https://yueluhhxx.github.io/Spatial-VLN-web/。",
    "abstract_en": "Zero-shot Vision-and-Language Navigation (VLN) agents leveraging Large Language Models (LLMs) excel in generalization but suffer from insufficient spatial perception. Focusing on complex continuous environments, we categorize key perceptual bottlenecks into three spatial challenges: door interaction,multi-room navigation, and ambiguous instruction execution, where existing methods consistently suffer high failure rates. We present Spatial-VLN, a perception-guided exploration framework designed to overcome these challenges. The framework consists of two main modules. The Spatial Perception Enhancement (SPE) module integrates panoramic filtering with specialized door and region experts to produce spatially coherent, cross-view consistent perceptual representations. Building on this foundation, our Explored Multi-expert Reasoning (EMR) module uses parallel LLM experts to address waypoint-level semantics and region-level spatial transitions. When discrepancies arise between expert predictions, a query-and-explore mechanism is activated, prompting the agent to actively probe critical areas and resolve perceptual ambiguities. Experiments on VLN-CE demonstrate that Spatial VLN achieves state-of-the-art performance using only low-cost LLMs. Furthermore, to validate real-world applicability, we introduce a value-based waypoint sampling strategy that effectively bridges the Sim2Real gap. Extensive real-world evaluations confirm that our framework delivers superior generalization and robustness in complex environments. Our codes and videos are available at https://yueluhhxx.github.io/Spatial-VLN-web/."
  },
  "2601.09111": {
    "title_zh": "迈向开放环境与指令：基于快慢交互推理的通用视觉语言导航",
    "abstract_zh": "视觉语言导航旨在使智能体能够根据语言指令导航至目标位置。传统的视觉语言导航通常遵循封闭集假设，即训练与测试数据共享相同风格的输入图像和指令。然而，现实世界是开放的，充满了各种未见过的环境，这对封闭集方法构成了巨大挑战。为此，我们聚焦于通用场景适应任务，旨在通过引入多样化的环境和不一致的指令来学习泛化的导航能力。面对这一任务，当遭遇未见环境和指令时，主要挑战在于如何使智能体在导航过程中动态生成泛化策略。近期研究表明，人类通过快慢认知系统能够生成稳定的策略，从而增强对开放世界的适应能力。受此启发，我们提出了慢促快视觉语言导航方法，构建了一个动态交互的快慢推理框架。其中，快速推理模块作为一个端到端的策略网络，通过实时输入输出动作，并在历史存储库中积累执行记录以构建记忆。慢速推理模块则分析快速推理模块生成的记忆，通过深度反思提取能够增强决策泛化能力的经验。这些经验被结构化存储，并用于持续优化快速推理模块。与将快慢推理视为独立机制的传统方法不同，我们的框架实现了快慢交互。通过利用慢速推理产生的经验，这种交互使系统能够持续适应，并在面对未见场景时高效执行导航任务。",
    "abstract_en": "Vision-Language Navigation aims to enable agents to navigate to a target location based on language instructions. Traditional VLN often follows a close-set assumption, i.e., training and test data share the same style of the input images and instructions. However, the real world is open and filled with various unseen environments, posing enormous difficulties for close-set methods. To this end, we focus on the General Scene Adaptation (GSA-VLN) task, aiming to learn generalized navigation ability by introducing diverse environments and inconsistent intructions.Towards this task, when facing unseen environments and instructions, the challenge mainly lies in how to enable the agent to dynamically produce generalized strategies during the navigation process. Recent research indicates that by means of fast and slow cognition systems, human beings could generate stable policies, which strengthen their adaptation for open world. Inspired by this idea, we propose the slow4fast-VLN, establishing a dynamic interactive fast-slow reasoning framework. The fast-reasoning module, an end-to-end strategy network, outputs actions via real-time input. It accumulates execution records in a history repository to build memory. The slow-reasoning module analyze the memories generated by the fast-reasoning module. Through deep reflection, it extracts experiences that enhance the generalization ability of decision-making. These experiences are structurally stored and used to continuously optimize the fast-reasoning module. Unlike traditional methods that treat fast-slow reasoning as independent mechanisms, our framework enables fast-slow interaction. By leveraging the experiences from slow reasoning. This interaction allows the system to continuously adapt and efficiently execute navigation tasks when facing unseen scenarios."
  },
  "2602.02468": {
    "title_zh": "Avenir-Web：基于混合定位专家的人类经验模仿式多模态网络代理",
    "abstract_zh": "尽管多模态大语言模型取得了进展，但自主网络代理在执行复杂动态网页界面的长时程任务时仍难以保证可靠性。现有代理常面临元素定位不准确、缺乏站点特定程序性知识，以及长期任务跟踪与记忆不稳定等问题，尤其在处理复杂的文档对象模型结构时更为突出。为应对这些局限，我们提出了Avenir-Web，一种在网络代理中实现开源新标杆的代理，在真实世界部署的Online-Mind2Web基准测试中表现卓越。Avenir-Web采用混合定位专家机制，通过经验模仿规划融入程序性先验知识，并结合任务跟踪清单与自适应记忆，以实现跨多样用户界面范式的稳健无缝交互。我们在Online-Mind2Web这一严格评估实时用户中心网络任务的基准上对Avenir-Web进行了测试。结果表明，Avenir-Web显著超越了先前的开源代理，并与顶尖专有模型达到性能持平，从而为实时网站上的可靠网络代理确立了新的开源标杆。",
    "abstract_en": "Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites."
  },
  "2602.02465": {
    "title_zh": "MentisOculi：揭示心智意象推理的局限性",
    "abstract_zh": "前沿模型正从仅能接收视觉信息的多模态大语言模型（MLLMs）向能够原生交错生成内容的统一多模态模型（UMMs）转变。这一转变激发了人们利用中间可视化作为推理辅助的兴趣，类似于人类的心智意象。该理念的核心在于以目标为导向形成、维持和操纵视觉表征的能力。为了评估和探究这一能力，我们开发了MentisOculi——一套程序化、分层化的多步推理问题集，适用于视觉化解决方案，并针对前沿模型的挑战进行了优化。通过评估从潜在标记到显式生成图像等多种视觉策略，我们发现它们通常未能提升性能。对UMMs的具体分析揭示了一个关键局限：尽管它们具备解决任务的文本推理能力，有时也能生成正确的视觉内容，但它们受到生成误差累积的影响，甚至无法有效利用真实的可视化信息。我们的研究结果表明，尽管视觉思维具有内在吸引力，但目前尚未对模型推理产生助益。MentisOculi为分析和弥合不同模型家族间的这一差距奠定了必要基础。",
    "abstract_en": "Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families."
  },
  "2602.02456": {
    "title_zh": "面向任务推理的关系感知分层三维场景图",
    "abstract_zh": "以结构化方式表示和理解三维环境对于自主智能体导航和推理其周围环境至关重要。传统的同步定位与建图（SLAM）方法虽能生成度量重建并可扩展为度量-语义建图，但缺乏更高层次的抽象和关系推理能力。为弥补这一不足，三维场景图作为一种能够捕捉层次结构和物体关系的强大表示方法应运而生。本研究提出了一种增强型分层三维场景图，它在多个抽象层次上整合了开放词汇特征，并支持物体关系推理。我们的方法利用视觉语言模型（VLM）来推断语义关系。特别地，我们引入了一个任务推理模块，该模块结合大型语言模型（LLM）和视觉语言模型（VLM）来解析场景图的语义与关系信息，使智能体能够更智能地进行任务推理并与环境交互。通过在四足机器人上部署该方法于多种环境和任务中，我们验证了其推理能力。",
    "abstract_en": "Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph's semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them."
  },
  "2602.02408": {
    "title_zh": "ReasonEdit：基于人类推理的视觉语言模型编辑",
    "abstract_zh": "模型编辑旨在修正大型预训练模型中的错误，同时不影响无关行为。尽管近期已有研究尝试编辑视觉语言模型（VLMs），但尚无现有编辑器能够处理需要人类与模型对图像进行复杂推理的任务。为此，我们提出了ReasonEdit，这是首个允许用户在编辑过程中解释其推理的VLM编辑器，引入了一种新颖且实用的模型编辑框架。ReasonEdit持续将人类推理存储于代码本中，并在推理时通过一种受网络科学启发的新型拓扑平衡多模态嵌入方法，仅检索相关事实。在多个基于推理的视觉问答数据集上对四种VLMs进行测试，ReasonEdit实现了最先进的编辑性能，最终证明在编辑过程中融入人类推理能显著提升编辑的泛化能力。",
    "abstract_en": "Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization."
  },
  "2602.02341": {
    "title_zh": "LongVPO：从锚定线索到自我推理的长视频偏好优化",
    "abstract_zh": "我们提出了LongVPO，一种新颖的两阶段直接偏好优化框架，使短上下文视觉语言模型能够稳健地理解超长视频，无需任何长视频标注。在第一阶段，我们通过将问题锚定到单个短视频片段、与干扰项交错排列，并应用视觉相似性和问题特异性过滤来合成偏好三元组，以减轻位置偏差并确保明确的监督。我们还通过仅评估锚定片段来近似参考模型在长上下文中的评分，从而降低计算开销。在第二阶段，我们在长视频上采用递归字幕生成流程来生成场景级元数据，然后使用大型语言模型构建多片段推理查询和不受偏好的响应，通过多片段推理任务来对齐模型的偏好。仅使用16K个合成示例且无需昂贵的人工标注，LongVPO在多个长视频基准测试中超越了最先进的开源模型，同时保持了强大的短视频性能（例如在MVBench上），为高效的长视频理解提供了一个可扩展的范式。",
    "abstract_en": "We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding."
  },
  "2602.02185": {
    "title_zh": "Vision-DeepResearch基准：重新思考多模态大语言模型的视觉与文本搜索能力",
    "abstract_zh": "多模态大语言模型（MLLMs）已推动视觉问答（VQA）的发展，并支持利用搜索引擎进行复杂视觉-文本事实查找的Vision-DeepResearch系统。然而，评估这些视觉与文本搜索能力仍具挑战，现有基准存在两大局限。首先，现有基准并非以视觉搜索为核心：本需视觉搜索的答案常通过文本问题中的跨文本线索泄露，或可从当前MLLMs的先验世界知识中推断。其次，评估场景过于理想化：在图像搜索方面，所需信息常可通过与完整图像的近似精确匹配获取；而文本搜索则过于直接且挑战性不足。为解决这些问题，我们构建了包含2000个VQA实例的Vision-DeepResearch基准（VDR-Bench）。所有问题均通过细致多阶段筛选流程与严格专家评审创建，旨在评估Vision-DeepResearch系统在真实世界条件下的表现。此外，针对当前MLLMs视觉检索能力不足的问题，我们提出一种简单的多轮裁剪搜索工作流程。该策略被证明能有效提升模型在真实视觉检索场景中的性能。总体而言，我们的研究结果为未来多模态深度研究系统的设计提供了实用指导。代码将在https://github.com/Osilly/Vision-DeepResearch发布。",
    "abstract_en": "Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch."
  },
  "2602.02043": {
    "title_zh": "Auto-Comp：面向对比式视觉语言模型可扩展组合性探测的自动化流程",
    "abstract_zh": "现代视觉语言模型在组合推理方面存在关键缺陷，常将‘红色立方体和蓝色球体’与‘蓝色立方体和红色球体’混淆。解构这些失败的视觉与语言根源是鲁棒性评估的根本挑战。为实现细粒度、可控的分析，我们引入了Auto-Comp，一个用于生成可扩展基准测试的完全自动化合成流程。其可控特性是剖析和隔离不同推理技能的关键。Auto-Comp从最小化描述（如‘白色背景上一辆自行车左侧的显示器’）和LLM生成的上下文描述（如‘在明亮摄影棚中，显示器位于自行车左侧’）生成配对图像，通过受控的A/B测试来分离核心绑定能力与视觉语言复杂性。我们对20个视觉语言模型在颜色绑定和空间关系新基准上的评估显示，CLIP和SigLIP模型家族普遍存在组合性失败。关键的是，我们新颖的‘混淆基准’揭示了超越简单属性交换的更深层缺陷：模型极易受低熵干扰项（如重复物体或颜色）影响，表明其组合性失败超出了已知的词袋模型限制。我们揭示了一个令人惊讶的权衡：提供全局场景线索的视觉语言上下文虽有助于空间推理，但同时会因引入视觉杂乱而阻碍局部属性绑定。我们发布Auto-Comp流程以促进未来基准测试的创建，并附上所有生成的基准测试集（https://huggingface.co/AutoComp）。",
    "abstract_en": "Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing \"a red cube and a blue sphere\" with \"a blue cube and a red sphere\". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., \"a monitor to the left of a bicycle on a white background\") and LLM-generated Contextual captions (e.g., \"In a brightly lit photography studio, a monitor is positioned to the left of a bicycle\"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel \"Confusion Benchmark\" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp)."
  },
  "2602.02033": {
    "title_zh": "一图多配：在大规模广告图像生成中协调多样化的群体点击偏好",
    "abstract_zh": "广告图像生成日益关注点击率等在线指标，但现有方法采用“一刀切”策略，仅优化整体点击率，忽视了用户群体间的偏好多样性。这导致特定群体的表现欠佳，限制了定向营销的效果。为弥补这一差距，我们提出了《一图多配》统一框架，旨在协调大规模广告图像生成中多样化的群体点击偏好。该框架首先进行产品感知的自适应分组，根据用户属性和产品特征动态组织用户，并用丰富的集体偏好特征表示每个群体。基于这些分组，偏好条件图像生成采用群体感知多模态大语言模型，为每个群体生成定制化图像。该模型经过预训练，能同时理解群体特征并生成广告图像。随后，我们使用提出的群体偏好优化方法对模型进行微调，以协调群体偏好，有效提升各群体在生成图像上的点击率。为推进该领域发展，我们引入了分组广告图像偏好数据集，这是首个大规模公开的群体图像偏好数据集，包含基于4000万用户构建的约60万个群体。大量实验表明，我们的框架在离线和在线场景下均实现了最先进的性能。代码和数据集将在https://github.com/JD-GenX/OSMF发布。",
    "abstract_en": "Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all\" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specific groups, limiting targeted marketing effectiveness. To bridge this gap, we present \\textit{One Size, Many Fits} (OSMF), a unified framework that aligns diverse group-wise click preferences in large-scale advertising image generation. OSMF begins with product-aware adaptive grouping, which dynamically organizes users based on their attributes and product characteristics, representing each group with rich collective preference features. Building on these groups, preference-conditioned image generation employs a Group-aware Multimodal Large Language Model (G-MLLM) to generate tailored images for each group. The G-MLLM is pre-trained to simultaneously comprehend group features and generate advertising images. Subsequently, we fine-tune the G-MLLM using our proposed Group-DPO for group-wise preference alignment, which effectively enhances each group's CTR on the generated images. To further advance this field, we introduce the Grouped Advertising Image Preference Dataset (GAIP), the first large-scale public dataset of group-wise image preferences, including around 600K groups built from 40M users. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance in both offline and online settings. Our code and datasets will be released at https://github.com/JD-GenX/OSMF."
  },
  "2602.04880": {
    "title_zh": "捕捉视觉环境结构与控制性能的相关性",
    "abstract_zh": "视觉表示的选择是扩展通用机器人策略的关键。然而，即使是在仿真环境中，通过策略部署进行直接评估的成本也很高。现有的代理指标侧重于表示捕捉视觉世界狭窄方面的能力，如物体形状，这限制了跨环境的泛化能力。在本文中，我们采取分析视角：通过测量预训练视觉编码器从图像中解码环境状态（包括几何结构、物体结构和物理属性）的能力，来探究这些编码器。利用能够获取真实状态信息的仿真环境，我们证明了这种探究精度与跨不同环境和学习设置的下游策略性能高度相关，显著优于先前的指标，并实现了高效的表示选择。更广泛地说，我们的研究为支持可泛化操作的表示属性提供了见解，表明学习编码环境的潜在物理状态是实现控制的一个有前景的目标。",
    "abstract_en": "The choice of visual representation is key to scaling generalist robot policies. However, direct evaluation via policy rollouts is expensive, even in simulation. Existing proxy metrics focus on the representation's capacity to capture narrow aspects of the visual world, like object shape, limiting generalization across environments. In this paper, we take an analytical perspective: we probe pretrained visual encoders by measuring how well they support decoding of environment state -- including geometry, object structure, and physical attributes -- from images. Leveraging simulation environments with access to ground-truth state, we show that this probing accuracy strongly correlates with downstream policy performance across diverse environments and learning settings, significantly outperforming prior metrics and enabling efficient representation selection. More broadly, our study provides insight into the representational properties that support generalizable manipulation, suggesting that learning to encode the latent physical state of the environment is a promising objective for control."
  },
  "2602.04877": {
    "title_zh": "CoWTracker：通过变形而非相关性进行跟踪",
    "abstract_zh": "密集点跟踪是计算机视觉中的一个基础问题，其应用范围从视频分析到机器人操作。当前最先进的跟踪器通常依赖成本体积来跨帧匹配特征，但这种方法在空间分辨率上具有二次复杂度，限制了可扩展性和效率。本文提出了一种新颖的密集点跟踪器——CoWTracker，它摒弃了成本体积，转而采用变形方法。受光流领域最新进展的启发，我们的方法基于当前估计，通过将目标帧的特征变形到查询帧，迭代地优化跟踪估计。结合一个在所有轨迹上进行联合时空推理的Transformer架构，我们的设计无需计算特征相关性即可建立长距离对应关系。该模型结构简洁，在标准密集点跟踪基准测试（包括TAP-Vid-DAVIS、TAP-Vid-Kinetics和Robo-TAP）中达到了最先进的性能。值得注意的是，该模型在光流估计方面也表现出色，有时在Sintel、KITTI和Spring基准测试中甚至超越了专用方法。这些结果表明，基于变形的架构可以统一密集点跟踪和光流估计。",
    "abstract_en": "Dense point tracking is a fundamental problem in computer vision, with applications ranging from video analysis to robotic manipulation. State-of-the-art trackers typically rely on cost volumes to match features across frames, but this approach incurs quadratic complexity in spatial resolution, limiting scalability and efficiency. In this paper, we propose \\method, a novel dense point tracker that eschews cost volumes in favor of warping. Inspired by recent advances in optical flow, our approach iteratively refines track estimates by warping features from the target frame to the query frame based on the current estimate. Combined with a transformer architecture that performs joint spatiotemporal reasoning across all tracks, our design establishes long-range correspondences without computing feature correlations. Our model is simple and achieves state-of-the-art performance on standard dense point tracking benchmarks, including TAP-Vid-DAVIS, TAP-Vid-Kinetics, and Robo-TAP. Remarkably, the model also excels at optical flow, sometimes outperforming specialized methods on the Sintel, KITTI, and Spring benchmarks. These results suggest that warping-based architectures can unify dense point tracking and optical flow estimation."
  },
  "2602.04635": {
    "title_zh": "面向自然语言指令中物体定位的关系场景图",
    "abstract_zh": "随着机器人在人类环境中的应用日益广泛，自然的人机交互需求愈发迫切。然而，理解自然语言指令要求机器人推断预期任务、将其分解为可执行动作，并将这些动作基于机器人对环境（包括相关物体、智能体和位置）的认知进行定位。这一挑战可通过结合大语言模型（LLMs）理解自然语言的能力与三维场景图（3DSGs）在环境语义表征中定位推断动作的能力来解决。然而，许多3DSGs缺乏物体间的显式空间关系，尽管人类在描述环境时常常依赖这些关系。本文探讨了将开放或封闭词汇的空间关系融入3DSGs是否能提升LLMs解释自然语言指令的能力。为此，我们提出了一种基于LLM的管道，用于从开放词汇语言指令中定位目标物体，以及一种基于视觉语言模型（VLM）的管道，用于从建图过程中捕获的图像向3DSGs添加开放词汇空间边。最后，通过一项研究评估了两种LLMs在目标物体定位下游任务中的表现。我们的研究表明，显式空间关系能有效提升LLMs的物体定位能力。此外，基于VLM的开放词汇关系生成在机器人捕获图像中具有可行性，但其相较于封闭词汇关系的优势较为有限。",
    "abstract_en": "Robots are finding wider adoption in human environments, increasing the need for natural human-robot interaction. However, understanding a natural language command requires the robot to infer the intended task and how to decompose it into executable actions, and to ground those actions in the robot's knowledge of the environment, including relevant objects, agents, and locations. This challenge can be addressed by combining the capabilities of Large language models (LLMs) to understand natural language with 3D scene graphs (3DSGs) for grounding inferred actions in a semantic representation of the environment. However, many 3DSGs lack explicit spatial relations between objects, even though humans often rely on these relations to describe an environment. This paper investigates whether incorporating open- or closed-vocabulary spatial relations into 3DSGs can improve the ability of LLMs to interpret natural language commands. To address this, we propose an LLM-based pipeline for target object grounding from open-vocabulary language commands and a vision language model (VLM)-based pipeline to add open-vocabulary spatial edges to 3DSGs from images captured while mapping. Finally, two LLMs are evaluated in a study assessing their performance on the downstream task of target object grounding. Our study demonstrates that explicit spatial relations improve the ability of LLMs to ground objects. Moreover, open-vocabulary relation generation with VLMs proves feasible from robot-captured images, but their advantage over closed-vocabulary relations is found to be limited."
  },
  "2602.04600": {
    "title_zh": "行动、感知、再行动：从大规模第一人称人类数据中学习非马尔可夫主动感知策略",
    "abstract_zh": "在无约束环境中实现泛化性操作要求机器人能够主动解决信息不确定性，即具备主动感知能力。然而，现有方法通常局限于有限的感知行为类型，限制了其在复杂环境中的适用性。本研究将主动感知形式化为一个由信息增益和决策分支驱动的非马尔可夫过程，并提出了视觉主动感知范式的结构化分类。基于这一视角，我们提出了CoMe-VLA框架——一种融合认知与记忆的视觉-语言-动作框架，该框架利用大规模人类第一人称数据来学习多功能的探索与操作先验。我们的框架集成了一个用于自主子任务转换的认知辅助头模块，以及一个通过融合本体感觉与视觉时序上下文来维持自我与环境一致感知的双轨记忆系统。通过将人类与机器人的手眼协调行为对齐到统一的第一人称动作空间中，我们分三个阶段逐步训练模型。在轮式人形机器人上进行的广泛实验表明，该方法在跨越多种主动感知场景的多样化长时程任务中展现出强大的鲁棒性与适应性。",
    "abstract_en": "Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios."
  },
  "2602.04522": {
    "title_zh": "基于互补性的统一方法在刚体操作与运动预测中的应用",
    "abstract_zh": "在非结构化环境中的机器人操作要求规划器能够同时推理自由空间运动与环境中持续的摩擦接触。现有的（局部）规划与仿真框架通常将这些状态分离，或依赖于简化的接触表示，尤其是在建模非凸或分布式接触区域时。这种近似限制了接触模式转换的保真度，并阻碍了在实时条件下稳健执行富含接触行为的任务。本文提出了一种统一的离散时间建模框架（Unicomp），用于机器人操作，该框架在单一数学形式中一致地捕捉自由运动与摩擦接触。基于互补性的刚体动力学，我们将自由空间运动与接触交互建模为耦合的线性和非线性互补问题，从而在不强制固定接触假设的情况下实现接触模式之间的原则性转换。对于平面区域接触，我们从最大功率耗散原理推导出一个摩擦接触模型，其中可接受的接触力矩集合由椭球极限曲面表示。这种表示捕捉了耦合的力-力矩效应，包括扭转摩擦，同时不依赖于接触区域底层的压力分布。由此产生的公式产生了一个离散时间预测模型，该模型通过二次约束关联广义速度与接触力矩，适用于基于优化的实时规划。实验结果表明，所提出的方法能够在交互速度下实现稳定、物理一致的行为，覆盖从平面推动到富含接触的全身操作等多种任务。",
    "abstract_en": "Robotic manipulation in unstructured environments requires planners to reason jointly about free-space motion and sustained, frictional contact with the environment. Existing (local) planning and simulation frameworks typically separate these regimes or rely on simplified contact representations, particularly when modeling non-convex or distributed contact patches. Such approximations limit the fidelity of contact-mode transitions and hinder the robust execution of contact-rich behaviors in real time. This paper presents a unified discrete-time modeling framework for robotic manipulation that consistently captures both free motion and frictional contact within a single mathematical formalism (Unicomp). Building on complementarity-based rigid-body dynamics, we formulate free-space motion and contact interactions as coupled linear and nonlinear complementarity problems, enabling principled transitions between contact modes without enforcing fixed-contact assumptions. For planar patch contact, we derive a frictional contact model from the maximum power dissipation principle in which the set of admissible contact wrenches is represented by an ellipsoidal limit surface. This representation captures coupled force-moment effects, including torsional friction, while remaining agnostic to the underlying pressure distribution across the contact patch. The resulting formulation yields a discrete-time predictive model that relates generalized velocities and contact wrenches through quadratic constraints and is suitable for real-time optimization-based planning. Experimental results show that the proposed approach enables stable, physically consistent behavior at interactive speeds across tasks, from planar pushing to contact-rich whole-body maneuvers."
  },
  "2602.04515": {
    "title_zh": "EgoActor：通过视觉语言模型将任务规划落地为具身机器人的空间感知自我中心动作",
    "abstract_zh": "在现实世界中部署具身机器人面临根本性挑战，因为它需要在部分信息观测和动态变化的环境下，紧密整合感知、移动和操作能力，并能在不同类型子任务间稳健切换。为应对这些挑战，我们提出一项新颖任务——EgoActing，要求将高层指令直接转化为多样、精确且具备空间感知的机器人动作。我们进一步通过引入EgoActor来实例化该任务，这是一个统一且可扩展的视觉语言模型，能够实时协调感知与执行，预测移动基元（如行走、转向、侧移、高度调整）、头部运动、操作指令以及人机交互动作。我们利用来自真实世界演示的纯RGB自我中心视角数据、空间推理问答以及仿真环境演示的广泛监督，使EgoActor能够做出稳健的情境感知决策，并以8B和4B参数模型实现流畅的动作推断（耗时低于1秒）。在仿真和真实环境中的大量评估表明，EgoActor有效桥接了抽象任务规划与具体运动执行，并能泛化至多样化任务及未见过的环境。",
    "abstract_en": "Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments."
  },
  "2602.04411": {
    "title_zh": "自演化的具身人工智能",
    "abstract_zh": "具身人工智能是由智能体与其环境通过主动感知、具身认知与行动交互形成的智能系统。现有具身人工智能仍局限于人工设定的框架，其中智能体基于给定记忆进行训练，并为特定任务构建模型，使固定具身形态与相对静态环境交互。此类方法难以适应具有可变具身形态和动态开放环境的真实世界场景。本文提出自演化具身人工智能这一新范式，智能体能够根据自身状态与环境变化，通过记忆自我更新、任务自主切换、环境自我预测、具身自适应以及模型自演化实现自主运作，旨在达成持续自适应与自主进化的智能。具体而言，我们阐述了自演化具身人工智能的定义、框架、组件与机制，系统综述了已实现组件的先进研究成果，探讨了实际应用场景，并指出了未来研究方向。我们相信，自演化具身人工智能能使智能体以类人方式自主学习和与环境交互，并为实现通用人工智能提供全新视角。",
    "abstract_en": "Embodied Artificial Intelligence (AI) is an intelligent system formed by agents and their environment through active perception, embodied cognition, and action interaction. Existing embodied AI remains confined to human-crafted setting, in which agents are trained on given memory and construct models for given tasks, enabling fixed embodiments to interact with relatively static environments. Such methods fail in in-the-wild setting characterized by variable embodiments and dynamic open environments. This paper introduces self-evolving embodied AI, a new paradigm in which agents operate based on their changing state and environment with memory self-updating, task self-switching, environment self-prediction, embodiment self-adaptation, and model self-evolution, aiming to achieve continually adaptive intelligence with autonomous evolution. Specifically, we present the definition, framework, components, and mechanisms of self-evolving embodied AI, systematically review state-of-the-art works for realized components, discuss practical applications, and point out future research directions. We believe that self-evolving embodied AI enables agents to autonomously learn and interact with environments in a human-like manner and provide a new perspective toward general artificial intelligence."
  },
  "2602.04315": {
    "title_zh": "GeneralVLA：具备知识引导轨迹规划的通用视觉-语言-动作模型",
    "abstract_zh": "大型基础模型已在视觉和语言领域展现出对复杂问题的强大开放世界泛化能力，但机器人学中尚未实现类似水平的泛化。一个根本性挑战在于这些模型表现出有限的零样本能力，这阻碍了它们有效泛化至未见场景。本研究提出GeneralVLA（具备知识引导轨迹规划的通用视觉-语言-动作模型），这是一种分层视觉-语言-动作模型，能更有效地利用基础模型的泛化能力，实现零样本操控并自动生成机器人学数据。具体而言，我们研究一类分层VLA模型：高层ASM（可供性分割模块）经微调后感知场景的图像关键点可供性；中层3DAgent执行任务理解、技能知识与轨迹规划，生成指示期望机器人末端执行器轨迹的三维路径。该中间三维路径预测随后作为低层三维感知控制策略的引导，实现精确操控。相较于其他方法，我们的方法无需真实世界机器人数据收集或人工示范，使其能更高效地扩展至多样化任务与视角。实验表明，GeneralVLA成功为14项任务生成轨迹，显著优于VoxPoser等先进方法。所生成的示范数据训练出的行为克隆策略，比基于人工示范或VoxPoser、Scaling-up、Code-As-Policies生成数据训练的策略更具鲁棒性。我们相信GeneralVLA可成为兼具机器人数据生成与零样本场景下解决新任务能力的可扩展方法。代码：https://github.com/AIGeeksGroup/GeneralVLA。项目网站：https://aigeeksgroup.github.io/GeneralVLA。",
    "abstract_en": "Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is that the models exhibit limited zero-shot capability, which hampers their ability to generalize effectively to unseen scenarios. In this work, we propose GeneralVLA (Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning), a hierarchical vision-language-action (VLA) model that can be more effective in utilizing the generalization of foundation models, enabling zero-shot manipulation and automatically generating data for robotics. In particular, we study a class of hierarchical VLA model where the high-level ASM (Affordance Segmentation Module) is finetuned to perceive image keypoint affordances of the scene; the mid-level 3DAgent carries out task understanding, skill knowledge, and trajectory planning to produce a 3D path indicating the desired robot end-effector trajectory. The intermediate 3D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Compared to alternative approaches, our method requires no real-world robotic data collection or human demonstration, making it much more scalable to diverse tasks and viewpoints. Empirically, GeneralVLA successfully generates trajectories for 14 tasks, significantly outperforming state-of-the-art methods such as VoxPoser. The generated demonstrations can train more robust behavior cloning policies than training with human demonstrations or from data generated by VoxPoser, Scaling-up, and Code-As-Policies. We believe GeneralVLA can be the scalable method for both generating data for robotics and solving novel tasks in a zero-shot setting. Code: https://github.com/AIGeeksGroup/GeneralVLA. Website: https://aigeeksgroup.github.io/GeneralVLA."
  },
  "2602.04243": {
    "title_zh": "视角至关重要：利用掩码自编码器动态优化视觉操控的视角",
    "abstract_zh": "机器人操控仍面临挑战，而模仿学习（IL）使机器人能够从专家演示中学习任务。当前的IL方法通常依赖于固定的相机设置，即相机被手动放置在静态位置，这极大地限制了系统的适应性和覆盖范围。受人类主动感知的启发——人类会动态调整视角以捕捉最相关且噪声最少的信息，我们提出了MAE-Select，一种用于单相机机器人系统中主动视角选择的新颖框架。MAE-Select充分利用了预训练的多视角掩码自编码器表示，并在每个时间块动态选择下一个最具信息量的视角，无需标注视角数据。大量实验表明，MAE-Select提升了单相机系统的能力，在某些情况下甚至超越了多相机设置。项目将在https://mae-select.github.io上公开。",
    "abstract_en": "Robotic manipulation continues to be a challenge, and imitation learning (IL) enables robots to learn tasks from expert demonstrations. Current IL methods typically rely on fixed camera setups, where cameras are manually positioned in static locations, imposing significant limitations on adaptability and coverage. Inspired by human active perception, where humans dynamically adjust their viewpoint to capture the most relevant and least noisy information, we propose MAE-Select, a novel framework for active viewpoint selection in single-camera robotic systems. MAE-Select fully leverages pre-trained multi-view masked autoencoder representations and dynamically selects the next most informative viewpoint at each time chunk without requiring labeled viewpoints. Extensive experiments demonstrate that MAE-Select improves the capabilities of single-camera systems and, in some cases, even surpasses multi-camera setups. The project will be available at https://mae-select.github.io."
  },
  "2602.04231": {
    "title_zh": "GeoLanG：基于统一RGB-D多模态学习的几何感知语言引导抓取",
    "abstract_zh": "语言引导抓取作为一种通过自然语言指令使机器人识别和操作目标对象的有前景范式，但在杂乱或遮挡场景中仍面临巨大挑战。现有方法通常依赖将物体感知与抓取分离的多阶段流程，导致跨模态融合有限、计算冗余，且在杂乱、遮挡或低纹理场景中泛化能力差。为应对这些局限，我们提出了GeoLanG，这是一个基于CLIP架构构建的端到端多任务框架，它将视觉和语言输入统一到共享表示空间中，以实现鲁棒的语义对齐和增强的泛化能力。为提升遮挡和低纹理条件下的目标辨别力，我们通过深度引导几何模块（DGGM）探索了更有效的深度信息利用方式，该模块将深度转换为显式几何先验，并在不增加额外计算开销的情况下将其注入注意力机制。此外，我们提出了自适应密集通道集成方法，可自适应平衡多层特征的贡献，以生成更具区分性和泛化能力的视觉表示。在OCID-VLG数据集以及仿真和真实硬件上的大量实验表明，GeoLanG能够在复杂、杂乱的环境中实现精确且鲁棒的语言引导抓取，为在真实世界以人为中心的环境中实现更可靠的多模态机器人操作铺平了道路。",
    "abstract_en": "Language-guided grasping has emerged as a promising paradigm for enabling robots to identify and manipulate target objects through natural language instructions, yet it remains highly challenging in cluttered or occluded scenes. Existing methods often rely on multi-stage pipelines that separate object perception and grasping, which leads to limited cross-modal fusion, redundant computation, and poor generalization in cluttered, occluded, or low-texture scenes. To address these limitations, we propose GeoLanG, an end-to-end multi-task framework built upon the CLIP architecture that unifies visual and linguistic inputs into a shared representation space for robust semantic alignment and improved generalization. To enhance target discrimination under occlusion and low-texture conditions, we explore a more effective use of depth information through the Depth-guided Geometric Module (DGGM), which converts depth into explicit geometric priors and injects them into the attention mechanism without additional computational overhead. In addition, we propose Adaptive Dense Channel Integration, which adaptively balances the contributions of multi-layer features to produce more discriminative and generalizable visual representations. Extensive experiments on the OCID-VLG dataset, as well as in both simulation and real-world hardware, demonstrate that GeoLanG enables precise and robust language-guided grasping in complex, cluttered environments, paving the way toward more reliable multimodal robotic manipulation in real-world human-centric settings."
  },
  "2602.04864": {
    "title_zh": "当LLaVA遇见物体：视觉语言模型的令牌组合",
    "abstract_zh": "当前的自回归视觉语言模型通常依赖大量视觉令牌来表示图像，导致在推理时尤其需要更多计算资源。为解决这一问题，我们提出了Mask-LLaVA框架，该框架利用不同层级的视觉特征，为自回归视觉语言模型创建紧凑且信息丰富的视觉表示。具体而言，我们将基于掩码的物体表示与全局令牌和局部补丁令牌相结合。尽管训练时使用所有令牌，但结果表明，所得模型在测试时能够灵活地减少特别是基于掩码的物体令牌数量，从而允许在推理过程中调整令牌数量，而无需重新训练模型且性能不会显著下降。我们在标准基准测试套件上评估了所提方法，结果显示其与当前令牌高效方法竞争激烈，且仅使用一小部分视觉令牌即可达到与原始LLaVA基线相当的性能。我们的分析表明，结合多层级特征能够以更少的令牌实现高效学习，同时允许在测试时动态选择令牌以保持良好的性能。",
    "abstract_en": "Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance."
  },
  "2602.04849": {
    "title_zh": "结构智能体：一种人工智能分子编辑器",
    "abstract_zh": "我们介绍了结构智能体（El Agente Estructural），这是一种多模态、自然语言驱动的几何生成与操作智能体，专为自主化学与分子建模而设计。与通过生成模型进行分子生成或编辑不同，结构智能体通过整合一套全面的领域知识工具和视觉语言模型，模拟人类专家在三维空间中直接操作分子系统的方式。这种设计使得无需重建庞大的核心分子框架，即可实现对原子或官能团替换、原子连接性以及立体化学的精确控制。通过一系列代表性案例研究，我们展示了结构智能体能够在广泛的现实场景中实现具有化学意义的几何操作。这些场景包括位点选择性功能化、配体结合、配体交换、立体化学控制的结构构建、异构体互变、片段级结构分析、基于图像从示意反应机理生成结构，以及机理驱动的几何生成与修改。这些示例说明了多模态推理与专业几何感知工具相结合，如何支持超越结构生成的交互式、情境感知的分子建模。展望未来，将结构智能体集成到自主多智能体量子化学平台El Agente Quntur中，通过添加用于三维结构生成和编辑的复杂工具，进一步增强了其能力。",
    "abstract_en": "We present El Agente Estructural, a multimodal, natural-language-driven geometry-generation and manipulation agent for autonomous chemistry and molecular modelling. Unlike molecular generation or editing via generative models, Estructural mimics how human experts directly manipulate molecular systems in three dimensions by integrating a comprehensive set of domain-informed tools and vision-language models. This design enables precise control over atomic or functional group replacements, atomic connectivity, and stereochemistry without the need to rebuild extensive core molecular frameworks. Through a series of representative case studies, we demonstrate that Estructural enables chemically meaningful geometry manipulation across a wide range of real-world scenarios. These include site-selective functionalization, ligand binding, ligand exchange, stereochemically controlled structure construction, isomer interconversion, fragment-level structural analysis, image-guided generation of structures from schematic reaction mechanisms, and mechanism-driven geometry generation and modification. These examples illustrate how multimodal reasoning, when combined with specialized geometry-aware tools, supports interactive and context-aware molecular modelling beyond structure generation. Looking forward, the integration of Estructural into El Agente Quntur, an autonomous multi-agent quantum chemistry platform, enhances its capabilities by adding sophisticated tools for the generation and editing of three-dimensional structures."
  },
  "2602.04802": {
    "title_zh": "VISTA-Bench：视觉语言模型真的能像理解纯文本一样理解图像中的文本吗？",
    "abstract_zh": "视觉语言模型（VLMs）在跨文本与视觉输入的多模态理解方面取得了显著成就，但现有基准测试主要关注纯文本查询。在现实场景中，语言也常以图像中嵌入的可视化文本形式出现，这引发了一个问题：当前的VLMs是否能同等处理这类输入请求。我们推出了VISTA-Bench，这是一个从多模态感知、推理到单模态理解领域的系统性基准测试。它通过在受控渲染条件下对比纯文本与可视化文本问题，评估模型对可视化文本的理解能力。对超过20个代表性VLMs的广泛评估揭示了一个显著的模态差距：在纯文本查询上表现良好的模型，当相同语义内容以可视化文本呈现时，性能往往大幅下降。这种差距随着感知难度的增加而进一步放大，突显了模型对渲染变化的敏感性，尽管语义内容保持不变。总体而言，VISTA-Bench提供了一个原则性的评估框架，用于诊断这一局限性，并指导在标记化文本与像素之间实现更统一语言表征的进展。源数据集可在https://github.com/QingAnLiu/VISTA-Bench获取。",
    "abstract_en": "Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench."
  },
  "2602.04739": {
    "title_zh": "多模态大语言模型中的对齐漂移：对八个模型版本有害性的两阶段纵向评估",
    "abstract_zh": "多模态大语言模型（MLLMs）正越来越多地部署于实际系统中，但其在对抗性提示下的安全性仍缺乏深入探究。我们采用一个由26名专业红队人员编写的固定基准测试集（包含726个对抗性提示），对MLLMs的有害性进行了两阶段评估。第一阶段评估了GPT-4o、Claude Sonnet 3.5、Pixtral 12B和Qwen VL Plus；第二阶段评估了它们的后续版本（GPT-5、Claude Sonnet 4.5、Pixtral Large和Qwen Omni），共获得82,256个人类有害性评分。不同模型系列间存在显著且持续的差异：Pixtral模型始终最为脆弱，而Claude模型因高拒绝率显得最安全。攻击成功率（ASR）显示出明显的对齐漂移现象：GPT和Claude模型在代际更新中ASR有所上升，而Pixtral和Qwen则呈现小幅下降。模态效应也随时间变化：第一阶段中纯文本提示更为有效，而第二阶段则出现模型特定的模式，GPT-5和Claude 4.5在不同模态下表现出近乎同等的脆弱性。这些发现表明，MLLMs的有害性在模型更新中既非一致也非稳定，凸显了需要建立纵向、多模态的基准测试来追踪其安全行为的演变。",
    "abstract_en": "Multimodal large language models (MLLMs) are increasingly deployed in real-world systems, yet their safety under adversarial prompting remains underexplored. We present a two-phase evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red teamers. Phase 1 assessed GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus; Phase 2 evaluated their successors (GPT-5, Claude Sonnet 4.5, Pixtral Large, and Qwen Omni) yielding 82,256 human harm ratings. Large, persistent differences emerged across model families: Pixtral models were consistently the most vulnerable, whereas Claude models appeared safest due to high refusal rates. Attack success rates (ASR) showed clear alignment drift: GPT and Claude models exhibited increased ASR across generations, while Pixtral and Qwen showed modest decreases. Modality effects also shifted over time: text-only prompts were more effective in Phase 1, whereas Phase 2 produced model-specific patterns, with GPT-5 and Claude 4.5 showing near-equivalent vulnerability across modalities. These findings demonstrate that MLLM harmlessness is neither uniform nor stable across updates, underscoring the need for longitudinal, multimodal benchmarks to track evolving safety behaviour."
  },
  "2602.04712": {
    "title_zh": "SAR-RAG：通过语义搜索、检索与多模态大语言模型生成的自动目标识别视觉问答",
    "abstract_zh": "本文提出了一种视觉上下文图像检索增强生成（ImageRAG）辅助的AI智能体，用于合成孔径雷达（SAR）的自动目标识别（ATR）。SAR是一种应用于国防与安全领域的遥感方法，用于检测和监控军事车辆的位置，这些车辆在图像中可能难以区分。研究人员已广泛研究SAR ATR，以提升对车辆类型、特征及尺寸的区分与识别能力。测试样本可与已知车辆目标类型进行比较，从而改进识别任务。新方法增强了神经网络、Transformer注意力机制及多模态大语言模型的能力。我们开发了一种智能AI方法，利用一组定义的工具，例如在相似样本库中进行搜索。我们提出的方法——SAR检索增强生成（SAR-RAG），将多模态大语言模型（MLLM）与语义嵌入向量数据库相结合，支持对具有已知特性的图像范例进行上下文搜索。通过检索具有已知真实目标类型的过往图像样本，SAR-RAG系统能够比较相似的车辆类别，从而提升ATR预测精度。我们通过搜索与检索指标、分类准确率以及车辆尺寸的数值回归进行评估。这些指标均表明，将SAR-RAG作为附加的ATR记忆库集成到MLLM基线方法中后，性能得到全面提升。",
    "abstract_en": "We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank."
  },
  "2602.04699": {
    "title_zh": "基于视觉语言模型的无标注航天器检测与分割",
    "abstract_zh": "视觉语言模型在开放世界零样本视觉识别任务中展现出卓越性能，但其在航天领域的应用潜力尚未得到充分探索。在航天场景中，由于低可见度、光照变化及目标与行星背景融合等因素，精确的人工标注尤为困难。因此，开发无需大量人工标注即可检测与分割航天器及轨道目标的方法至关重要。本研究提出一种基于视觉语言模型的无标注空间目标检测与分割流程。该方法首先利用预训练的视觉语言模型自动为少量未标注真实数据生成伪标签，随后通过师生标签蒸馏框架，利用这些伪标签训练轻量化模型。尽管伪标签本身存在固有噪声，但蒸馏过程相较于直接零样本视觉语言模型推理仍带来显著的性能提升。在SPARK-2024、SPEED+和TANGO数据集上的分割任务实验评估表明，平均精度最高可提升10个百分点。代码与模型已发布于https://github.com/giddyyupp/annotation-free-spacecraft-segmentation。",
    "abstract_en": "Vision Language Models (VLMs) have demonstrated remarkable performance in open-world zero-shot visual recognition. However, their potential in space-related applications remains largely unexplored. In the space domain, accurate manual annotation is particularly challenging due to factors such as low visibility, illumination variations, and object blending with planetary backgrounds. Developing methods that can detect and segment spacecraft and orbital targets without requiring extensive manual labeling is therefore of critical importance. In this work, we propose an annotation-free detection and segmentation pipeline for space targets using VLMs. Our approach begins by automatically generating pseudo-labels for a small subset of unlabeled real data with a pre-trained VLM. These pseudo-labels are then leveraged in a teacher-student label distillation framework to train lightweight models. Despite the inherent noise in the pseudo-labels, the distillation process leads to substantial performance gains over direct zero-shot VLM inference. Experimental evaluations on the SPARK-2024, SPEED+, and TANGO datasets on segmentation tasks demonstrate consistent improvements in average precision (AP) by up to 10 points. Code and models are available at https://github.com/giddyyupp/annotation-free-spacecraft-segmentation."
  },
  "2602.04672": {
    "title_zh": "AGILE：基于智能体生成从视频重建手-物交互",
    "abstract_zh": "从单目视频重建动态的手-物交互对于灵巧操作数据收集以及为机器人与虚拟现实创建逼真的数字孪生至关重要。然而，现有方法面临两大障碍：(1) 依赖神经渲染常导致在严重遮挡下产生碎片化、无法直接用于仿真的几何体；(2) 依赖脆弱的运动恢复结构初始化，导致对野外拍摄视频频繁失败。为克服这些局限，我们提出了AGILE，一个将交互学习范式从重建转向智能体生成的鲁棒框架。首先，我们采用智能体流程，通过视觉语言模型引导生成模型合成完整、封闭的高保真纹理物体网格，不受视频遮挡影响。其次，完全绕过脆弱的运动恢复结构，我们提出一种鲁棒的锚定-跟踪策略：利用基础模型在单个交互起始帧初始化物体姿态，并通过生成资产与视频观测间的强视觉相似性进行时序传播。最后，通过接触感知优化整合语义、几何与交互稳定性约束，确保物理合理性。在HO3D、DexYCB及野外视频上的大量实验表明，AGILE在全局几何精度上超越基线方法，并在现有方法常失效的挑战性序列中展现出卓越的鲁棒性。通过优先保证物理有效性，我们的方法可生成可直接用于仿真的资产，并已通过真实到仿真的重定向在机器人应用中验证。",
    "abstract_en": "Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications."
  },
  "2602.04657": {
    "title_zh": "PIO-FVLM：从推理目标视角重新审视用于VLM加速的无训练视觉令牌缩减",
    "abstract_zh": "近年来，通过减少视觉-语言模型（VLMs）中的冗余视觉令牌以加速VLM推理已成为热门话题。然而，现有方法大多依赖于基于视觉令牌间相似性或跨模态视觉-文本相似性构建的启发式规则，这导致其在压缩性能和实际部署中存在一定局限性。相比之下，我们从推理目标的角度出发，提出了PIO-FVLM，将视觉令牌压缩转化为保持输出结果不变性的问题，并依据令牌对此目标的重要性进行筛选。具体而言，我们通过设计的层局部代理损失（一种从当前层到最终结果的粗略约束）生成令牌级梯度显著性，并以此指导视觉令牌重新排序。随后，遵循非极大值抑制（NMS）原则选取最有价值的视觉令牌。所提出的PIO-FVLM无需训练，且与FlashAttention兼容，便于实际应用与部署。它可作为无编码器方法独立部署，也可与VisionZip等编码器压缩方法结合，作为编码器参与的方法使用。在LLaVA-Next-7B模型上，PIO-FVLM仅保留11.1%的视觉令牌，却能维持97.2%的原始性能，同时实现预填充速度提升2.67倍、推理速度提升2.11倍、计算量（FLOPs）降低6.22倍，并减少6.05倍的KV缓存开销。代码已开源：https://github.com/ocy1/PIO-FVLM。",
    "abstract_en": "Recently, reducing redundant visual tokens in vision-language models (VLMs) to accelerate VLM inference has emerged as a hot topic. However, most existing methods rely on heuristics constructed based on inter-visual-token similarity or cross-modal visual-text similarity, which gives rise to certain limitations in compression performance and practical deployment. In contrast, we propose PIO-FVLM from the perspective of inference objectives, which transforms visual token compression into preserving output result invariance and selects tokens primarily by their importance to this goal. Specially, vision tokens are reordered with the guidance of token-level gradient saliency generated by our designed layer-local proxy loss, a coarse constraint from the current layer to the final result. Then the most valuable vision tokens are selected following the non-maximum suppression (NMS) principle. The proposed PIO-FVLM is training-free and compatible with FlashAttention, friendly to practical application and deployment. It can be deployed independently as an encoder-free method, or combined with encoder compression approaches like VisionZip for use as an encoder-involved method. On LLaVA-Next-7B, PIO-FVLM retains just 11.1% of visual tokens but maintains 97.2% of the original performance, with a 2.67$\\times$ prefill speedup, 2.11$\\times$ inference speedup, 6.22$\\times$ lower FLOPs, and 6.05$\\times$ reduced KV Cache overhead. Our code is available at https://github.com/ocy1/PIO-FVLM."
  },
  "2602.04617": {
    "title_zh": "LEAD：面向忠实放射学报告生成的层级专家对齐解码",
    "abstract_zh": "放射学报告生成（RRG）旨在从医学图像中生成准确且连贯的诊断结果。尽管大型视觉语言模型（LVLM）提升了报告的流畅性与准确性，但它们存在幻觉问题，会生成看似合理但缺乏图像依据的病理细节。现有方法主要依赖外部知识引导来促进生成文本与视觉信息的对齐，然而这些方法往往忽视了预训练模型固有的解码先验和视觉-语言对齐偏差，且因依赖构建的引导而缺乏鲁棒性。本文提出层级专家对齐解码（LEAD），一种新颖的方法，旨在从根本上修改LVLM的解码轨迹。我们设计了一个多专家模块，用于提取不同的病理特征，并通过门控机制将其整合到每个解码器层中。这种层级架构使LLM能够在每个推理步骤中通过学习的门控函数咨询专家特征，从而动态纠正解码偏差，并引导生成过程朝向事实一致性。在多个公共数据集上进行的实验表明，LEAD方法在临床准确性指标上实现了有效提升，缓解了幻觉问题，同时保持了高质量生成。",
    "abstract_en": "Radiology Report Generation (RRG) aims to produce accurate and coherent diagnostics from medical images. Although large vision language models (LVLM) improve report fluency and accuracy, they exhibit hallucinations, generating plausible yet image-ungrounded pathological details. Existing methods primarily rely on external knowledge guidance to facilitate the alignment between generated text and visual information. However, these approaches often ignore the inherent decoding priors and vision-language alignment biases in pretrained models and lack robustness due to reliance on constructed guidance. In this paper, we propose Layer-wise Expert-aligned Decoding (LEAD), a novel method to inherently modify the LVLM decoding trajectory. A multiple experts module is designed for extracting distinct pathological features which are integrated into each decoder layer via a gating mechanism. This layer-wise architecture enables the LLM to consult expert features at every inference step via a learned gating function, thereby dynamically rectifying decoding biases and steering the generation toward factual consistency. Experiments conducted on multiple public datasets demonstrate that the LEAD method yields effective improvements in clinical accuracy metrics and mitigates hallucinations while preserving high generation quality."
  },
  "2602.09878": {
    "title_zh": "MVISTA-4D：具有测试时动作推理能力的视图一致4D世界模型，用于机器人操作",
    "abstract_zh": "基于世界模型的“先想象后行动”范式已成为机器人操作领域的一个有前景的方向，但现有方法通常仅支持纯图像预测或对部分3D几何进行推理，限制了其预测完整4D场景动态的能力。本文提出了一种新颖的具身4D世界模型，能够实现几何一致的任意视角RGBD生成：仅以单视角RGBD观测作为输入，该模型可想象剩余视角，进而通过反投影与融合，跨时间组装出更完整的3D结构。为高效学习多视角跨模态生成，我们显式设计了跨视角与跨模态特征融合机制，共同促进RGB与深度间的一致性，并强制实现跨视角的几何对齐。除预测外，将生成的未来状态转化为动作通常通过逆动力学处理，但由于同一状态转移可由多种动作解释，该问题具有不适定性。我们通过一种测试时动作优化策略解决此问题：该策略通过生成模型反向传播，推断出与预测未来最匹配的轨迹级潜在变量，并配合一个残差逆动力学模型，将这一轨迹先验转化为精确可执行的动作。在三个数据集上的实验表明，该方法在4D场景生成与下游操作任务中均表现优异，消融研究则为关键设计选择提供了实用见解。",
    "abstract_en": "World-model-based imagine-then-act becomes a promising paradigm for robotic manipulation, yet existing approaches typically support either purely image-based forecasting or reasoning over partial 3D geometry, limiting their ability to predict complete 4D scene dynamics. This work proposes a novel embodied 4D world model that enables geometrically consistent, arbitrary-view RGBD generation: given only a single-view RGBD observation as input, the model imagines the remaining viewpoints, which can then be back-projected and fused to assemble a more complete 3D structure across time. To efficiently learn the multi-view, cross-modality generation, we explicitly design cross-view and cross-modality feature fusion that jointly encourage consistency between RGB and depth and enforce geometric alignment across views. Beyond prediction, converting generated futures into actions is often handled by inverse dynamics, which is ill-posed because multiple actions can explain the same transition. We address this with a test-time action optimization strategy that backpropagates through the generative model to infer a trajectory-level latent best matching the predicted future, and a residual inverse dynamics model that turns this trajectory prior into accurate executable actions. Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation, and ablations provide practical insights into the key design choices."
  },
  "2602.09856": {
    "title_zh": "Code2World：通过可渲染代码生成的GUI世界模型",
    "abstract_zh": "自主GUI代理通过感知界面和执行动作与环境交互。作为一种虚拟沙盒，GUI世界模型通过支持基于动作的预测，赋予代理类人的预见能力。然而，现有的基于文本和像素的方法难以同时实现高视觉保真度和细粒度结构可控性。为此，我们提出了Code2World，一种通过可渲染代码生成来模拟下一视觉状态的视觉语言编码器。具体而言，为解决数据稀缺问题，我们构建了AndroidCode数据集，将GUI轨迹转换为高保真的HTML代码，并通过视觉反馈修正机制优化合成代码，生成了包含超过8万对高质量屏幕-动作对的语料库。为使现有视觉语言模型适应代码预测任务，我们首先进行监督微调作为格式布局遵循的冷启动，随后进一步应用渲染感知强化学习，以渲染结果作为奖励信号，强化视觉语义保真度和动作一致性。大量实验表明，Code2World-8B在下一UI预测任务中表现最佳，可与竞争模型GPT-5和Gemini-3-Pro-Image相媲美。值得注意的是，Code2World以灵活方式显著提升了下游导航任务的成功率，在AndroidWorld导航任务中将Gemini-2.5-Flash的性能提高了9.5%。代码已开源：https://github.com/AMAP-ML/Code2World。",
    "abstract_en": "Autonomous GUI agents interact with environments by perceiving interfaces and executing actions. As a virtual sandbox, the GUI World model empowers agents with human-like foresight by enabling action-conditioned prediction. However, existing text- and pixel-based approaches struggle to simultaneously achieve high visual fidelity and fine-grained structural controllability. To this end, we propose Code2World, a vision-language coder that simulates the next visual state via renderable code generation. Specifically, to address the data scarcity problem, we construct AndroidCode by translating GUI trajectories into high-fidelity HTML and refining synthesized code through a visual-feedback revision mechanism, yielding a corpus of over 80K high-quality screen-action pairs. To adapt existing VLMs into code prediction, we first perform SFT as a cold start for format layout following, then further apply Render-Aware Reinforcement Learning which uses rendered outcome as the reward signal by enforcing visual semantic fidelity and action consistency. Extensive experiments demonstrate that Code2World-8B achieves the top-performing next UI prediction, rivaling the competitive GPT-5 and Gemini-3-Pro-Image. Notably, Code2World significantly enhances downstream navigation success rates in a flexible manner, boosting Gemini-2.5-Flash by +9.5% on AndroidWorld navigation. The code is available at https://github.com/AMAP-ML/Code2World."
  },
  "2602.09849": {
    "title_zh": "BagelVLA：通过交错视觉-语言-动作生成增强长时程操作能力",
    "abstract_zh": "为具身智能体赋予任务推理、物理结果预见和精确动作生成的能力，是实现通用操作的关键。尽管近期的视觉-语言-动作模型已利用预训练基础模型，但它们通常孤立地关注语言规划或视觉预测，很少同时整合这两种能力来指导动作生成，导致在复杂长时程操作任务中表现欠佳。为弥补这一不足，我们提出了BagelVLA，一个在统一框架内集成语言规划、视觉预测和动作生成的模型。该模型基于预训练的统一理解与生成模型初始化，通过训练将文本推理和视觉预测直接交错融入动作执行循环中。为高效耦合这些模态，我们引入了残差流引导技术，该技术从当前观测初始化，并利用单步去噪提取预测性视觉特征，以极低延迟指导动作生成。大量实验表明，BagelVLA在多个模拟和真实世界基准测试中显著优于现有基线方法，尤其在需要多阶段推理的任务中表现突出。",
    "abstract_en": "Equipping embodied agents with the ability to reason about tasks, foresee physical outcomes, and generate precise actions is essential for general-purpose manipulation. While recent Vision-Language-Action (VLA) models have leveraged pre-trained foundation models, they typically focus on either linguistic planning or visual forecasting in isolation. These methods rarely integrate both capabilities simultaneously to guide action generation, leading to suboptimal performance in complex, long-horizon manipulation tasks. To bridge this gap, we propose BagelVLA, a unified model that integrates linguistic planning, visual forecasting, and action generation within a single framework. Initialized from a pretrained unified understanding and generative model, BagelVLA is trained to interleave textual reasoning and visual prediction directly into the action execution loop. To efficiently couple these modalities, we introduce Residual Flow Guidance (RFG), which initializes from current observation and leverages single-step denoising to extract predictive visual features, guiding action generation with minimal latency. Extensive experiments demonstrate that BagelVLA outperforms existing baselines by a significant margin on multiple simulated and real-world benchmarks, particularly in tasks requiring multi-stage reasoning."
  },
  "2602.09765": {
    "title_zh": "NavDreamer：视频模型作为零样本三维导航器",
    "abstract_zh": "以往的视觉-语言-动作模型在导航任务中面临关键限制：数据稀缺且多样，依赖劳动密集型收集，以及静态表示无法捕捉时间动态和物理规律。我们提出NavDreamer，一个基于视频的三维导航框架，利用生成式视频模型作为语言指令与导航轨迹之间的通用接口。我们的核心假设是，视频编码时空信息和物理动态的能力，结合互联网规模的数据可用性，能够在导航中实现强大的零样本泛化。为减轻生成预测的随机性，我们引入一种基于采样的优化方法，利用视觉语言模型对轨迹进行评分和选择。通过逆动力学模型，从生成的视频计划中解码出可执行的路径点以执行导航。为系统评估该范式在多种视频模型骨干上的表现，我们提出了一个涵盖物体导航、精确导航、空间定位、语言控制和场景推理的综合基准。大量实验证明了其在未见过的物体和环境中的稳健泛化能力，消融研究揭示，导航的高层决策特性使其特别适合基于视频的规划。",
    "abstract_en": "Previous Vision-Language-Action models face critical limitations in navigation: scarce, diverse data from labor-intensive collection and static representations that fail to capture temporal dynamics and physical laws. We propose NavDreamer, a video-based framework for 3D navigation that leverages generative video models as a universal interface between language instructions and navigation trajectories. Our main hypothesis is that video's ability to encode spatiotemporal information and physical dynamics, combined with internet-scale availability, enables strong zero-shot generalization in navigation. To mitigate the stochasticity of generative predictions, we introduce a sampling-based optimization method that utilizes a VLM for trajectory scoring and selection. An inverse dynamics model is employed to decode executable waypoints from generated video plans for navigation. To systematically evaluate this paradigm in several video model backbones, we introduce a comprehensive benchmark covering object navigation, precise navigation, spatial grounding, language control, and scene reasoning. Extensive experiments demonstrate robust generalization across novel objects and unseen environments, with ablation studies revealing that navigation's high-level decision-making nature makes it particularly suited for video-based planning."
  },
  "2602.09722": {
    "title_zh": "重新审视视觉-语言-动作模型的规模化：对齐、混合与正则化",
    "abstract_zh": "尽管视觉-语言-动作（VLA）模型在通用机器人控制方面展现出巨大潜力，但标准化的“数据规模化”方法是否适用于机器人领域仍不明确，因为机器人训练数据在具体实现、传感器和动作空间上天然具有异质性。本文对VLA模型的规模化进行了系统且受控的研究，重新审视了跨不同机器人预训练的核心训练选择。我们采用一个代表性的VLA框架，将视觉-语言骨干网络与流匹配相结合，在匹配条件下消融关键设计决策，并通过广泛的仿真和真实机器人实验进行评估。为提高真实世界结果的可靠性，我们引入了分组盲测集成协议，该协议使操作员对模型身份不知情，并将策略执行与结果判断分离，从而减少实验者偏差。我们的分析聚焦于VLA规模化的三个维度：（1）物理对齐：研究表明，统一的末端执行器相对动作表示对于实现稳健的跨实现迁移至关重要。（2）实现混合：我们发现，简单混合异构机器人数据集往往导致负迁移而非性能提升，这凸显了不加区分的数据规模化的脆弱性。（3）训练正则化：我们观察到，直观的策略（如感官丢弃和多阶段微调）在大规模训练中并不能持续提升性能。总之，本研究挑战了关于具身智能规模化的一些常见假设，并为从多样化机器人数据中训练大规模VLA策略提供了实用指导。项目网站：https://research.beingbeyond.com/rethink_vla",
    "abstract_en": "While Vision-Language-Action (VLA) models show strong promise for generalist robot control, it remains unclear whether -- and under what conditions -- the standard \"scale data\" recipe translates to robotics, where training data is inherently heterogeneous across embodiments, sensors, and action spaces. We present a systematic, controlled study of VLA scaling that revisits core training choices for pretraining across diverse robots. Using a representative VLA framework that combines a vision-language backbone with flow-matching, we ablate key design decisions under matched conditions and evaluate in extensive simulation and real-robot experiments. To improve the reliability of real-world results, we introduce a Grouped Blind Ensemble protocol that blinds operators to model identity and separates policy execution from outcome judgment, reducing experimenter bias. Our analysis targets three dimensions of VLA scaling. (1) Physical alignment: we show that a unified end-effector (EEF)-relative action representation is critical for robust cross-embodiment transfer. (2) Embodiment mixture: we find that naively pooling heterogeneous robot datasets often induces negative transfer rather than gains, underscoring the fragility of indiscriminate data scaling. (3) Training regularization: we observe that intuitive strategies, such as sensory dropout and multi-stage fine-tuning, do not consistently improve performance at scale. Together, this study challenge some common assumptions about embodied scaling and provide practical guidance for training large-scale VLA policies from diverse robotic data. Project website: https://research.beingbeyond.com/rethink_vla"
  },
  "2602.09657": {
    "title_zh": "AutoFly：面向野外无人机自主导航的视觉-语言-动作模型",
    "abstract_zh": "视觉语言导航（VLN）要求智能体通过结合语言指令与视觉观察来在环境中导航，是具身人工智能的核心任务之一。当前针对无人机（UAV）的VLN研究依赖于详细、预先指定的指令来引导无人机沿预定路径飞行。然而，现实世界的户外探索通常发生在未知环境中，无法提供详细的导航指令，仅能给予粗略的位置或方向性指导，这要求无人机通过持续规划与避障实现自主导航。为弥合这一差距，我们提出了AutoFly，一种用于无人机自主导航的端到端视觉-语言-动作（VLA）模型。AutoFly引入了一种伪深度编码器，可从RGB输入中提取深度感知特征以增强空间推理能力，并结合渐进式两阶段训练策略，有效对齐视觉、深度和语言表征与动作策略。此外，现有VLN数据集在现实世界自主导航方面存在根本性局限，主要源于其过度依赖显式指令跟随而非自主决策，以及真实世界数据不足。为解决这些问题，我们构建了一个新颖的自主导航数据集，通过以下方式将范式从指令跟随转向自主行为建模：（1）轨迹收集强调连续避障、自主规划和识别流程；（2）全面的真实世界数据整合。实验结果表明，AutoFly相比最先进的VLA基线方法成功率提高了3.9%，且在仿真和真实环境中均表现出一致的性能。",
    "abstract_en": "Vision-language navigation (VLN) requires intelligent agents to navigate environments by interpreting linguistic instructions alongside visual observations, serving as a cornerstone task in Embodied AI. Current VLN research for unmanned aerial vehicles (UAVs) relies on detailed, pre-specified instructions to guide the UAV along predetermined routes. However, real-world outdoor exploration typically occurs in unknown environments where detailed navigation instructions are unavailable. Instead, only coarse-grained positional or directional guidance can be provided, requiring UAVs to autonomously navigate through continuous planning and obstacle avoidance. To bridge this gap, we propose AutoFly, an end-to-end Vision-Language-Action (VLA) model for autonomous UAV navigation. AutoFly incorporates a pseudo-depth encoder that derives depth-aware features from RGB inputs to enhance spatial reasoning, coupled with a progressive two-stage training strategy that effectively aligns visual, depth, and linguistic representations with action policies. Moreover, existing VLN datasets have fundamental limitations for real-world autonomous navigation, stemming from their heavy reliance on explicit instruction-following over autonomous decision-making and insufficient real-world data. To address these issues, we construct a novel autonomous navigation dataset that shifts the paradigm from instruction-following to autonomous behavior modeling through: (1) trajectory collection emphasizing continuous obstacle avoidance, autonomous planning, and recognition workflows; (2) comprehensive real-world data integration. Experimental results demonstrate that AutoFly achieves a 3.9% higher success rate compared to state-of-the-art VLA baselines, with consistent performance across simulated and real environments."
  },
  "2602.09638": {
    "title_zh": "VideoAfford：基于多模态大语言模型从人-物交互视频中实现三维功能可及性接地",
    "abstract_zh": "三维功能可及性接地的目标在于突出三维物体上可操作区域，这对机器人操控至关重要。先前研究主要集中于从语言和图像等静态线索中学习可及性知识，难以提供足够的动态交互上下文以揭示时序与因果线索。为缓解此困境，我们收集了一个全面的基于视频的三维可及性数据集——VIDA，该数据集包含38K个人-物交互视频，涵盖16种可及性类型、38个物体类别以及22K个点云。基于VIDA，我们提出了一个强基线模型：VideoAfford，该模型通过增强可及性分割能力激活多模态大语言模型，在统一框架内同时实现世界知识推理与细粒度可及性接地。为提升动作理解能力，我们利用潜在动作编码器从人-物交互视频中提取动态交互先验。此外，我们引入了一种空间感知损失函数，使VideoAfford能够获取全面的三维空间知识。大量实验评估表明，我们的模型显著优于现有成熟方法，并展现出强大的开放世界泛化能力与可及性推理能力。所有数据集与代码将公开发布，以推动该领域的研究进展。",
    "abstract_en": "3D affordance grounding aims to highlight the actionable regions on 3D objects, which is crucial for robotic manipulation. Previous research primarily focused on learning affordance knowledge from static cues such as language and images, which struggle to provide sufficient dynamic interaction context that can reveal temporal and causal cues. To alleviate this predicament, we collect a comprehensive video-based 3D affordance dataset, \\textit{VIDA}, which contains 38K human-object-interaction videos covering 16 affordance types, 38 object categories, and 22K point clouds. Based on \\textit{VIDA}, we propose a strong baseline: VideoAfford, which activates multimodal large language models with additional affordance segmentation capabilities, enabling both world knowledge reasoning and fine-grained affordance grounding within a unified framework. To enhance action understanding capability, we leverage a latent action encoder to extract dynamic interaction priors from HOI videos. Moreover, we introduce a \\textit{spatial-aware} loss function to enable VideoAfford to obtain comprehensive 3D spatial knowledge. Extensive experimental evaluations demonstrate that our model significantly outperforms well-established methods and exhibits strong open-world generalization with affordance reasoning abilities. All datasets and code will be publicly released to advance research in this area."
  },
  "2602.09600": {
    "title_zh": "Hand2World：基于自由空间手势的自回归第一人称交互生成",
    "abstract_zh": "第一人称交互世界模型对于增强现实和具身人工智能至关重要，其视觉生成需以低延迟、几何一致性和长期稳定性响应用户输入。本研究探讨基于单张场景图像和自由空间手势的第一人称交互生成，旨在合成逼真视频，其中手部进入场景、与物体交互，并在头部运动下引发合理的世界动态。该设定带来若干根本性挑战，包括自由空间手势与高接触训练数据间的分布偏移、单目视图中手部运动与相机运动的模糊性，以及任意长度视频生成的需求。我们提出Hand2World，一个统一的自回归框架，通过基于投影3D手部网格的遮挡不变手部条件化应对这些挑战，使可见性和遮挡可从场景上下文中推断，而非编码于控制信号中。为稳定第一人称视角变化，我们通过逐像素普吕克射线嵌入显式注入相机几何，解耦相机运动与手部运动，防止背景漂移。进一步开发了全自动单目标注流程，并将双向扩散模型蒸馏为因果生成器，实现任意长度合成。在三个第一人称交互基准测试上的实验表明，该方法在感知质量和3D一致性方面取得显著提升，同时支持相机控制和长时程交互生成。",
    "abstract_en": "Egocentric interactive world models are essential for augmented reality and embodied AI, where visual generation must respond to user input with low latency, geometric consistency, and long-term stability. We study egocentric interaction generation from a single scene image under free-space hand gestures, aiming to synthesize photorealistic videos in which hands enter the scene, interact with objects, and induce plausible world dynamics under head motion. This setting introduces fundamental challenges, including distribution shift between free-space gestures and contact-heavy training data, ambiguity between hand motion and camera motion in monocular views, and the need for arbitrary-length video generation. We present Hand2World, a unified autoregressive framework that addresses these challenges through occlusion-invariant hand conditioning based on projected 3D hand meshes, allowing visibility and occlusion to be inferred from scene context rather than encoded in the control signal. To stabilize egocentric viewpoint changes, we inject explicit camera geometry via per-pixel Plücker-ray embeddings, disentangling camera motion from hand motion and preventing background drift. We further develop a fully automated monocular annotation pipeline and distill a bidirectional diffusion model into a causal generator, enabling arbitrary-length synthesis. Experiments on three egocentric interaction benchmarks show substantial improvements in perceptual quality and 3D consistency while supporting camera control and long-horizon interactive generation."
  },
  "2602.09583": {
    "title_zh": "面向可变形物体操作的偏好对齐视觉运动扩散策略",
    "abstract_zh": "人类自然地发展出对如何执行操作任务的偏好，这些偏好通常是微妙、个性化且难以明确表达的。尽管机器人需要考虑到这些偏好以提升个性化水平和用户满意度，但在机器人操作领域，尤其是在处理衣物和织物等可变形物体时，这一问题仍未得到充分探索。本研究探讨了如何利用有限的演示数据，调整预训练的视觉运动扩散策略以反映偏好行为。我们提出了RKO，一种新颖的偏好对齐方法，它结合了RPO和KTO两种近期框架的优势。我们在真实世界的布料折叠任务中，针对多种衣物和偏好设置，将RKO与常见的偏好学习框架（包括上述两种）以及基线标准扩散策略进行了比较评估。结果表明，偏好对齐策略（特别是RKO）相较于标准扩散策略微调方法，在性能和样本效率方面均表现出更优的表现。这些发现凸显了结构化偏好学习在扩展复杂可变形物体操作任务中个性化机器人行为方面的重要性和可行性。",
    "abstract_en": "Humans naturally develop preferences for how manipulation tasks should be performed, which are often subtle, personal, and difficult to articulate. Although it is important for robots to account for these preferences to increase personalization and user satisfaction, they remain largely underexplored in robotic manipulation, particularly in the context of deformable objects like garments and fabrics. In this work, we study how to adapt pretrained visuomotor diffusion policies to reflect preferred behaviors using limited demonstrations. We introduce RKO, a novel preference-alignment method that combines the benefits of two recent frameworks: RPO and KTO. We evaluate RKO against common preference learning frameworks, including these two, as well as a baseline vanilla diffusion policy, on real-world cloth-folding tasks spanning multiple garments and preference settings. We show that preference-aligned policies (particularly RKO) achieve superior performance and sample efficiency compared to standard diffusion policy fine-tuning. These results highlight the importance and feasibility of structured preference learning for scaling personalized robot behavior in complex deformable object manipulation tasks."
  },
  "2602.09534": {
    "title_zh": "AUHead：基于动作单元控制的逼真情感说话头部生成",
    "abstract_zh": "逼真的说话头部视频生成对于虚拟化身、电影制作和交互系统至关重要。现有方法因缺乏细粒度情感控制而难以实现细腻的情感表达。为解决此问题，我们提出了一种新颖的两阶段方法（AUHead），以从音频中解耦细粒度情感控制（即动作单元，AUs），并实现可控生成。在第一阶段，我们探索了大型音频-语言模型（ALMs）的AU生成能力，通过时空AU标记化和“先情感后AU”的思维链机制，旨在从原始语音中解耦AUs，有效捕捉微妙的情感线索。在第二阶段，我们提出了一种AU驱动的可控扩散模型，该模型基于AU序列合成逼真的说话头部视频。具体而言，我们首先将AU序列映射为结构化的二维面部表示以增强空间保真度，然后在交叉注意力模块中建模AU与视觉的交互。为实现灵活的AU-质量权衡控制，我们在推理过程中引入了AU解耦引导策略，进一步优化生成视频的情感表现力和身份一致性。在基准数据集上的结果表明，我们的方法在情感真实性、准确的唇部同步和视觉连贯性方面实现了有竞争力的性能，显著超越了现有技术。我们的实现代码可在https://github.com/laura990501/AUHead_ICLR获取。",
    "abstract_en": "Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage method (AUHead) to disentangle fine-grained emotion control, i.e. , Action Units (AUs), from audio and achieve controllable generation. In the first stage, we explore the AU generation abilities of large audio-language models (ALMs), by spatial-temporal AU tokenization and an \"emotion-then-AU\" chain-of-thought mechanism. It aims to disentangle AUs from raw speech, effectively capturing subtle emotional cues. In the second stage, we propose an AU-driven controllable diffusion model that synthesizes realistic talking-head videos conditioned on AU sequences. Specifically, we first map the AU sequences into the structured 2D facial representation to enhance spatial fidelity, and then model the AU-vision interaction within cross-attention modules. To achieve flexible AU-quality trade-off control, we introduce an AU disentanglement guidance strategy during inference, further refining the emotional expressiveness and identity consistency of the generated videos. Results on benchmark datasets demonstrate that our approach achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence, significantly surpassing existing techniques. Our implementation is available at https://github.com/laura990501/AUHead_ICLR"
  },
  "2602.08236": {
    "title_zh": "何时想象与想象多少：基于世界模型的自适应测试时缩放用于视觉空间推理",
    "abstract_zh": "尽管多模态大语言模型（MLLMs）发展迅速，但在正确答案依赖于场景在未见或替代视角下如何呈现时，视觉空间推理仍不可靠。近期研究通过结合世界模型进行视觉想象来增强推理，但何时想象真正必要、多少想象有益以及何时想象有害等问题仍未得到充分理解。实践中，不加区分的想象会增加计算负担，甚至因引入误导性证据而降低性能。本研究对测试时视觉想象作为空间推理的可控资源进行了深入分析，探讨了静态视觉证据何时足够、想象何时能改进推理，以及过度或不必要的想象如何影响准确性和效率。为支持此分析，我们提出了AVIC，一种基于世界模型的自适应测试时框架，该框架在选择性调用和缩放视觉想象前，明确推理当前视觉证据的充分性。在空间推理基准（SAT、MMSI）和具身导航基准（R2R）上的实验结果表明，想象在关键、边缘或有害场景中存在明确区分，且选择性控制能以显著更少的世界模型调用和语言标记匹配或超越固定想象策略。总体而言，我们的发现强调了分析和控制测试时想象对于高效可靠空间推理的重要性。",
    "abstract_en": "Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning."
  },
  "2602.07629": {
    "title_zh": "LCLA：面向视觉语言导航的语言条件化潜在对齐框架",
    "abstract_zh": "我们提出了LCLA（语言条件化潜在对齐），一种用于视觉语言导航的框架，通过学习将感知观察对齐到专家策略的潜在表示，构建模块化的感知-动作接口。首先利用特权状态信息训练专家策略，生成一个足以支持控制的潜在空间，随后冻结其潜在接口和动作头部。接着训练一个轻量级适配器，通过冻结的视觉语言模型将原始视觉-语言观察映射到专家的潜在空间中，从而将视觉运动学习问题简化为监督式潜在对齐，而非端到端的策略优化。这种解耦强化了感知与控制之间的稳定契约，使得专家行为能够在不同感知模态和环境变化中复用。我们在视觉语言室内导航任务中实例化LCLA并进行评估，结果表明对齐的潜在空间在分布内任务上表现优异，且能零样本泛化至未见过的环境、光照条件和视角，同时在推理阶段保持轻量化。",
    "abstract_en": "We propose LCLA (Language-Conditioned Latent Alignment), a framework for vision-language navigation that learns modular perception-action interfaces by aligning sensory observations to a latent representation of an expert policy. The expert is first trained with privileged state information, inducing a latent space sufficient for control, after which its latent interface and action head are frozen. A lightweight adapter is then trained to map raw visual-language observations, via a frozen vision-language model, into the expert's latent space, reducing the problem of visuomotor learning to supervised latent alignment rather than end-to-end policy optimization. This decoupling enforces a stable contract between perception and control, enabling expert behavior to be reused across sensing modalities and environmental variations. We instantiate LCLA and evaluate it on a vision-language indoor navigation task, where aligned latent spaces yield strong in-distribution performance and robust zero-shot generalization to unseen environments, lighting conditions, and viewpoints while remaining lightweight at inference time."
  },
  "2602.06427": {
    "title_zh": "弥合室内外鸿沟：面向最后几米的视觉中心化指令引导具身导航",
    "abstract_zh": "具身导航在最后一公里配送等现实应用中具有重要前景。然而，现有方法大多局限于室内或室外单一环境，并严重依赖精确坐标系等强假设。当前室外方法虽能通过粗粒度定位引导智能体接近目标，却无法实现通过特定建筑入口的细粒度进入，这在需要无缝室外到室内转换的实际部署场景中严重限制了其实用性。为弥合这一差距，我们提出一项新颖任务：无先验知识的室外到室内指令驱动具身导航。该框架明确摒弃对精确外部先验信息的依赖，要求智能体仅基于以指令引导的自我中心视觉观察进行导航。针对此任务，我们提出一种视觉中心化的具身导航框架，利用基于图像的提示驱动决策。此外，我们发布了该任务的首个开源数据集，其数据生成流程融合了轨迹条件视频合成技术。通过大量实验验证，我们提出的方法在成功率与路径效率等关键指标上均持续优于现有最先进基线模型。",
    "abstract_en": "Embodied navigation holds significant promise for real-world applications such as last-mile delivery. However, most existing approaches are confined to either indoor or outdoor environments and rely heavily on strong assumptions, such as access to precise coordinate systems. While current outdoor methods can guide agents to the vicinity of a target using coarse-grained localization, they fail to enable fine-grained entry through specific building entrances, critically limiting their utility in practical deployment scenarios that require seamless outdoor-to-indoor transitions. To bridge this gap, we introduce a novel task: out-to-in prior-free instruction-driven embodied navigation. This formulation explicitly eliminates reliance on accurate external priors, requiring agents to navigate solely based on egocentric visual observations guided by instructions. To tackle this task, we propose a vision-centric embodied navigation framework that leverages image-based prompts to drive decision-making. Additionally, we present the first open-source dataset for this task, featuring a pipeline that integrates trajectory-conditioned video synthesis into the data generation process. Through extensive experiments, we demonstrate that our proposed method consistently outperforms state-of-the-art baselines across key metrics including success rate and path efficiency."
  },
  "2602.06356": {
    "title_zh": "防微杜渐：基于回溯修正的鲁棒视觉语言导航",
    "abstract_zh": "视觉语言导航要求具身智能体理解自然语言指令并在复杂的连续三维环境中行进。然而，主流的模仿学习范式存在暴露偏差问题，即推理过程中的微小偏离会导致误差累积。尽管DAgger类方法试图通过纠正错误状态来缓解此问题，但我们发现一个关键局限：指令-状态失配。强制智能体从偏离路径的状态学习恢复动作，常会产生与原指令语义冲突的监督信号。针对这些挑战，我们提出BudVLN在线框架，该框架通过构建与当前状态分布匹配的监督信号，从策略滚动中学习。BudVLN通过反事实重锚定与决策条件监督合成实现回溯修正，利用测地线预言机合成源自有效历史状态的矫正轨迹，确保语义一致性。在标准R2R-CE和RxR-CE基准测试上的实验表明，BudVLN能持续缓解分布偏移，并在成功率和SPL指标上均达到最优性能。",
    "abstract_en": "Vision-Language Navigation (VLN) requires embodied agents to interpret natural language instructions and navigate through complex continuous 3D environments. However, the dominant imitation learning paradigm suffers from exposure bias, where minor deviations during inference lead to compounding errors. While DAgger-style approaches attempt to mitigate this by correcting error states, we identify a critical limitation: Instruction-State Misalignment. Forcing an agent to learn recovery actions from off-track states often creates supervision signals that semantically conflict with the original instruction. In response to these challenges, we introduce BudVLN, an online framework that learns from on-policy rollouts by constructing supervision to match the current state distribution. BudVLN performs retrospective rectification via counterfactual re-anchoring and decision-conditioned supervision synthesis, using a geodesic oracle to synthesize corrective trajectories that originate from valid historical states, ensuring semantic consistency. Experiments on the standard R2R-CE and RxR-CE benchmarks demonstrate that BudVLN consistently mitigates distribution shift and achieves state-of-the-art performance in both Success Rate and SPL."
  },
  "2602.05827": {
    "title_zh": "稀疏视频生成推动现实世界超视距视觉语言导航",
    "abstract_zh": "为何视觉语言导航必须依赖于详尽冗长的语言指令？尽管这些细节有助于决策制定，但它们从根本上违背了现实世界导航的目标。理想情况下，智能体应具备自主性，仅凭简单高层的意图引导在未知环境中导航。实现这一愿景带来了严峻挑战：超视距导航，即智能体必须在缺乏密集逐步指导的情况下定位远处不可见的目标。现有基于大语言模型的方法虽擅长遵循密集指令，但由于依赖短视距监督，常表现出短视行为。然而，单纯扩展监督视距会破坏大语言模型训练的稳定性。本研究发现，视频生成模型天生受益于长视距监督以对齐语言指令，使其特别适用于超视距导航任务。基于这一洞见，我们首次将视频生成模型引入该领域。然而，生成长达数十秒视频的过高延迟使其难以实际部署。为弥合这一差距，我们提出SparseVideoNav，通过生成跨越20秒视距的稀疏未来轨迹，实现亚秒级轨迹推断，相比未优化版本获得惊人的27倍加速。大量现实世界零样本实验表明，SparseVideoNav在超视距导航任务上的成功率达到最先进大语言模型基线的2.5倍，并首次在极具挑战性的夜间场景中实现了此类能力。",
    "abstract_en": "Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes."
  },
  "2602.05789": {
    "title_zh": "他者中心感知器：通过框架实例化从他者视觉先验中解耦他者中心推理",
    "abstract_zh": "随着视觉语言导航/动作等空间基础任务需求的日益增长，视觉语言模型（VLMs）中的他者中心感知能力正受到越来越多的关注。然而，在处理需要显式视角转换的他者中心空间查询时，VLMs仍显脆弱——这类查询的答案依赖于目标中心框架而非观察相机视角的推理。为此，我们提出了他者中心感知器，一种无需训练的策略：利用现成的几何专家从单张或多张图像中恢复度量三维状态，随后实例化一个与指令语义意图对齐的查询条件化他者中心参考框架。通过将重建的几何结构确定性地转换至目标框架，并以结构化、几何基础的表征提示骨干VLM，他者中心感知器将心理旋转从隐式推理卸载至显式计算。我们在多个空间推理基准上评估了不同骨干系列的他者中心感知器，观察到在他者中心任务上取得了一致且显著的提升（约10%），同时保持了强大的自我中心性能，并超越了经过空间感知微调的模型以及最先进的开源与专有模型。",
    "abstract_en": "With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction's semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models."
  },
  "2602.05467": {
    "title_zh": "MerNav：一种高度可泛化的记忆-执行-回顾框架，用于零样本目标导航",
    "abstract_zh": "视觉语言导航（VLN）是具身智能的基本能力之一，也是一个亟待解决的关键挑战。然而，现有方法在成功率（SR）和泛化性方面均不尽如人意：监督微调（SFT）方法通常能获得更高的SR，而无训练（TF）方法则往往泛化性更好，但两者难以兼得。为此，我们提出了一种记忆-执行-回顾框架。该框架包含三个部分：一个用于提供信息支持的分层记忆模块，一个用于常规决策与行动的执行模块，以及一个用于处理异常情况并修正行为的回顾模块。我们在目标导航任务上验证了该框架的有效性。在4个数据集上，与所有基线方法相比，我们的平均SR在TF和零样本（ZS）设置下分别实现了7%和5%的绝对提升。在最常用的HM3D_v0.1和更具挑战性的开放词汇数据集HM3D_OVON上，SR在ZS设置下分别提升了8%和6%。此外，在MP3D和HM3D_OVON数据集上，我们的方法不仅优于所有TF方法，还超越了所有SFT方法，在SR（分别提升5%和2%）和泛化性方面均实现了全面领先。",
    "abstract_en": "Visual Language Navigation (VLN) is one of the fundamental capabilities for embodied intelligence and a critical challenge that urgently needs to be addressed. However, existing methods are still unsatisfactory in terms of both success rate (SR) and generalization: Supervised Fine-Tuning (SFT) approaches typically achieve higher SR, while Training-Free (TF) approaches often generalize better, but it is difficult to obtain both simultaneously. To this end, we propose a Memory-Execute-Review framework. It consists of three parts: a hierarchical memory module for providing information support, an execute module for routine decision-making and actions, and a review module for handling abnormal situations and correcting behavior. We validated the effectiveness of this framework on the Object Goal Navigation task. Across 4 datasets, our average SR achieved absolute improvements of 7% and 5% compared to all baseline methods under TF and Zero-Shot (ZS) settings, respectively. On the most commonly used HM3D_v0.1 and the more challenging open vocabulary dataset HM3D_OVON, the SR improved by 8% and 6%, under ZS settings. Furthermore, on the MP3D and HM3D_OVON datasets, our method not only outperformed all TF methods but also surpassed all SFT methods, achieving comprehensive leadership in both SR (5% and 2%) and generalization."
  },
  "2602.09850": {
    "title_zh": "Reason-IAD：面向可解释工业异常检测的知识引导动态潜在推理框架",
    "abstract_zh": "工业异常检测需要对细粒度缺陷模式进行精确推理。然而，现有基于通用领域数据预训练的多模态大语言模型（MLLMs）往往难以捕捉特定类别的异常，从而限制了检测准确性和可解释性。为应对这些局限，本文提出Reason-IAD，一种用于可解释工业异常检测的知识引导动态潜在推理框架。Reason-IAD包含两个核心组件：首先，检索增强知识模块将特定类别的文本描述融入模型输入，实现对领域特定缺陷的上下文感知推理；其次，熵驱动潜在推理机制在紧凑潜在空间中利用可优化的潜在思考令牌进行迭代探索，并通过基于熵的奖励引导，以鼓励自信且稳定的预测。此外，动态视觉注入策略选择性地将信息量最大的图像块整合到潜在序列中，将推理过程导向对异常检测至关重要的区域。大量实验结果表明，Reason-IAD在各项指标上均持续优于现有最先进方法。代码将在https://github.com/chenpeng052/Reason-IAD公开提供。",
    "abstract_en": "Industrial anomaly detection demands precise reasoning over fine-grained defect patterns. However, existing multimodal large language models (MLLMs), pretrained on general-domain data, often struggle to capture category-specific anomalies, thereby limiting both detection accuracy and interpretability. To address these limitations, we propose Reason-IAD, a knowledge-guided dynamic latent reasoning framework for explainable industrial anomaly detection. Reason-IAD comprises two core components. First, a retrieval-augmented knowledge module incorporates category-specific textual descriptions into the model input, enabling context-aware reasoning over domain-specific defects. Second, an entropy-driven latent reasoning mechanism conducts iterative exploration within a compact latent space using optimizable latent think tokens, guided by an entropy-based reward that encourages confident and stable predictions. Furthermore, a dynamic visual injection strategy selectively incorporates the most informative image patches into the latent sequence, directing the reasoning process toward regions critical for anomaly detection. Extensive experimental results demonstrate that Reason-IAD consistently outperforms state-of-the-art methods. The code will be publicly available at https://github.com/chenpeng052/Reason-IAD."
  },
  "2602.09843": {
    "title_zh": "Kelix技术报告",
    "abstract_zh": "自回归大型语言模型（LLMs）通过将多样化任务表达为离散自然语言标记序列，并采用下一标记预测进行训练，实现了良好的扩展性，从而在自监督下统一了理解与生成能力。将这一范式扩展至多模态数据，需要跨模态的共享离散表示。然而，大多数视觉语言模型（VLMs）仍依赖混合接口：离散文本标记与连续的视觉变换器（ViT）特征配对。由于监督主要基于文本驱动，这些模型往往偏向于理解任务，无法充分利用非文本数据的大规模自监督学习。近期研究探索了离散视觉标记化，以实现完全自回归的多模态建模，在统一理解与生成方面展现出有前景的进展。然而，现有离散视觉标记常因编码容量有限而丢失信息，导致其理解能力明显弱于基于连续特征的VLMs。本文提出Kelix，一种完全离散的自回归统一模型，它弥合了离散与连续视觉表示之间的理解差距。",
    "abstract_en": "Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision. Extending this paradigm to multimodal data requires a shared, discrete representation across modalities. However, most vision-language models (VLMs) still rely on a hybrid interface: discrete text tokens paired with continuous Vision Transformer (ViT) features. Because supervision is largely text-driven, these models are often biased toward understanding and cannot fully leverage large-scale self-supervised learning on non-text data. Recent work has explored discrete visual tokenization to enable fully autoregressive multimodal modeling, showing promising progress toward unified understanding and generation. Yet existing discrete vision tokens frequently lose information due to limited code capacity, resulting in noticeably weaker understanding than continuous-feature VLMs. We present Kelix, a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations."
  },
  "2602.09825": {
    "title_zh": "SAKED：通过稳定性感知的知识增强解码缓解大型视觉语言模型中的幻觉问题",
    "abstract_zh": "大型视觉语言模型（LVLMs）中的幻觉问题在实际应用中带来了显著的安全性和可靠性风险。受人类在不确定或犹豫时更容易出错的观察启发，我们研究了模型内部知识的不稳定性如何导致LVLM产生幻觉。我们从注意力头、模型层和解码标记三个角度进行了广泛的实证分析，并识别出三种关键的幻觉模式：(i) 注意力头间的视觉激活漂移，(ii) 跨层的显著知识波动，以及(iii) 相邻输出标记间的视觉焦点分散。基于这些发现，我们提出了稳定性感知的知识增强解码（SAKED），该方法引入了层级的知识稳定性分数（KSS）来量化整个模型中的知识稳定性。通过对比最具稳定性感知和最不具稳定性感知的层，SAKED抑制了解码噪声，并动态利用最可靠的内部知识来生成忠实的标记。此外，SAKED无需训练，可以无缝集成到不同的架构中。大量实验表明，SAKED在各种模型、任务和基准测试中实现了最先进的幻觉缓解性能。",
    "abstract_en": "Hallucinations in Large Vision-Language Models (LVLMs) pose significant security and reliability risks in real-world applications. Inspired by the observation that humans are more error-prone when uncertain or hesitant, we investigate how instability in a model 's internal knowledge contributes to LVLM hallucinations. We conduct extensive empirical analyses from three perspectives, namely attention heads, model layers, and decoding tokens, and identify three key hallucination patterns: (i) visual activation drift across attention heads, (ii) pronounced knowledge fluctuations across layers, and (iii) visual focus distraction between neighboring output tokens. Building on these findings, we propose Stability-Aware Knowledge-Enhanced Decoding (SAKED), which introduces a layer-wise Knowledge Stability Score (KSS) to quantify knowledge stability throughout the model. By contrasting the most stability-aware and stability-agnostic layers, SAKED suppresses decoding noise and dynamically leverages the most reliable internal knowledge for faithful token generation. Moreover, SAKED is training-free and can be seamlessly integrated into different architectures. Extensive experiments demonstrate that SAKED achieves state-of-the-art performance for hallucination mitigation on various models, tasks, and benchmarks."
  },
  "2602.09701": {
    "title_zh": "GenSeg-R1：基于强化学习的视觉语言细粒度指代分割",
    "abstract_zh": "本文通过解耦的‘先推理后分割’流程研究细粒度指代图像分割。视觉语言模型接收图像和自然语言查询，对场景进行推理，并输出结构化空间提示：每个指代实例的边界框及两个内部关键点。冻结的可提示分割器将这些提示转换为高质量掩码。在GenSeg-R1框架中，我们使用组相对策略优化对Qwen3-VL模型进行微调，无需监督式推理链标注。在RefCOCOg验证集上，最佳模型达到0.7127 cIoU和0.7382 mIoU，显著超越基线模型，并在相同评估条件下优于Seg-Zero-7B。我们还提出GenSeg-R1-G变体，通过SAM 2在线奖励直接优化掩码质量，在GRefCOCO验证集上实现76.69%目标mIoU和82.40%负提示准确率，大幅超越现有模型。在ReasonSeg测试集上，GenSeg-R1-4B达到68.40% mIoU，领先Seg-Zero-7B和Seg-R1-7B。",
    "abstract_en": "We study fine-grained referring image segmentation via a decoupled reason-then-segment pipeline. A vision-language model (VLM) receives an image and a natural-language query, reasons about the scene, and emits structured spatial prompts: a bounding box plus two interior keypoints for every referred instance. A frozen promptable segmenter (SAM 2) converts these prompts into high-quality masks.   Within our GenSeg-R1 framework we finetune Qwen3-VL models (4B and 8B parameters) using Group Relative Policy Optimization (GRPO), requiring no supervised reasoning-chain annotations. On RefCOCOg validation our best model (GenSeg-R1-8B) achieves 0.7127 cIoU and 0.7382 mIoU, substantially outperforming the corresponding Qwen3-VL Instruct baselines (+15.3 and +21.9 points, respectively) and surpassing Seg-Zero-7B [3] by +3.3 cIoU under identical evaluation.   We further introduce GenSeg-R1-G, a variant trained on GRefCOCO [9] with a SAM 2 in-the-loop reward that directly optimizes mask quality. On GRefCOCO validation GenSeg-R1-G achieves 76.69% target mIoU with 82.40% accuracy on negative (no-target) prompts, substantially outperforming Seg-R1-7B and Seg-Zero-7B, which lack no-target detection capability. On ReasonSeg test, GenSeg-R1-4B reaches 68.40% mIoU, surpassing Seg-Zero-7B by +7.0 and Seg-R1-7B by +10.7 points."
  },
  "2602.09611": {
    "title_zh": "AGMark：面向大型视觉语言模型的注意力引导动态水印技术",
    "abstract_zh": "水印技术已成为大型视觉语言模型（LVLMs）中内容溯源与知识产权保护的关键解决方案。然而，视觉无关的水印可能引入视觉上无关的标记，并通过施加无差别的伪随机偏差破坏视觉基础。此外，现有的视觉专用水印依赖于对视觉关键权重的静态一次性估计，并在确定受保护标记比例时忽略了权重分布密度。这种设计未能考虑生成过程中视觉依赖性的动态变化，并可能在长尾部分引入低质量标记。为解决这些挑战，我们提出了注意力引导动态水印（AGMark），这是一种新颖的框架，可在严格保持视觉保真度的同时嵌入可检测信号。在每个解码步骤中，AGMark首先基于注意力权重动态识别视觉相关性的语义关键证据，并结合上下文感知的连贯性线索，从而产生更自适应且校准良好的证据权重分布。随后，它通过联合考虑不确定性感知（标记熵）和证据校准（权重密度）来确定语义关键标记的比例，从而实现自适应词汇划分以避免无关标记。实证结果证实，AGMark优于传统方法，显著提升了生成质量，并在生成后期阶段尤其增强了视觉语义保真度。该框架在保持高度竞争力的检测准确率（至少99.36% AUC）和强大的攻击鲁棒性（至少88.61% AUC）的同时，未牺牲推理效率，有效确立了可靠性保持的多模态水印新标准。",
    "abstract_en": "Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases. Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail. To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution. It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens. Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation. The framework maintains highly competitive detection accuracy (at least 99.36\\% AUC) and robust attack resilience (at least 88.61\\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking."
  },
  "2602.09609": {
    "title_zh": "Tele-Omni：面向视频生成与编辑的统一多模态框架",
    "abstract_zh": "近期基于扩散模型的视频生成技术在视觉保真度和时序连贯性方面取得了显著进展。然而，现有方法大多仍局限于特定任务，且主要依赖文本指令，难以在统一框架内处理多模态输入、上下文参考以及多样化的视频生成与编辑场景。此外，许多视频编辑方法依赖于针对单一操作精心设计的流程，这限制了其扩展性与组合性。本文提出Tele-Omni，一个统一的多模态视频生成与编辑框架，能够通过单一模型响应包含文本、图像和参考视频在内的多模态指令。Tele-Omni利用预训练的多模态大语言模型解析异构指令并推断结构化的生成或编辑意图，同时基于扩散模型的生成器根据这些结构化信号执行高质量视频合成。为实现跨异构视频任务的联合训练，我们提出一种任务感知的数据处理流程，将多模态输入统一为结构化指令格式，同时保留任务特定的约束条件。Tele-Omni支持广泛的视频中心任务，包括文本到视频生成、图像到视频生成、首尾帧视频生成、上下文视频生成以及上下文视频编辑。通过将指令解析与视频合成解耦，并结合任务感知的数据设计，Tele-Omni在保持强大时序连贯性与视觉一致性的同时，实现了灵活的多模态控制。实验结果表明，Tele-Omni在多项任务中均展现出具有竞争力的性能。",
    "abstract_en": "Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks."
  },
  "2602.09586": {
    "title_zh": "探索基于视觉-语言表征的光谱聚类方法",
    "abstract_zh": "光谱聚类是无监督数据分析中的一项强大技术。现有方法大多依赖单一模态，未能充分利用多模态表征中的丰富信息。受近期视觉-语言预训练成功的启发，本文将光谱聚类的研究范畴从单模态拓展至多模态领域。具体而言，我们提出神经正切核光谱聚类方法，该方法利用预训练视觉-语言模型中的跨模态对齐特性。通过以语义接近目标图像的积极名词作为神经正切核的锚点，我们将图像间的亲和度定义为视觉邻近性与语义重叠度的耦合。研究表明，该公式能增强簇内连接，同时抑制簇间的虚假关联，从而促进块对角结构的形成。此外，我们提出一种正则化亲和度扩散机制，可自适应地融合由不同提示词生成的亲和度矩阵。在涵盖经典数据集、大规模数据集、细粒度数据集及域偏移数据集的16个基准测试上的大量实验表明，本方法始终以显著优势超越现有最优技术。",
    "abstract_en": "Spectral clustering is known as a powerful technique in unsupervised data analysis. The vast majority of approaches to spectral clustering are driven by a single modality, leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of spectral clustering from a single-modal to a multi-modal regime. Particularly, we propose Neural Tangent Kernel Spectral Clustering that leverages cross-modal alignment in pre-trained vision-language models. By anchoring the neural tangent kernel with positive nouns, i.e., those semantically close to the images of interest, we arrive at formulating the affinity between images as a coupling of their visual proximity and semantic overlap. We show that this formulation amplifies within-cluster connections while suppressing spurious ones across clusters, hence encouraging block-diagonal structures. In addition, we present a regularized affinity diffusion mechanism that adaptively ensembles affinity matrices induced by different prompts. Extensive experiments on \\textbf{16} benchmarks -- including classical, large-scale, fine-grained and domain-shifted datasets -- manifest that our method consistently outperforms the state-of-the-art by a large margin."
  },
  "2602.09541": {
    "title_zh": "手术刀：通过混合高斯桥精细对齐注意力激活流形以缓解多模态幻觉",
    "abstract_zh": "大型视觉语言模型（LVLMs）的快速发展已在视觉语言任务中实现了前所未有的性能。然而，由于大型语言模型（LLMs）的强大先验以及跨模态注意力未对齐，LVLMs常生成与视觉内容不一致的输出——即幻觉现象。为解决此问题，我们提出\\textbf{Scalpel}方法，通过将注意力激活分布细化至更可信区域来减少幻觉。Scalpel在推理过程中预测Transformer各层中每个注意力头的可信方向，并相应调整激活值。该方法采用高斯混合模型捕捉信任与幻觉流形中注意力的多峰分布，并利用熵最优传输（等价于薛定谔桥问题）精确映射高斯分量。在缓解过程中，Scalpel根据分量隶属度及幻觉与信任激活间的映射关系，动态调整干预强度与方向。在多个数据集和基准上的广泛实验表明，Scalpel能有效缓解幻觉，性能超越现有方法，达到最先进水平。此外，Scalpel具有模型与数据无关性，无需额外计算，仅需单步解码即可完成。",
    "abstract_en": "Rapid progress in large vision-language models (LVLMs) has achieved unprecedented performance in vision-language tasks. However, due to the strong prior of large language models (LLMs) and misaligned attention across modalities, LVLMs often generate outputs inconsistent with visual content - termed hallucination. To address this, we propose \\textbf{Scalpel}, a method that reduces hallucination by refining attention activation distributions toward more credible regions. Scalpel predicts trusted attention directions for each head in Transformer layers during inference and adjusts activations accordingly. It employs a Gaussian mixture model to capture multi-peak distributions of attention in trust and hallucination manifolds, and uses entropic optimal transport (equivalent to Schrödinger bridge problem) to map Gaussian components precisely. During mitigation, Scalpel dynamically adjusts intervention strength and direction based on component membership and mapping relationships between hallucination and trust activations. Extensive experiments across multiple datasets and benchmarks demonstrate that Scalpel effectively mitigates hallucinations, outperforming previous methods and achieving state-of-the-art performance. Moreover, Scalpel is model- and data-agnostic, requiring no additional computation, only a single decoding step."
  },
  "2602.09531": {
    "title_zh": "DR.Experts：面向盲图像质量评估的失真感知专家差分细化方法",
    "abstract_zh": "盲图像质量评估旨在无需参考图像的情况下模拟人类对视觉质量的感知，在视觉任务中扮演关键角色。然而，现有模型往往难以有效捕捉细微的失真线索，导致其与人类主观判断存在偏差。我们发现，这一局限的根本原因在于缺乏可靠的失真先验知识，现有方法通常仅学习统一图像特征与质量分数之间的浅层关联，使其对失真不敏感，从而限制了性能。为解决此问题，我们提出了DR.Experts——一种新颖的先验驱动盲图像质量评估框架，旨在显式地融入失真先验，实现可靠的质量评估。DR.Experts首先利用退化感知的视觉语言模型获取失真特异性先验，随后通过提出的失真显著性差分模块，通过区分失真与语义注意力，进一步细化和增强这些先验，从而确保失真的真实表征。接着，通过提出的动态失真加权模块（一种专家混合风格模块），将细化后的先验与语义及桥接表征进行融合。该机制根据每种失真特异性特征的感知影响进行加权，确保最终的质量预测与人类感知一致。在五个具有挑战性的盲图像质量评估基准上进行的广泛实验表明，DR.Experts优于当前方法，并在泛化能力和数据效率方面展现出卓越性能。",
    "abstract_en": "Blind Image Quality Assessment, aiming to replicate human perception of visual quality without reference, plays a key role in vision tasks, yet existing models often fail to effectively capture subtle distortion cues, leading to a misalignment with human subjective judgments. We identify that the root cause of this limitation lies in the lack of reliable distortion priors, as methods typically learn shallow relationships between unified image features and quality scores, resulting in their insensitive nature to distortions and thus limiting their performance. To address this, we introduce DR.Experts, a novel prior-driven BIQA framework designed to explicitly incorporate distortion priors, enabling a reliable quality assessment. DR.Experts begins by leveraging a degradation-aware vision-language model to obtain distortion-specific priors, which are further refined and enhanced by the proposed Distortion-Saliency Differential Module through distinguishing them from semantic attentions, thereby ensuring the genuine representations of distortions. The refined priors, along with semantics and bridging representation, are then fused by a proposed mixture-of-experts style module named the Dynamic Distortion Weighting Module. This mechanism weights each distortion-specific feature as per its perceptual impact, ensuring that the final quality prediction aligns with human perception. Extensive experiments conducted on five challenging BIQA benchmarks demonstrate the superiority of DR.Experts over current methods and showcase its excellence in terms of generalization and data efficiency."
  },
  "2602.12281": {
    "title_zh": "扩展验证在视觉-语言-动作对齐中比扩展策略学习更有效",
    "abstract_zh": "通用机器人的长期愿景依赖于其理解和执行自然语言指令的能力。视觉-语言-动作（VLA）模型在这一目标上取得了显著进展，但其生成的动作仍可能与给定指令存在偏差。本文研究了测试时验证作为缩小“意图-动作差距”的手段。我们首先描述了具身指令跟随的测试时扩展规律，并证明联合扩展重述指令的数量和生成动作的多样性，能显著提升测试时样本的多样性，通常比独立扩展每个维度更有效地恢复正确动作。为利用这些扩展规律，我们提出了CoVer——一种用于视觉-语言-动作对齐的对比验证器，并展示了其架构能随着计算资源和数据的增加而优雅扩展。接着，我们引入了“启动时计算”和分层验证推理流程用于VLA模型。在部署时，我们的框架通过视觉语言模型（VLM）预计算一组多样化的重述指令，为每条指令重复生成动作候选，然后使用验证器选择最优的高层提示和低层动作片段。与在相同数据上扩展策略预训练相比，我们的验证方法在SIMPLER基准测试中实现了22%的分布内增益和13%的分布外增益，并在真实世界实验中进一步提升了45%。在PolaRiS基准测试中，CoVer在任务进度上取得了14%的增益，成功率提升了9%。",
    "abstract_en": "The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the \"intention-action gap.'' We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce \"boot-time compute\" and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate."
  },
  "2602.12136": {
    "title_zh": "面向共址蓝领工作团队协作的具身人工智能体",
    "abstract_zh": "蓝领工作通常具有高度协作性、具身性，并发生于共享的物理环境中，然而现有关于协作式人工智能的研究大多聚焦于白领工作。本立场文件探讨了人工智能体的具身特性如何支持共址蓝领工作场所中的团队协作与沟通。基于我们新启动的CAI-BLUE研究项目背景，我们提出了来自工业与维护领域的两个前瞻性场景，阐明具身人工智能体如何促进共享情境感知，并助力跨经验层级的包容性沟通。我们围绕工人包容性、主体能动性、蓝领协作实践随时间的演变，以及可接受的人工智能具身形式等议题，梳理了具身人工智能体设计相关的开放性问题。我们认为，具身性不仅是美学选择，更应成为蓝领工作场所中人工智能系统的社会-物质设计策略。",
    "abstract_en": "Blue-collar work is often highly collaborative, embodied, and situated in shared physical environments, yet most research on collaborative AI has focused on white-collar work. This position paper explores how the embodied nature of AI agents can support team collaboration and communication in co-located blue-collar workplaces. From the context of our newly started CAI-BLUE research project, we present two speculative scenarios from industrial and maintenance contexts that illustrate how embodied AI agents can support shared situational awareness and facilitate inclusive communication across experience levels. We outline open questions related to embodied AI agent design around worker inclusion, agency, transformation of blue-collar collaboration practices over time, and forms of acceptable AI embodiments. We argue that embodiment is not just an aesthetic choice but should become a socio-material design strategy of AI systems in blue-collar workplaces."
  },
  "2602.12099": {
    "title_zh": "GigaBrain-0.5M*：一种基于世界模型强化学习训练的视觉-语言-动作模型",
    "abstract_zh": "直接从当前观测预测多步动作块的视觉-语言-动作（VLA）模型，因场景理解受限和未来预测能力薄弱而面临固有局限。相比之下，基于网络规模视频语料预训练的视频世界模型展现出强大的时空推理和精准的未来预测能力，使其成为增强VLA学习的天然基础。为此，我们提出\\textit{GigaBrain-0.5M*}，一种通过基于世界模型的强化学习训练的VLA模型。该模型建立在\\textit{GigaBrain-0.5}之上——后者已在超过10,000小时的机器人操作数据上预训练，其中间版本目前在国际RoboChallenge基准测试中位列第一。\\textit{GigaBrain-0.5M*}进一步通过\\textit{RAMP}（基于世界模型条件策略的强化学习）集成基于世界模型的强化学习，以实现鲁棒的跨任务适应。实证结果表明，\\textit{RAMP}相比RECAP基线取得了显著的性能提升，在包括\\texttt{衣物折叠}、\\texttt{装箱打包}和\\texttt{意式咖啡制备}等挑战性任务上实现了约30%的改进。关键的是，\\textit{GigaBrain-0.5M*}展现出可靠的长时程执行能力，能够持续完成复杂操作任务且无失败，这已通过我们\\href{https://gigabrain05m.github.io}{项目页面}上的真实部署视频得到验证。",
    "abstract_en": "Vision-language-action (VLA) models that directly predict multi-step action chunks from current observations face inherent limitations due to constrained scene understanding and weak future anticipation capabilities. In contrast, video world models pre-trained on web-scale video corpora exhibit robust spatiotemporal reasoning and accurate future prediction, making them a natural foundation for enhancing VLA learning. Therefore, we propose \\textit{GigaBrain-0.5M*}, a VLA model trained via world model-based reinforcement learning. Built upon \\textit{GigaBrain-0.5}, which is pre-trained on over 10,000 hours of robotic manipulation data, whose intermediate version currently ranks first on the international RoboChallenge benchmark. \\textit{GigaBrain-0.5M*} further integrates world model-based reinforcement learning via \\textit{RAMP} (Reinforcement leArning via world Model-conditioned Policy) to enable robust cross-task adaptation. Empirical results demonstrate that \\textit{RAMP} achieves substantial performance gains over the RECAP baseline, yielding improvements of approximately 30\\% on challenging tasks including \\texttt{Laundry Folding}, \\texttt{Box Packing}, and \\texttt{Espresso Preparation}. Critically, \\textit{GigaBrain-0.5M$^*$} exhibits reliable long-horizon execution, consistently accomplishing complex manipulation tasks without failure as validated by real-world deployment videos on our \\href{https://gigabrain05m.github.io}{project page}."
  },
  "2602.12063": {
    "title_zh": "VLAW：视觉-语言-动作策略与世界模型的迭代协同改进",
    "abstract_zh": "本文的目标是通过迭代式在线交互提升视觉-语言-动作（VLA）模型的性能与可靠性。由于在现实世界中收集策略部署数据成本高昂，我们探究是否可利用学习型模拟器——特别是动作条件视频生成模型——来生成额外的部署数据。然而，现有世界模型缺乏策略改进所需的物理保真度：它们主要基于演示数据集训练，这些数据集缺乏对多种物理交互（尤其是失败案例）的覆盖，且难以准确建模接触密集型物体操作中微小但关键的物理细节。我们提出一种简单的迭代改进算法，利用真实世界部署数据提升世界模型的保真度，进而生成补充性合成数据以改进VLA模型。在真实机器人实验中，我们运用该方法提升了先进VLA模型在多个下游任务中的表现。相比基线策略，我们实现了39.2%的绝对成功率提升，其中通过生成合成部署数据训练带来11.6%的改进。演示视频可访问此匿名网站：https://sites.google.com/view/vla-w",
    "abstract_en": "The goal of this paper is to improve the performance and reliability of vision-language-action (VLA) models through iterative online interaction. Since collecting policy rollouts in the real world is expensive, we investigate whether a learned simulator-specifically, an action-conditioned video generation model-can be used to generate additional rollout data. Unfortunately, existing world models lack the physical fidelity necessary for policy improvement: they are predominantly trained on demonstration datasets that lack coverage of many different physical interactions (particularly failure cases) and struggle to accurately model small yet critical physical details in contact-rich object manipulation. We propose a simple iterative improvement algorithm that uses real-world roll-out data to improve the fidelity of the world model, which can then, in turn, be used to generate supplemental synthetic data for improving the VLA model. In our experiments on a real robot, we use this approach to improve the performance of a state-of-the-art VLA model on multiple downstream tasks. We achieve a 39.2% absolute success rate improvement over the base policy and 11.6% improvement from training with the generated synthetic rollouts. Videos can be found at this anonymous website: https://sites.google.com/view/vla-w"
  },
  "2602.12062": {
    "title_zh": "HoloBrain-0技术报告",
    "abstract_zh": "本工作介绍了HoloBrain-0，这是一个全面的视觉-语言-动作（VLA）框架，旨在弥合基础模型研究与可靠现实世界机器人部署之间的鸿沟。我们系统的核心是一种新颖的VLA架构，该架构明确整合了机器人本体先验知识，包括多视角相机参数和运动学描述（URDF），以增强三维空间推理能力并支持多样化的机器人本体。我们通过可扩展的“预训练后微调”范式验证了这一设计，在RoboTwin 2.0、LIBERO和GenieSim等仿真基准测试中取得了最先进的结果，同时在具有挑战性的长时程现实世界操作任务中也表现出色。值得注意的是，我们高效的0.2B参数变体可与规模显著更大的基线模型相媲美，实现了低延迟的端侧部署。为进一步加速研究和实际应用，我们完全开源了完整的HoloBrain生态系统，包括：（1）强大的预训练VLA基础模型；（2）针对多个仿真套件和现实世界任务的微调检查点；以及（3）RoboOrchard，一个用于数据管理、模型训练和部署的全栈VLA基础设施。结合标准化的数据收集协议，此次发布为研究社区提供了一条完整、可复现的实现高性能机器人操作的路径。",
    "abstract_en": "In this work, we introduce HoloBrain-0, a comprehensive Vision-Language-Action (VLA) framework that bridges the gap between foundation model research and reliable real-world robot deployment. The core of our system is a novel VLA architecture that explicitly incorporates robot embodiment priors, including multi-view camera parameters and kinematic descriptions (URDF), to enhance 3D spatial reasoning and support diverse embodiments. We validate this design through a scalable ``pre-train then post-train\" paradigm, achieving state-of-the-art results on simulation benchmarks such as RoboTwin 2.0, LIBERO, and GenieSim, as well as strong results on challenging long-horizon real-world manipulation tasks. Notably, our efficient 0.2B-parameter variant rivals significantly larger baselines, enabling low-latency on-device deployment. To further accelerate research and practical adoption, we fully open-source the entire HoloBrain ecosystem, which includes: (1) powerful pre-trained VLA foundations; (2) post-trained checkpoints for multiple simulation suites and real-world tasks; and (3) RoboOrchard, a full-stack VLA infrastructure for data curation, model training and deployment. Together with standardized data collection protocols, this release provides the community with a complete, reproducible path toward high-performance robotic manipulation."
  },
  "2602.12032": {
    "title_zh": "视觉-本体感知策略在机器人操作中何时会失效？",
    "abstract_zh": "本体感知信息通过提供实时机器人状态，对于精确的伺服控制至关重要。其与视觉的结合被高度期待以提升复杂任务中操作策略的性能。然而，近期研究在视觉-本体感知策略的泛化能力上报告了不一致的观察结果。在本工作中，我们通过时间控制的实验对此进行了探究。我们发现，在机器人运动转换的任务子阶段（这些阶段需要目标定位），视觉-本体感知策略的视觉模态作用有限。进一步分析揭示，在训练过程中，策略自然倾向于使用能更快降低损失的简洁本体感知信号，从而主导优化过程，并在运动转换阶段抑制视觉模态的学习。为缓解此问题，我们提出了基于阶段引导的梯度调整（GAP）算法，该算法自适应地调节本体感知的优化，实现视觉-本体感知策略内部的动态协作。具体而言，我们利用本体感知捕获机器人状态，并估计轨迹中每个时间步属于运动转换阶段的概率。在策略学习过程中，我们应用细粒度调整，根据估计的概率降低本体感知梯度的大小，从而获得鲁棒且可泛化的视觉-本体感知策略。全面的实验表明，GAP算法在模拟和真实环境中均适用，覆盖单臂和双臂设置，并与传统模型及视觉-语言-动作模型兼容。我们相信这项工作能为机器人操作中视觉-本体感知策略的发展提供有价值的见解。",
    "abstract_en": "Proprioceptive information is critical for precise servo control by providing real-time robotic states. Its collaboration with vision is highly expected to enhance performances of the manipulation policy in complex tasks. However, recent studies have reported inconsistent observations on the generalization of vision-proprioception policies. In this work, we investigate this by conducting temporally controlled experiments. We found that during task sub-phases that robot's motion transitions, which require target localization, the vision modality of the vision-proprioception policy plays a limited role. Further analysis reveals that the policy naturally gravitates toward concise proprioceptive signals that offer faster loss reduction when training, thereby dominating the optimization and suppressing the learning of the visual modality during motion-transition phases. To alleviate this, we propose the Gradient Adjustment with Phase-guidance (GAP) algorithm that adaptively modulates the optimization of proprioception, enabling dynamic collaboration within the vision-proprioception policy. Specifically, we leverage proprioception to capture robotic states and estimate the probability of each timestep in the trajectory belonging to motion-transition phases. During policy learning, we apply fine-grained adjustment that reduces the magnitude of proprioception's gradient based on estimated probabilities, leading to robust and generalizable vision-proprioception policies. The comprehensive experiments demonstrate GAP is applicable in both simulated and real-world environments, across one-arm and dual-arm setups, and compatible with both conventional and Vision-Language-Action models. We believe this work can offer valuable insights into the development of vision-proprioception policies in robotic manipulation."
  },
  "2602.11934": {
    "title_zh": "Robot-DIFT：提取扩散特征以实现几何一致的视觉运动控制",
    "abstract_zh": "我们假设，通用机器人操作的关键瓶颈不仅在于数据规模或策略容量，更在于当前视觉主干网络与闭环控制的物理需求之间的结构不匹配。虽然最先进的视觉编码器（包括用于视觉语言模型中的编码器）通过语义不变性优化以稳定分类，但操作通常需要几何敏感性——即能够将毫米级的姿态变化映射为可预测的特征变化。其判别性目标为细粒度控制创造了“盲点”，而生成式扩散模型则在其潜在流形中固有地编码了几何依赖性，促进了密集多尺度空间结构的保留。然而，直接将随机扩散特征用于控制受到随机不稳定性、推理延迟和微调期间表示漂移的阻碍。为弥合这一差距，我们提出了Robot-DIFT框架，该框架通过流形蒸馏将几何信息的来源与推理过程解耦。通过将冻结的扩散教师模型蒸馏为确定性的空间语义特征金字塔网络（S2-FPN），我们保留了生成模型的丰富几何先验，同时确保了时间稳定性、实时执行能力和对漂移的鲁棒性。在大规模DROID数据集上进行预训练后，Robot-DIFT相比领先的判别性基线展现出更优的几何一致性和控制性能，支持了“模型如何学习观察决定了其如何学习行动”的观点。",
    "abstract_en": "We hypothesize that a key bottleneck in generalizable robot manipulation is not solely data scale or policy capacity, but a structural mismatch between current visual backbones and the physical requirements of closed-loop control. While state-of-the-art vision encoders (including those used in VLAs) optimize for semantic invariance to stabilize classification, manipulation typically demands geometric sensitivity the ability to map millimeter-level pose shifts to predictable feature changes. Their discriminative objective creates a \"blind spot\" for fine-grained control, whereas generative diffusion models inherently encode geometric dependencies within their latent manifolds, encouraging the preservation of dense multi-scale spatial structure. However, directly deploying stochastic diffusion features for control is hindered by stochastic instability, inference latency, and representation drift during fine-tuning. To bridge this gap, we propose Robot-DIFT, a framework that decouples the source of geometric information from the process of inference via Manifold Distillation. By distilling a frozen diffusion teacher into a deterministic Spatial-Semantic Feature Pyramid Network (S2-FPN), we retain the rich geometric priors of the generative model while ensuring temporal stability, real-time execution, and robustness against drift. Pretrained on the large-scale DROID dataset, Robot-DIFT demonstrates superior geometric consistency and control performance compared to leading discriminative baselines, supporting the view that how a model learns to see dictates how well it can learn to act."
  },
  "2602.11832": {
    "title_zh": "JEPA-VLA：视觉语言动作模型需要视频预测性嵌入",
    "abstract_zh": "近年来，基于预训练视觉语言模型构建的视觉语言动作模型在机器人操作领域取得了显著进展。然而，当前的视觉语言动作模型仍面临样本效率低下和泛化能力有限的问题。本文认为，这些局限性与一个被忽视的组件——预训练视觉表征密切相关，该组件在环境理解和策略先验两方面均提供不足的知识。通过深入分析，我们发现视觉语言动作模型中常用的视觉表征，无论是通过语言-图像对比学习还是基于图像的自监督学习预训练，在捕捉关键任务相关的环境信息以及诱导有效的策略先验（即成功执行任务时环境如何演变的预见性知识）方面仍存在不足。相比之下，我们发现基于视频预训练的预测性嵌入，特别是V-JEPA 2，能够灵活剔除不可预测的环境因素，并编码任务相关的时间动态，从而有效弥补现有视觉语言动作模型中视觉表征的关键缺陷。基于这些观察，我们提出了JEPA-VLA，一种简单而有效的方法，能够自适应地将预测性嵌入集成到现有的视觉语言动作模型中。实验表明，JEPA-VLA在包括LIBERO、LIBERO-plus、RoboTwin2.0以及真实机器人任务在内的一系列基准测试中均带来了显著的性能提升。",
    "abstract_en": "Recent vision-language-action (VLA) models built upon pretrained vision-language models (VLMs) have achieved significant improvements in robotic manipulation. However, current VLAs still suffer from low sample efficiency and limited generalization. This paper argues that these limitations are closely tied to an overlooked component, pretrained visual representation, which offers insufficient knowledge on both aspects of environment understanding and policy prior. Through an in-depth analysis, we find that commonly used visual representations in VLAs, whether pretrained via language-image contrastive learning or image-based self-supervised learning, remain inadequate at capturing crucial, task-relevant environment information and at inducing effective policy priors, i.e., anticipatory knowledge of how the environment evolves under successful task execution. In contrast, we discover that predictive embeddings pretrained on videos, in particular V-JEPA 2, are adept at flexibly discarding unpredictable environment factors and encoding task-relevant temporal dynamics, thereby effectively compensating for key shortcomings of existing visual representations in VLAs. Building on these observations, we introduce JEPA-VLA, a simple yet effective approach that adaptively integrates predictive embeddings into existing VLAs. Our experiments demonstrate that JEPA-VLA yields substantial performance gains across a range of benchmarks, including LIBERO, LIBERO-plus, RoboTwin2.0, and real-robot tasks."
  },
  "2602.11660": {
    "title_zh": "Clutt3R-Seg：面向杂乱场景中语言驱动抓取的稀疏视角三维实例分割",
    "abstract_zh": "可靠的三维实例分割是语言驱动机器人操作的基础。其关键应用在于杂乱环境中，其中遮挡、有限视角和噪声掩码会降低感知性能。为应对这些挑战，我们提出了Clutt3R-Seg，一种用于杂乱场景中语言驱动抓取的零样本鲁棒三维实例分割流程。我们的核心思想是引入一个基于语义线索的层次化实例树。与先前试图优化噪声掩码的方法不同，我们的方法将其作为信息线索加以利用：通过跨视角分组和条件替换，该树结构抑制了过分割和欠分割，从而生成视角一致的掩码和鲁棒的三维实例。每个实例均通过开放词汇语义嵌入进行增强，使其能够根据自然语言指令准确选择目标。为处理多阶段任务中的场景变化，我们进一步引入了基于一致性的更新机制，仅需单次交互后图像即可保持实例对应关系，从而实现无需重新扫描的高效适应。Clutt3R-Seg在合成和真实数据集上进行了评估，并在真实机器人上得到验证。在所有设置中，其在杂乱和稀疏视角场景下均持续优于现有先进基线方法。即使在最具挑战性的重度杂乱序列中，Clutt3R-Seg的AP@25达到61.66，超过基线方法2.2倍以上；仅使用四个输入视角时，其性能超过使用八个视角的MaskClustering方法2倍以上。代码已开源：https://github.com/jeonghonoh/clutt3r-seg。",
    "abstract_en": "Reliable 3D instance segmentation is fundamental to language-grounded robotic manipulation. Its critical application lies in cluttered environments, where occlusions, limited viewpoints, and noisy masks degrade perception. To address these challenges, we present Clutt3R-Seg, a zero-shot pipeline for robust 3D instance segmentation for language-grounded grasping in cluttered scenes. Our key idea is to introduce a hierarchical instance tree of semantic cues. Unlike prior approaches that attempt to refine noisy masks, our method leverages them as informative cues: through cross-view grouping and conditional substitution, the tree suppresses over- and under-segmentation, yielding view-consistent masks and robust 3D instances. Each instance is enriched with open-vocabulary semantic embeddings, enabling accurate target selection from natural language instructions. To handle scene changes during multi-stage tasks, we further introduce a consistency-aware update that preserves instance correspondences from only a single post-interaction image, allowing efficient adaptation without rescanning. Clutt3R-Seg is evaluated on both synthetic and real-world datasets, and validated on a real robot. Across all settings, it consistently outperforms state-of-the-art baselines in cluttered and sparse-view scenarios. Even on the most challenging heavy-clutter sequences, Clutt3R-Seg achieves an AP@25 of 61.66, over 2.2x higher than baselines, and with only four input views it surpasses MaskClustering with eight views by more than 2x. The code is available at: https://github.com/jeonghonoh/clutt3r-seg."
  },
  "2602.11643": {
    "title_zh": "ViTaS：面向视觉运动学习的视觉触觉软融合对比学习",
    "abstract_zh": "触觉信息在人类操作任务中起着至关重要的作用，近年来在机器人操作领域也日益受到关注。然而，现有方法大多侧重于视觉与触觉特征的对齐，且融合机制往往采用直接拼接的方式。因此，由于忽视了两种模态固有的互补性，这些方法难以有效应对遮挡场景，且对齐效果可能未被充分利用，限制了其在现实世界部署的潜力。本文提出ViTaS，一个简单而有效的框架，整合视觉与触觉信息以指导智能体行为。我们引入了软融合对比学习——一种传统对比学习方法的进阶版本，并结合CVAE模块，以充分利用视觉触觉表征间的对齐性与互补性。我们在12个模拟环境和3个真实世界环境中验证了方法的有效性，实验表明ViTaS显著优于现有基线。项目页面：https://skyrainwind.github.io/ViTaS/index.html。",
    "abstract_en": "Tactile information plays a crucial role in human manipulation tasks and has recently garnered increasing attention in robotic manipulation. However, existing approaches mostly focus on the alignment of visual and tactile features and the integration mechanism tends to be direct concatenation. Consequently, they struggle to effectively cope with occluded scenarios due to neglecting the inherent complementary nature of both modalities and the alignment may not be exploited enough, limiting the potential of their real-world deployment. In this paper, we present ViTaS, a simple yet effective framework that incorporates both visual and tactile information to guide the behavior of an agent. We introduce Soft Fusion Contrastive Learning, an advanced version of conventional contrastive learning method and a CVAE module to utilize the alignment and complementarity within visuo-tactile representations. We demonstrate the effectiveness of our method in 12 simulated and 3 real-world environments, and our experiments show that ViTaS significantly outperforms existing baselines. Project page: https://skyrainwind.github.io/ViTaS/index.html."
  },
  "2602.11598": {
    "title_zh": "ABot-N0：面向通用具身导航的视觉-语言-动作基础模型技术报告",
    "abstract_zh": "具身导航领域长期受限于任务专用架构的碎片化问题。本文提出ABot-N0——一个统一的视觉-语言-动作基础模型，实现了点目标导航、物体目标导航、指令跟随、兴趣点目标导航及人员跟随这五大核心任务的“大一统”。该模型采用分层式“大脑-动作”架构，将基于大语言模型的认知大脑（负责语义推理）与基于流匹配的动作专家（生成精确连续轨迹）相结合。为支撑大规模学习，我们开发了ABot-N0数据引擎，在7,802个高保真3D场景（总面积10.7平方公里）中构建了1,690万条专家轨迹与500万条推理样本。ABot-N0在7项基准测试中均刷新了最高性能记录，显著超越各类专用模型。此外，我们研发的智能导航系统通过规划器与分层拓扑记忆的融合，实现了动态真实环境中鲁棒的长时程任务执行。",
    "abstract_en": "Embodied navigation has long been fragmented by task-specific architectures. We introduce ABot-N0, a unified Vision-Language-Action (VLA) foundation model that achieves a ``Grand Unification'' across 5 core tasks: Point-Goal, Object-Goal, Instruction-Following, POI-Goal, and Person-Following. ABot-N0 utilizes a hierarchical ``Brain-Action'' architecture, pairing an LLM-based Cognitive Brain for semantic reasoning with a Flow Matching-based Action Expert for precise, continuous trajectory generation.   To support large-scale learning, we developed the ABot-N0 Data Engine, curating 16.9M expert trajectories and 5.0M reasoning samples across 7,802 high-fidelity 3D scenes (10.7 $\\text{km}^2$). ABot-N0 achieves new SOTA performance across 7 benchmarks, significantly outperforming specialized models. Furthermore, our Agentic Navigation System integrates a planner with hierarchical topological memory, enabling robust, long-horizon missions in dynamic real-world environments."
  },
  "2602.09972": {
    "title_zh": "Hydra-Nav：基于自适应双过程推理的目标导航",
    "abstract_zh": "尽管大型视觉语言模型在目标导航任务中展现出潜力，但现有方法仍面临成功率低、对未见物体定位效率不足的问题，这些失败主要归因于时空推理能力薄弱。近期尝试将推理机制注入基于视觉语言模型的智能体虽提升了成功率，却带来了显著的计算开销。为解决现有方法在效果与效率上的双重不足，本文提出Hydra-Nav——一种统一的自适应视觉语言模型架构，能够在用于分析探索历史并制定高层规划的审慎慢速系统与用于高效执行的反应式快速系统之间动态切换。我们通过三阶段课程学习训练Hydra-Nav：（1）空间-动作对齐以强化轨迹规划能力；（2）记忆-推理融合以提升长时探索中的时空推理能力；（3）迭代拒绝微调以实现关键决策点的选择性推理。大量实验表明，Hydra-Nav在HM3D、MP3D和OVON基准测试中均取得最先进的性能，分别超越次优方法11.1%、17.4%和21.2%。此外，我们提出SOT（操作时间加权成功率）这一新指标，用于衡量不同推理强度的视觉语言模型的搜索效率。实验结果显示，自适应推理机制相比固定频率基线方法显著提升了搜索效率。",
    "abstract_en": "While large vision-language models (VLMs) show promise for object goal navigation, current methods still struggle with low success rates and inefficient localization of unseen objects--failures primarily attributed to weak temporal-spatial reasoning. Meanwhile, recent attempts to inject reasoning into VLM-based agents improve success rates but incur substantial computational overhead. To address both the ineffectiveness and inefficiency of existing approaches, we introduce Hydra-Nav, a unified VLM architecture that adaptively switches between a deliberative slow system for analyzing exploration history and formulating high-level plans, and a reactive fast system for efficient execution. We train Hydra-Nav through a three-stage curriculum: (i) spatial-action alignment to strengthen trajectory planning, (ii) memory-reasoning integration to enhance temporal-spatial reasoning over long-horizon exploration, and (iii) iterative rejection fine-tuning to enable selective reasoning at critical decision points. Extensive experiments demonstrate that Hydra-Nav achieves state-of-the-art performance on the HM3D, MP3D, and OVON benchmarks, outperforming the second-best methods by 11.1%, 17.4%, and 21.2%, respectively. Furthermore, we introduce SOT (Success weighted by Operation Time), a new metric to measure search efficiency across VLMs with varying reasoning intensity. Results show that adaptive reasoning significantly enhances search efficiency over fixed-frequency baselines."
  },
  "2602.12203": {
    "title_zh": "ExStrucTiny：面向文档图像中模式可变结构化信息提取的基准数据集",
    "abstract_zh": "企业文档（如表格和报告）中蕴含的关键信息对于数据归档、自动化工作流和分析等下游应用至关重要。尽管通用视觉语言模型在现有文档理解基准测试中表现良好，但其在不同文档类型和灵活模式间进行整体、细粒度结构化提取的能力尚未得到充分研究。现有的关键实体提取、关系提取和视觉问答数据集受限于狭窄的实体本体、简单查询或同质文档类型，往往忽视了适应性结构化提取的需求。为填补这些空白，我们提出了ExStrucTiny——一个面向文档图像结构化信息提取的新基准数据集，它统一了关键实体提取、关系提取和视觉问答的多个方面。通过结合人工与合成人工验证样本的新颖流程构建，ExStrucTiny涵盖了更丰富的文档类型和提取场景。我们在此基准上分析了开放和封闭的视觉语言模型，揭示了模式适配、查询欠规范和答案定位等挑战。我们希望这项工作能为提升通用模型在文档结构化信息提取方面的能力奠定基础。",
    "abstract_en": "Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents."
  },
  "2602.12196": {
    "title_zh": "视觉推理基准：评估多模态大语言模型在基础教育课堂真实视觉问题上的表现",
    "abstract_zh": "AI模型在文本推理方面已达到最先进水平，但其在空间和关系结构上的推理能力仍是一个关键瓶颈——尤其是在低年级数学中，这类问题高度依赖视觉信息。本文介绍了视觉推理基准（VRB），这是一个新颖的数据集，旨在评估多模态大语言模型（MLLMs）解决课堂真实视觉问题的能力。该基准基于从赞比亚和印度小学考试中收集的701道题目构建，涵盖类比推理、模式补全和空间匹配等多种任务。我们概述了基准的方法论和开发过程，其有意使用未经编辑、文本极少的图像，以测试模型是否能满足基础教育的实际需求。我们的研究揭示了一个“能力参差不齐的前沿”：模型在静态技能（如计数和缩放）上表现出较好的熟练度，但在面对动态操作（如折叠、反射和旋转）时，却触及了一个明显的“空间天花板”。这些弱点对课堂中视觉推理问题的应用构成了风险，可能导致错误评分、虚假辅助以及强化学生的误解。因此，像VRB这样专注于教育的基准对于确定课堂中使用的多模态工具的功能边界至关重要。",
    "abstract_en": "AI models have achieved state-of-the-art results in textual reasoning; however, their ability to reason over spatial and relational structures remains a critical bottleneck -- particularly in early-grade maths, which relies heavily on visuals. This paper introduces the visual reasoning benchmark (VRB), a novel dataset designed to evaluate Multimodal Large Language Models (MLLMs) on their ability to solve authentic visual problems from classrooms. This benchmark is built on a set of 701 questions sourced from primary school examinations in Zambia and India, which cover a range of tasks such as reasoning by analogy, pattern completion, and spatial matching. We outline the methodology and development of the benchmark which intentionally uses unedited, minimal-text images to test if models can meet realistic needs of primary education. Our findings reveal a ``jagged frontier'' of capability where models demonstrate better proficiency in static skills such as counting and scaling, but reach a distinct ``spatial ceiling'' when faced with dynamic operations like folding, reflection, and rotation. These weaknesses pose a risk for classroom use on visual reasoning problems, with the potential for incorrect marking, false scaffolding, and reinforcing student misconceptions. Consequently, education-focused benchmarks like the VRB are essential for determining the functional boundaries of multimodal tools used in classrooms."
  },
  "2602.12159": {
    "title_zh": "3DGSNav：通过主动3D高斯泼溅增强视觉语言模型在物体导航中的推理能力",
    "abstract_zh": "物体导航是具身智能的核心能力，使智能体能够在未知环境中定位目标物体。视觉语言模型（VLMs）的最新进展推动了零样本物体导航（ZSON）的发展。然而，现有方法通常依赖于将环境转换为语义地图或文本表示的场景抽象，导致高层决策受限于低层感知的准确性。本文提出3DGSNav，一种新颖的ZSON框架，通过嵌入3D高斯泼溅（3DGS）作为VLMs的持久记忆来增强空间推理。通过主动感知，3DGSNav逐步构建环境的3DGS表示，实现基于轨迹引导的、前沿感知的第一人称自由视点渲染。此外，我们设计了结构化视觉提示，并将其与思维链（CoT）提示相结合，以进一步提升VLM的推理能力。在导航过程中，实时物体检测器过滤潜在目标，而VLM驱动的主动视点切换执行目标重新验证，确保高效可靠的识别。在多个基准测试中的广泛评估以及在四足机器人上的真实世界实验表明，我们的方法相较于最先进方法实现了稳健且具有竞争力的性能。项目页面：https://aczheng-cai.github.io/3dgsnav.github.io/",
    "abstract_en": "Object navigation is a core capability of embodied intelligence, enabling an agent to locate target objects in unknown environments. Recent advances in vision-language models (VLMs) have facilitated zero-shot object navigation (ZSON). However, existing methods often rely on scene abstractions that convert environments into semantic maps or textual representations, causing high-level decision making to be constrained by the accuracy of low-level perception. In this work, we present 3DGSNav, a novel ZSON framework that embeds 3D Gaussian Splatting (3DGS) as persistent memory for VLMs to enhance spatial reasoning. Through active perception, 3DGSNav incrementally constructs a 3DGS representation of the environment, enabling trajectory-guided free-viewpoint rendering of frontier-aware first-person views. Moreover, we design structured visual prompts and integrate them with Chain-of-Thought (CoT) prompting to further improve VLM reasoning. During navigation, a real-time object detector filters potential targets, while VLM-driven active viewpoint switching performs target re-verification, ensuring efficient and reliable recognition. Extensive evaluations across multiple benchmarks and real-world experiments on a quadruped robot demonstrate that our method achieves robust and competitive performance against state-of-the-art approaches.The Project Page:https://aczheng-cai.github.io/3dgsnav.github.io/"
  },
  "2602.12092": {
    "title_zh": "DeepSight：一体化大型模型安全工具箱",
    "abstract_zh": "随着大型模型的快速发展，其安全性已成为优先关注点。当前大型语言模型和多模态大型语言模型的安全工作流程中，评估、诊断和对齐通常由独立工具处理。具体而言，安全评估仅能定位外部行为风险，无法查明内部根本原因；而安全诊断常脱离具体风险场景，停留在可解释性层面。这导致安全对齐缺乏对内部机制变化的专门解释，可能损害模型的通用能力。为系统解决这些问题，我们提出了一个开源项目DeepSight，实践一种新的安全评估-诊断一体化范式。DeepSight是一个低成本、可复现、高效且高度可扩展的大规模模型安全评估项目，包含评估工具包DeepSafe和诊断工具包DeepScan。通过统一任务与数据协议，我们在两个阶段间建立连接，将安全评估从黑盒洞察转变为白盒洞察。此外，DeepSight是首个支持前沿人工智能风险评估及联合安全评估与诊断的开源工具箱。",
    "abstract_en": "As the development of Large Models (LMs) progresses rapidly, their safety is also a priority. In current Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) safety workflow, evaluation, diagnosis, and alignment are often handled by separate tools. Specifically, safety evaluation can only locate external behavioral risks but cannot figure out internal root causes. Meanwhile, safety diagnosis often drifts from concrete risk scenarios and remains at the explainable level. In this way, safety alignment lack dedicated explanations of changes in internal mechanisms, potentially degrading general capabilities. To systematically address these issues, we propose an open-source project, namely DeepSight, to practice a new safety evaluation-diagnosis integrated paradigm. DeepSight is low-cost, reproducible, efficient, and highly scalable large-scale model safety evaluation project consisting of a evaluation toolkit DeepSafe and a diagnosis toolkit DeepScan. By unifying task and data protocols, we build a connection between the two stages and transform safety evaluation from black-box to white-box insight. Besides, DeepSight is the first open source toolkit that support the frontier AI risk evaluation and joint safety evaluation and diagnosis."
  },
  "2602.12065": {
    "title_zh": "可供性图化任务世界：面向可扩展具身学习的自演化任务生成",
    "abstract_zh": "直接在现实世界中训练机器人策略成本高昂且难以扩展。尽管生成式仿真能够实现大规模数据合成，但现有方法常因开环执行而难以生成逻辑连贯的长时程任务，并受动态物理不确定性的困扰。为解决这些挑战，我们提出了可供性图化任务世界（AGT-World），这是一个基于真实世界观察自主构建交互式仿真环境及相应机器人任务策略的统一框架。与依赖随机提议或静态复制的方法不同，AGT-World将任务空间形式化为结构化图，实现了复杂目标向理论基础的原子原语的精确、层次化分解。此外，我们引入了一种结合混合反馈的自演化机制，融合视觉语言模型推理与几何验证，以自主优化策略。大量实验表明，我们的方法在成功率和泛化能力上显著优于现有技术，实现了提议、执行与修正的自改进循环，从而推动可扩展的机器人学习。",
    "abstract_en": "Training robotic policies directly in the real world is expensive and unscalable. Although generative simulation enables large-scale data synthesis, current approaches often fail to generate logically coherent long-horizon tasks and struggle with dynamic physical uncertainties due to open-loop execution. To address these challenges, we propose Affordance-Graphed Task Worlds (AGT-World), a unified framework that autonomously constructs interactive simulated environments and corresponding robot task policies based on real-world observations. Unlike methods relying on random proposals or static replication, AGT-World formalizes the task space as a structured graph, enabling the precise, hierarchical decomposition of complex goals into theoretically grounded atomic primitives. Furthermore, we introduce a Self-Evolution mechanism with hybrid feedback to autonomously refine policies, combining Vision-Language Model reasoning and geometric verification. Extensive experiments demonstrate that our method significantly outperforms in success rates and generalization, achieving a self-improving cycle of proposal, execution, and correction for scalable robot learning."
  },
  "2602.12002": {
    "title_zh": "本地视觉语言模型能否超越视觉Transformer提升活动识别能力？——以新生儿复苏为例的研究",
    "abstract_zh": "准确记录新生儿复苏过程对于质量改进和遵循临床指南至关重要，但在实践中仍未得到充分利用。先前使用3D-CNN和视觉Transformer（ViT）的研究在从新生儿复苏视频中检测关键活动方面已显示出有希望的结果，但也凸显了识别此类细粒度活动所面临的挑战。本研究探讨了生成式人工智能（GenAI）方法在提升此类视频活动识别能力方面的潜力。具体而言，我们探索了本地视觉语言模型（VLM）与大型语言模型（LLM）的结合使用，并将其与监督式TimeSFormer基线模型进行比较。利用包含13.26小时新生儿复苏视频的模拟数据集，我们评估了多种基于零样本VLM的策略以及带有分类头（包括低秩适应LoRA）的微调VLM。结果表明，小型（本地）VLM存在幻觉问题，但通过LoRA微调后，其F1分数达到0.91，超越了TimeSformer的0.70结果。",
    "abstract_en": "Accurate documentation of newborn resuscitation is essential for quality improvement and adherence to clinical guidelines, yet remains underutilized in practice. Previous work using 3D-CNNs and Vision Transformers (ViT) has shown promising results in detecting key activities from newborn resuscitation videos, but also highlighted the challenges in recognizing such fine-grained activities. This work investigates the potential of generative AI (GenAI) methods to improve activity recognition from such videos. Specifically, we explore the use of local vision-language models (VLMs), combined with large language models (LLMs), and compare them to a supervised TimeSFormer baseline. Using a simulated dataset comprising 13.26 hours of newborn resuscitation videos, we evaluate several zero-shot VLM-based strategies and fine-tuned VLMs with classification heads, including Low-Rank Adaptation (LoRA). Our results suggest that small (local) VLMs struggle with hallucinations, but when fine-tuned with LoRA, the results reach F1 score at 0.91, surpassing the TimeSformer results of 0.70."
  },
  "2602.11980": {
    "title_zh": "空间思维链：连接理解与生成模型以实现空间推理生成",
    "abstract_zh": "尽管扩散模型在美学图像合成方面展现出卓越能力，但在复杂空间理解和推理任务中常面临挑战。现有方法多依赖多模态大语言模型（MLLMs）来增强此能力，但要么通过联合训练导致高昂计算成本，要么因仅依赖文本提示而遭受空间信息损失。为缓解这些局限，我们提出空间思维链（SCoT）框架——一种即插即用方法，有效桥接MLLMs的推理能力与扩散模型的生成能力。具体而言，我们首先通过交错文本-坐标指令格式训练扩散模型以增强其布局感知能力；随后利用先进MLLMs作为规划器生成全面布局方案，将其空间规划能力直接迁移至生成过程。大量实验表明，我们的方法在图像生成基准测试中达到最先进性能，在复杂推理任务上显著超越基线模型，同时在图像编辑场景中也展现出强大效能。",
    "abstract_en": "While diffusion models have shown exceptional capabilities in aesthetic image synthesis, they often struggle with complex spatial understanding and reasoning. Existing approaches resort to Multimodal Large Language Models (MLLMs) to enhance this capability. However, they either incur high computational costs through joint training or suffer from spatial information loss when relying solely on textual prompts. To alleviate these limitations, we propose a Spatial Chain-of-Thought (SCoT) framework, a plug-and-play approach that effectively bridges the reasoning capabilities of MLLMs with the generative power of diffusion models. Specifically, we first enhance the diffusion model's layout awareness by training it on an interleaved text-coordinate instruction format. We then leverage state-of-the-art MLLMs as planners to generate comprehensive layout plans, transferring their spatial planning capabilities directly to the generation process. Extensive experiments demonstrate that our method achieves state-of-the-art performance on image generation benchmarks and significantly outperforms baselines on complex reasoning tasks, while also showing strong efficacy in image editing scenarios."
  },
  "2602.11960": {
    "title_zh": "评估视觉语言模型在法语PDF转Markdown任务中的性能基准",
    "abstract_zh": "本报告评估了近期视觉语言模型在处理具有挑战性的法语文档时的PDF转Markdown转换能力。文档解析是检索增强生成流程中的关键步骤，转录和布局错误会传播至下游的检索与内容锚定环节。现有基准测试通常侧重于英语或中文，并可能过度惩罚那些对下游应用影响甚微的良性格式与线性化选择（如换行符、列表分段、替代表格渲染方式）。我们引入了一个专注于法语的基准测试集，通过模型分歧采样从包含60,000份文档的语料库中筛选出困难页面，涵盖手写表格、复杂布局、密集表格及图形丰富的页面。评估采用单元测试风格的检查方法，针对具体故障模式（文本存在性、阅读顺序及局部表格约束），并结合针对特定类别的标准化处理，以忽略仅影响呈现方式的差异。在15个模型的测试中，我们发现最强专有模型在手写体和表格处理上表现出显著更高的鲁棒性，而多个开源权重系统在标准印刷布局上仍保持竞争力。",
    "abstract_en": "This report evaluates PDF-to-Markdown conversion using recent Vision-Language Models (VLMs) on challenging French documents. Document parsing is a critical step for Retrieval-Augmented Generation (RAG) pipelines, where transcription and layout errors propagate to downstream retrieval and grounding. Existing benchmarks often emphasize English or Chinese and can over-penalize benign formatting and linearization choices (e.g., line breaks, list segmentation, alternative table renderings) that are largely irrelevant for downstream use.   We introduce a French-focused benchmark of difficult pages selected via model-disagreement sampling from a corpus of 60{,}000 documents, covering handwritten forms, complex layouts, dense tables, and graphics-rich pages. Evaluation is performed with unit-test-style checks that target concrete failure modes (text presence, reading order, and local table constraints) combined with category-specific normalization designed to discount presentation-only variance. Across 15 models, we observe substantially higher robustness for the strongest proprietary models on handwriting and forms, while several open-weights systems remain competitive on standard printed layouts."
  },
  "2602.11957": {
    "title_zh": "双LLM是否优于单一模型？一种用于医药内容优化的师生双头LLM架构",
    "abstract_zh": "大型语言模型（LLMs）在医药等受监管领域的内容创作中应用日益广泛，其输出必须确保科学准确且符合法规要求。人工质量控制（QC）过程缓慢、易出错，可能成为内容发布的瓶颈。本文提出LRBTC，一种模块化的LLM与视觉语言模型（VLM）驱动的QC架构，涵盖语言、法规、品牌、技术和内容结构五大检查维度。LRBTC结合了师生双模型架构、人机协同（HITL）工作流以及瀑布式规则过滤，实现了可扩展、可验证的内容审核与优化。在AIReg-Bench基准测试中，该方法取得了83.0%的F1分数和97.5%的召回率，相比Gemini 2.5 Pro将违规漏检率降低了5倍；在CSpelling测试中，平均准确率提升了26.7%。错误分析进一步表明，当前模型虽能有效检测拼写错误（召回率92.5%），但在识别复杂医学语法错误（召回率25.0%）和标点错误（召回率41.7%）方面表现不足，这指明了未来研究的关键方向。本研究为高风险、强合规行业的内容质量控制提供了一种即插即用、可靠透明的实用解决方案。我们同时依据MIT许可证开放了演示系统访问权限。",
    "abstract_en": "Large language models (LLMs) are increasingly used to create content in regulated domains such as pharmaceuticals, where outputs must be scientifically accurate and legally compliant. Manual quality control (QC) is slow, error prone, and can become a publication bottleneck. We introduce LRBTC, a modular LLM and vision language model (VLM) driven QC architecture covering Language, Regulatory, Brand, Technical, and Content Structure checks. LRBTC combines a Student-Teacher dual model architecture, human in the loop (HITL) workflow with waterfall rule filtering to enable scalable, verifiable content validation and optimization. On AIReg-Bench, our approach achieves 83.0% F1 and 97.5% recall, reducing missed violations by 5x compared with Gemini 2.5 Pro. On CSpelling, it improves mean accuracy by 26.7%. Error analysis further reveals that while current models are strong at detecting misspellings (92.5 recall), they fail to identify complex medical grammatical (25.0 recall) and punctuation (41.7 recall) errors, highlighting a key area for future work. This work provides a practical, plug and play solution for reliable, transparent quality control of content in high stakes, compliance critical industries. We also provide access to our Demo under MIT Licenses."
  },
  "2602.17659": {
    "title_zh": "当视觉凌驾于语言之上：评估与缓解视觉语言动作模型中的反事实失败",
    "abstract_zh": "视觉-语言-动作模型（VLAs）有望将语言指令落地到机器人控制中，但在实践中往往无法忠实地遵循语言指令。当面对缺乏强场景特定监督的指令时，VLAs会遭受反事实失败：它们基于数据集偏见诱导的视觉捷径采取行动，反复执行已习得的行为，并选择训练中频繁出现的对象，而忽略语言意图。为系统研究此问题，我们引入了LIBERO-CF——首个针对VLAs的反事实基准测试，通过在视觉上合理的LIBERO布局中分配替代指令来评估语言遵循能力。我们的评估表明，反事实失败在现有最先进的VLAs中普遍存在且尚未得到充分探索。我们提出了反事实动作引导（CAG），一种简单而有效的双分支推理方案，能显式地正则化VLAs中的语言条件。CAG将标准VLA策略与无语言条件的视觉-动作（VA）模块相结合，实现了动作选择过程中的反事实比较。这一设计减少了对视觉捷径的依赖，提高了在低观测任务上的鲁棒性，且无需额外演示或修改现有架构或预训练模型。大量实验证明了其在不同VLA模型中的即插即用集成能力及持续的性能提升。例如，在LIBERO-CF上，CAG通过免训练策略将低观测任务的语言遵循准确率提升9.7%（π₀.₅指标），任务成功率提升3.6%；当与VA模型结合时，这两项指标分别进一步提升了15.5%和8.5%。在真实世界评估中，CAG平均减少了9.4%的反事实失败，并将任务成功率提高了17.2%。",
    "abstract_en": "Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average."
  },
  "2602.17345": {
    "title_zh": "什么在破坏具身人工智能安全：大语言模型漏洞、信息物理系统缺陷，还是其他因素？",
    "abstract_zh": "具身人工智能系统（如自动驾驶汽车、服务机器人和基于大语言模型的交互式智能体）正迅速从受控环境转向安全关键的实际部署。与非具身人工智能不同，具身智能的失败会导致不可逆转的物理后果，从而引发关于安全性、可靠性和稳健性的根本性问题。尽管现有研究主要从大语言模型漏洞或经典信息物理系统故障的角度分析具身人工智能，但本综述认为，这些视角单独来看均不足以解释现代具身系统中观察到的许多故障。我们提出，一类重要的故障源于具身化引发的系统级不匹配，而非孤立的模型缺陷或传统的信息物理系统攻击。具体而言，我们识别了四个核心见解，用以解释为何具身人工智能本质上更难保障安全：（i）语义正确性并不意味着物理安全，因为语言层面的推理抽象了几何、动力学和接触约束；（ii）由于非线性动力学和状态不确定性，相同的行动在不同物理状态下可能导致截然不同的结果；（iii）微小误差在紧密耦合的感知-决策-行动循环中传播并放大；（iv）安全性在时间或系统层级上不具备组合性，使得局部安全的决策可能累积为全局不安全的行为。这些见解表明，保障具身人工智能安全需要超越组件级防御，转向对物理风险、不确定性和故障传播的系统级推理。",
    "abstract_en": "Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from isolated model flaws or traditional CPS attacks. Specifically, we identify four core insights that explain why embodied AI is fundamentally harder to secure: (i) semantic correctness does not imply physical safety, as language-level reasoning abstracts away geometry, dynamics, and contact constraints; (ii) identical actions can lead to drastically different outcomes across physical states due to nonlinear dynamics and state uncertainty; (iii) small errors propagate and amplify across tightly coupled perception-decision-action loops; and (iv) safety is not compositional across time or system layers, enabling locally safe decisions to accumulate into globally unsafe behavior. These insights suggest that securing embodied AI requires moving beyond component-level defenses toward system-level reasoning about physical risk, uncertainty, and failure propagation."
  },
  "2602.17259": {
    "title_zh": "FRAPPE：通过多未来表示对齐将世界建模融入通用策略",
    "abstract_zh": "使视觉语言动作（VLA）模型能够预测环境动态（即世界建模）已被视为提升机器人推理与泛化能力的关键。然而，现有方法面临两大问题：1. 训练目标迫使模型过度关注像素级重建，限制了语义学习与泛化能力；2. 推理时依赖预测的未来观测常导致误差累积。为应对这些挑战，我们提出了并行渐进扩展的未来表示对齐方法（FRAPPE）。该方法采用两阶段微调策略：在中期训练阶段，模型学习预测未来观测的潜在表示；在后训练阶段，我们并行扩展计算负载，并同时与多个不同的视觉基础模型进行表示对齐。通过显著提升微调效率并减少对动作标注数据的依赖，FRAPPE为增强通用机器人策略的世界感知能力提供了一条可扩展且数据高效的路径。在RoboTwin基准测试和真实世界任务上的实验表明，FRAPPE优于现有先进方法，并在长时程和未见场景中展现出强大的泛化能力。",
    "abstract_en": "Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios."
  },
  "2602.17245": {
    "title_zh": "网络动词：面向智能网络可靠任务组合的类型化抽象",
    "abstract_zh": "网络正从人类浏览的媒介演变为软件代理代表用户执行操作的环境。大型语言模型（LLM）的进步使自然语言成为目标导向任务的实用接口，但当前大多数网络代理仍基于点击和按键等底层原语进行操作。这些操作脆弱、低效且难以验证。在补充如NLWeb检索语义层等内容导向工作的基础上，我们认为智能网络同样需要针对网络动作的语义层。我们提出**网络动词**，这是一个网络规模、类型化且语义化文档化的函数集合，通过统一接口暴露网站功能，无论其通过API还是稳健的客户端工作流实现。这些动词作为稳定且可组合的单元，可供代理发现、选择并合成为简洁的程序。该抽象统一了基于API和基于浏览器的范式，使LLM能够合成具有显式控制流和数据流的可靠、可审计工作流。动词可携带前置条件、后置条件、策略标签和日志支持，从而通过提供稳定接口提升**可靠性**，通过将数十个步骤简化为少量函数调用提高**效率**，并通过类型化契约和可检查的追踪实现**可验证性**。我们阐述了愿景、概念验证实现及代表性案例研究，展示了相较于现有代理的简洁且稳健的执行效果。最后，我们概述了标准化路线图，以使动词能在网络规模上可部署且可信赖。",
    "abstract_en": "The Web is evolving from a medium that humans browse to an environment where software agents act on behalf of users. Advances in large language models (LLMs) make natural language a practical interface for goal-directed tasks, yet most current web agents operate on low-level primitives such as clicks and keystrokes. These operations are brittle, inefficient, and difficult to verify. Complementing content-oriented efforts such as NLWeb's semantic layer for retrieval, we argue that the agentic web also requires a semantic layer for web actions. We propose \\textbf{Web Verbs}, a web-scale set of typed, semantically documented functions that expose site capabilities through a uniform interface, whether implemented through APIs or robust client-side workflows. These verbs serve as stable and composable units that agents can discover, select, and synthesize into concise programs. This abstraction unifies API-based and browser-based paradigms, enabling LLMs to synthesize reliable and auditable workflows with explicit control and data flow. Verbs can carry preconditions, postconditions, policy tags, and logging support, which improves \\textbf{reliability} by providing stable interfaces, \\textbf{efficiency} by reducing dozens of steps into a few function calls, and \\textbf{verifiability} through typed contracts and checkable traces. We present our vision, a proof-of-concept implementation, and representative case studies that demonstrate concise and robust execution compared to existing agents. Finally, we outline a roadmap for standardization to make verbs deployable and trustworthy at web scale."
  },
  "2602.17101": {
    "title_zh": "评估物体姿态估计与重建对机器人抓取成功率影响的基准研究",
    "abstract_zh": "三维重建是机器人感知任务（如六维物体姿态估计与抓取姿态生成）的基础层。现代物体三维重建方法能够从多视角图像生成视觉与几何上均令人印象深刻的网格模型，然而标准的几何评估未能反映重建质量如何影响下游任务（如机器人操作性能）。本文通过引入一个大规模、基于物理的基准来填补这一空白，该基准根据六维姿态估计器与三维网格模型在抓取功能上的有效性进行评估。我们通过在不同重建三维网格上生成抓取姿态，并在真实模型上执行这些姿态，模拟了使用不完美模型生成的抓取姿态如何影响与真实物体的交互，从而评估了姿态误差、抓取鲁棒性以及三维重建几何不准确性的综合影响。结果表明，重建伪影显著减少了抓取姿态候选数量，但在姿态估计准确的情况下对抓取性能影响可忽略。此外，研究发现抓取成功率与姿态误差的关系主要受空间误差主导，即使是简单的平移误差也能为对称物体的抓取姿态成功率提供洞察。这项工作揭示了感知系统如何与机器人物体操作相关联。",
    "abstract_en": "3D reconstruction serves as the foundational layer for numerous robotic perception tasks, including 6D object pose estimation and grasp pose generation. Modern 3D reconstruction methods for objects can produce visually and geometrically impressive meshes from multi-view images, yet standard geometric evaluations do not reflect how reconstruction quality influences downstream tasks such as robotic manipulation performance. This paper addresses this gap by introducing a large-scale, physics-based benchmark that evaluates 6D pose estimators and 3D mesh models based on their functional efficacy in grasping. We analyze the impact of model fidelity by generating grasps on various reconstructed 3D meshes and executing them on the ground-truth model, simulating how grasp poses generated with an imperfect model affect interaction with the real object. This assesses the combined impact of pose error, grasp robustness, and geometric inaccuracies from 3D reconstruction. Our results show that reconstruction artifacts significantly decrease the number of grasp pose candidates but have a negligible effect on grasping performance given an accurately estimated pose. Our results also reveal that the relationship between grasp success and pose error is dominated by spatial error, and even a simple translation error provides insight into the success of the grasping pose of symmetric objects. This work provides insight into how perception systems relate to object manipulation using robots."
  },
  "2602.16898": {
    "title_zh": "MALLVI：一种面向集成通用机器人操作的多智能体框架",
    "abstract_zh": "利用大语言模型进行机器人操作任务规划是一个新兴领域。现有方法通常依赖专用模型、微调或提示调整，并以开环方式运行，缺乏鲁棒的环境反馈，导致其在动态环境中表现脆弱。本文提出MALLVi，一种多智能体大语言与视觉框架，实现了基于闭环反馈驱动的机器人操作。给定自然语言指令和环境图像，MALLVi为机器人操作器生成可执行的原子动作。动作执行后，视觉语言模型评估环境反馈，并决定重复该过程或进入下一步。MALLVi并非使用单一模型，而是协调分解器、定位器、思考器和反思器等多个专用智能体，分别处理感知、定位、推理和高级规划。可选的描述器智能体提供初始状态的视觉记忆。反思器通过仅重新激活相关智能体来支持针对性错误检测与恢复，避免了完全重新规划。仿真和真实环境实验表明，迭代式闭环多智能体协调提升了零样本操作任务的泛化能力与成功率。代码发布于https://github.com/iman1234ahmadi/MALLVI。",
    "abstract_en": "Task planning for robotic manipulation with large language models (LLMs) is an emerging area. Prior approaches rely on specialized models, fine tuning, or prompt tuning, and often operate in an open loop manner without robust environmental feedback, making them fragile in dynamic settings.We present MALLVi, a Multi Agent Large Language and Vision framework that enables closed loop feedback driven robotic manipulation. Given a natural language instruction and an image of the environment, MALLVi generates executable atomic actions for a robot manipulator. After action execution, a Vision Language Model (VLM) evaluates environmental feedback and decides whether to repeat the process or proceed to the next step.Rather than using a single model, MALLVi coordinates specialized agents, Decomposer, Localizer, Thinker, and Reflector, to manage perception, localization, reasoning, and high level planning. An optional Descriptor agent provides visual memory of the initial state. The Reflector supports targeted error detection and recovery by reactivating only relevant agents, avoiding full replanning.Experiments in simulation and real world settings show that iterative closed loop multi agent coordination improves generalization and increases success rates in zero shot manipulation tasks.Code available at https://github.com/iman1234ahmadi/MALLVI."
  },
  "2602.16710": {
    "title_zh": "EgoScale：利用多样化的自我中心人类数据扩展灵巧操作能力",
    "abstract_zh": "人类行为是学习物理智能最具扩展性的数据来源之一，然而如何有效利用其实现灵巧操作仍不明确。尽管先前研究在受限环境中展示了人类到机器人的技能迁移，但大规模人类数据能否支持精细、高自由度的灵巧操作尚存疑问。我们提出EgoScale，一个基于大规模自我中心人类数据的人类到灵巧操作迁移框架。我们在超过20,854小时带有动作标注的自我中心人类视频上训练了一个视觉-语言-动作模型，数据规模是先前工作的20倍以上，并揭示了人类数据规模与验证损失之间的对数线性扩展规律。该验证损失与下游真实机器人性能强相关，从而确立大规模人类数据作为可预测的监督来源。除规模外，我们引入了一个简单的两阶段迁移方案：大规模人类预训练后接轻量级的人类-机器人对齐中期训练。这实现了强大的长时程灵巧操作能力，并能以极少的机器人监督进行单次任务适应。我们的最终策略在使用22自由度灵巧机械手时，相比无预训练基线平均成功率提升54%，并能有效迁移至低自由度机械手，表明大规模人类运动数据提供了一个可重用、与具体形态无关的运动先验。",
    "abstract_en": "Human behavior is among the most scalable sources of data for learning physical intelligence, yet how to effectively leverage it for dexterous manipulation remains unclear. While prior work demonstrates human to robot transfer in constrained settings, it is unclear whether large scale human data can support fine grained, high degree of freedom dexterous manipulation. We present EgoScale, a human to dexterous manipulation transfer framework built on large scale egocentric human data. We train a Vision Language Action (VLA) model on over 20,854 hours of action labeled egocentric human video, more than 20 times larger than prior efforts, and uncover a log linear scaling law between human data scale and validation loss. This validation loss strongly correlates with downstream real robot performance, establishing large scale human data as a predictable supervision source. Beyond scale, we introduce a simple two stage transfer recipe: large scale human pretraining followed by lightweight aligned human robot mid training. This enables strong long horizon dexterous manipulation and one shot task adaptation with minimal robot supervision. Our final policy improves average success rate by 54% over a no pretraining baseline using a 22 DoF dexterous robotic hand, and transfers effectively to robots with lower DoF hands, indicating that large scale human motion provides a reusable, embodiment agnostic motor prior."
  },
  "2602.16444": {
    "title_zh": "RoboGene：通过多样性驱动的智能体框架提升视觉语言动作预训练，实现真实世界任务生成",
    "abstract_zh": "通用机器人操作的发展受到多样化真实世界交互数据稀缺的阻碍。与视觉或语言领域可从网络收集数据不同，机器人数据收集是一个主动过程，涉及高昂的物理成本。因此，自动化任务生成以最大化数据价值，成为一个关键但尚未充分探索的挑战。现有手动方法难以扩展且偏向常见任务，而现成的基础模型常产生物理上不可行的指令幻觉。为解决这一问题，我们提出了RoboGene，一个旨在为单臂、双臂及移动机器人自动生成多样化、物理可行的操作任务的智能体框架。RoboGene整合了三个核心组件：用于广泛任务覆盖的多样性驱动采样、强制执行物理约束的自我反思机制，以及持续改进的人机协同优化。我们进行了广泛的定量分析和大规模真实世界实验，收集了包含1.8万条轨迹的数据集，并引入了新指标以评估任务质量、可行性和多样性。结果表明，RoboGene显著优于最先进的基础模型（如GPT-4o、Gemini 2.5 Pro）。此外，真实世界实验显示，使用RoboGene预训练的视觉语言动作模型实现了更高的成功率和更优的泛化能力，凸显了高质量任务生成的重要性。项目网址：https://robogene-boost-vla.github.io。",
    "abstract_en": "The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io."
  },
  "2602.15724": {
    "title_zh": "学习检索可导航候选对象以实现高效的视觉与语言导航",
    "abstract_zh": "视觉与语言导航（VLN）要求智能体遵循自然语言指令，在未见过的环境中进行导航。由于大型语言模型（LLM）的灵活性和推理能力，近期方法越来越多地将其用作高层导航器。然而，基于提示的LLM导航常因决策效率低下而受限，因为模型必须在每一步从头解释指令，并对嘈杂且冗长的可导航候选对象进行推理。本文提出一种检索增强框架，旨在不修改或微调底层语言模型的前提下，提升基于LLM的VLN的效率和稳定性。我们的方法在两个互补层面引入检索机制：在任务层面，通过指令级嵌入检索器选择语义相似的成功导航轨迹作为上下文示例，为指令落地提供任务特定的先验知识；在步骤层面，通过模仿学习训练的候选检索器在LLM推理前剪除无关的导航方向，从而减少动作歧义和提示复杂度。两个检索模块均设计为轻量级、模块化，且独立于LLM进行训练。我们在Room-to-Room（R2R）基准测试上评估了该方法，实验结果表明，在已见和未见环境中，成功率、最优成功率及SPL指标均获得持续提升。消融研究进一步显示，指令级示例检索与候选剪枝在全局引导和逐步决策效率方面具有互补优势。这些结果证明，检索增强的决策支持是提升基于LLM的视觉与语言导航的有效且可扩展策略。",
    "abstract_en": "Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation."
  },
  "2602.15682": {
    "title_zh": "下一代范式是用户中心智能体，而非平台中心服务",
    "abstract_zh": "现代数字服务已演变为不可或缺的工具，驱动着当前大规模信息系统的运行。然而，当前主流的平台中心模式——服务围绕平台驱动的指标（如用户参与度和转化率）进行优化——往往无法与用户的真实需求保持一致。尽管平台技术已取得显著进步，尤其是在集成大语言模型（LLMs）方面，但我们认为平台服务质量的提升未必能转化为对用户的真实益处。相反，平台中心服务将提供方目标置于用户福祉之上，导致与用户利益产生冲突。本文主张，数字服务的未来应从平台中心转向用户中心智能体。这些用户中心智能体优先保障隐私，与用户自定义目标保持一致，并赋予用户对其偏好和行为的控制权。随着大语言模型和设备端智能技术的进步，这一愿景的实现已成为可能。本文探讨了向用户中心智能转型的机遇与挑战，提出了一种可行的设备-云端实施管道，并讨论了其落地所需的治理与生态系统架构。",
    "abstract_en": "Modern digital services have evolved into indispensable tools, driving the present large-scale information systems. Yet, the prevailing platform-centric model, where services are optimized for platform-driven metrics such as engagement and conversion, often fails to align with users' true needs. While platform technologies have advanced significantly-especially with the integration of large language models (LLMs)-we argue that improvements in platform service quality do not necessarily translate to genuine user benefit. Instead, platform-centric services prioritize provider objectives over user welfare, resulting in conflicts against user interests. This paper argues that the future of digital services should shift from a platform-centric to a user-centric agent. These user-centric agents prioritize privacy, align with user-defined goals, and grant users control over their preferences and actions. With advancements in LLMs and on-device intelligence, the realization of this vision is now feasible. This paper explores the opportunities and challenges in transitioning to user-centric intelligence, presents a practical device-cloud pipeline for its implementation, and discusses the necessary governance and ecosystem structures for its adoption."
  },
  "2602.15400": {
    "title_zh": "一智体引领全局：通过显式世界表征赋能多模态大语言模型实现视觉与语言导航",
    "abstract_zh": "可导航智能体需同时理解高层语义指令与精确空间感知。基于多模态大语言模型（MLLMs）构建导航智能体因其强大的泛化能力展现出广阔前景，但当前紧耦合的设计严重制约了系统性能。本研究提出一种解耦设计，将低层空间状态估计与高层语义规划分离。区别于以往依赖预定义、过度简化的文本地图的方法，我们引入一种交互式度量世界表征，该表征能保持丰富且一致的信息，使MLLMs能够与之交互并进行推理决策。此外，通过引入反事实推理进一步激发MLLMs的潜能，而度量世界表征确保了生成动作的物理有效性。我们在仿真与真实环境中进行了全面实验：本方法在零样本条件下刷新了最佳性能，在R2R-CE和RxR-CE基准测试中分别达到48.8%和42.2%的成功率。为验证度量表征的普适性，我们展示了跨多样实体（包括轮式TurtleBot 4机器人及定制空中无人机）的零样本仿真到现实迁移能力。这些真实场景部署证实，我们的解耦框架为具身视觉与语言导航提供了鲁棒且领域无关的接口。",
    "abstract_en": "A navigable agent needs to understand both high-level semantic instructions and precise spatial perceptions. Building navigation agents centered on Multimodal Large Language Models (MLLMs) demonstrates a promising solution due to their powerful generalization ability. However, the current tightly coupled design dramatically limits system performance. In this work, we propose a decoupled design that separates low-level spatial state estimation from high-level semantic planning. Unlike previous methods that rely on predefined, oversimplified textual maps, we introduce an interactive metric world representation that maintains rich and consistent information, allowing MLLMs to interact with and reason on it for decision-making. Furthermore, counterfactual reasoning is introduced to further elicit MLLMs' capacity, while the metric world representation ensures the physical validity of the produced actions. We conduct comprehensive experiments in both simulated and real-world environments. Our method establishes a new zero-shot state-of-the-art, achieving 48.8\\% Success Rate (SR) in R2R-CE and 42.2\\% in RxR-CE benchmarks. Furthermore, to validate the versatility of our metric representation, we demonstrate zero-shot sim-to-real transfer across diverse embodiments, including a wheeled TurtleBot 4 and a custom-built aerial drone. These real-world deployments verify that our decoupled framework serves as a robust, domain-invariant interface for embodied Vision-and-Language navigation."
  },
  "2602.14401": {
    "title_zh": "pFedNavi：面向具身AI的结构感知个性化联邦视觉语言导航",
    "abstract_zh": "视觉语言导航（VLN）需要来自私有室内环境的大规模轨迹指令数据，这引发了显著的隐私担忧。联邦学习（FL）通过将数据保留在设备端来缓解这一问题，但传统的FL在VLN中面临环境和指令风格的极端跨客户端异构性挑战，导致单一的全局模型效果不佳。本文提出了pFedNavi，一种专为VLN设计的结构感知且动态自适应的个性化联邦学习框架。我们的核心思想是在关键处实现个性化：pFedNavi通过分层混合系数自适应地识别客户端特定层，并对选定组件（如编码器-解码器投影层和环境敏感的解码器层）执行细粒度参数融合，以平衡全局知识共享与本地专业化。我们在两个标准VLN基准测试R2R和RxR上评估了pFedNavi，使用了ResNet和CLIP两种视觉表示。在所有指标上，pFedNavi均一致优于基于FedAvg的VLN基线，导航成功率提升高达7.5%，轨迹保真度增益高达7.8%，同时在非独立同分布条件下收敛速度加快1.38倍。",
    "abstract_en": "Vision-Language Navigation VLN requires large-scale trajectory instruction data from private indoor environments, raising significant privacy concerns. Federated Learning FL mitigates this by keeping data on-device, but vanilla FL struggles under VLNs' extreme cross-client heterogeneity in environments and instruction styles, making a single global model suboptimal. This paper proposes pFedNavi, a structure-aware and dynamically adaptive personalized federated learning framework tailored for VLN. Our key idea is to personalize where it matters: pFedNavi adaptively identifies client-specific layers via layer-wise mixing coefficients, and performs fine-grained parameter fusion on the selected components (e.g., the encoder-decoder projection and environment-sensitive decoder layers) to balance global knowledge sharing with local specialization. We evaluate pFedNavi on two standard VLN benchmarks, R2R and RxR, using both ResNet and CLIP visual representations. Across all metrics, pFedNavi consistently outperforms the FedAvg-based VLN baseline, achieving up to 7.5% improvement in navigation success rate and up to 7.8% gain in trajectory fidelity, while converging 1.38x faster under non-IID conditions."
  },
  "2602.17645": {
    "title_zh": "通过细粒度细节定位推动黑盒大视觉语言模型攻击前沿",
    "abstract_zh": "针对大视觉语言模型（LVLM）的黑盒对抗攻击因梯度缺失和多模态边界复杂而极具挑战。尽管先前最先进的基于迁移的攻击方法（如M-Attack）通过源图像与目标图像之间的局部裁剪级匹配取得了良好效果，但我们发现这会导致迭代间产生高方差、近乎正交的梯度，破坏了连贯的局部对齐并导致优化不稳定。我们将此归因于：（i）ViT的平移敏感性产生尖峰状梯度；（ii）源与目标裁剪之间的结构不对称性。我们将局部匹配重新表述为源变换与目标语义的非对称期望，并构建了M-Attack的梯度去噪升级版本。在源端，多裁剪对齐（MCA）通过每轮迭代中对多个独立采样的局部视图梯度进行平均来降低方差。在目标端，辅助目标对齐（ATA）用来自语义相关分布的小型辅助集替代激进的目标增强，从而生成更平滑、低方差的目标流形。我们进一步将动量重新解释为补丁动量，通过重放历史裁剪梯度；结合改进的补丁尺寸集成（PE+），这增强了可迁移方向。这些模块共同构成了M-Attack-V2——一个对M-Attack的简单模块化增强方案，显著提升了针对前沿LVLM的基于迁移的黑盒攻击效果：在Claude-4.0上的成功率从8%提升至30%，Gemini-2.5-Pro从83%提升至97%，GPT-5从98%提升至100%，超越了先前的黑盒LVLM攻击方法。代码与数据已公开于：https://github.com/vila-lab/M-Attack-V2。",
    "abstract_en": "Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: https://github.com/vila-lab/M-Attack-V2."
  },
  "2602.17625": {
    "title_zh": "抗灾难性遗忘的单次增量联邦学习",
    "abstract_zh": "现代大数据系统产生大规模、异构且地理分散的隐私敏感流式数据，使得数据集中化处理面临挑战。尽管联邦学习（FL）提供了一种增强隐私的训练机制，但其假设数据流是静态的，并通过多轮学习协作模型，这在通信受限的场景下难以应对增量数据的学习。本文提出了单次增量联邦学习（OSI-FL），这是首个同时应对通信开销和灾难性遗忘双重挑战的FL框架。OSI-FL在单轮通信中，利用冻结的视觉语言模型（VLM）从每个客户端提取类别特定的嵌入表示，服务器端的预训练扩散模型使用这些嵌入合成与客户端数据分布相似的新数据。合成样本在服务器端用于训练。然而，仍存在两个挑战：i）增量到达的任务需要重新训练全局模型，ii）随着未来任务的到来，重新训练模型会引发灾难性遗忘。为此，我们通过选择性样本保留（SSR）增强训练，该方法基于样本损失识别并保留每个类别和任务对中最具信息量的前p个样本。SSR通过确保在后续迭代中将具有代表性的保留样本纳入训练，从而限制遗忘。实验结果表明，在三个基准数据集上的类增量和域增量场景中，OSI-FL均优于传统及单次FL基线方法。",
    "abstract_en": "Modern big-data systems generate massive, heterogeneous, and geographically dispersed streams that are large-scale and privacy-sensitive, making centralization challenging. While federated learning (FL) provides a privacy-enhancing training mechanism, it assumes a static data flow and learns a collaborative model over multiple rounds, making learning with \\textit{incremental} data challenging in limited-communication scenarios. This paper presents One-Shot Incremental Federated Learning (OSI-FL), the first FL framework that addresses the dual challenges of communication overhead and catastrophic forgetting. OSI-FL communicates category-specific embeddings, devised by a frozen vision-language model (VLM) from each client in a single communication round, which a pre-trained diffusion model at the server uses to synthesize new data similar to the client's data distribution. The synthesized samples are used on the server for training. However, two challenges still persist: i) tasks arriving incrementally need to retrain the global model, and ii) as future tasks arrive, retraining the model introduces catastrophic forgetting. To this end, we augment training with Selective Sample Retention (SSR), which identifies and retains the top-p most informative samples per category and task pair based on sample loss. SSR bounds forgetting by ensuring that representative retained samples are incorporated into training in further iterations. The experimental results indicate that OSI-FL outperforms baselines, including traditional and one-shot FL approaches, in both class-incremental and domain-incremental scenarios across three benchmark datasets."
  },
  "2602.17594": {
    "title_zh": "AI游戏商店：通过人类游戏实现机器通用智能的可扩展、开放式评估",
    "abstract_zh": "在技术飞速发展的时代，针对人类通用智能的广泛谱系来严格评估机器智能变得日益重要且充满挑战。传统的AI基准测试通常仅评估人类活动中有限范围的狭窄能力，且大多为静态设计，随着开发者显式或隐式地针对其进行优化，这些基准很快趋于饱和。我们提出，评估AI系统中类人通用智能的一个更有前景的方法是通过一种特别强大的通用游戏玩法形式：研究它们如何以及以何种水平玩并学会玩**所有可能的人类游戏**，并与具有相同经验水平、时间或其他资源的人类玩家进行比较。我们将“人类游戏”定义为由人类为人类设计的游戏，并论证了评估这一人类能够想象并享受的所有此类游戏空间——即“人类游戏多元宇宙”——的适宜性。为实现这一愿景迈出第一步，我们引入了AI游戏商店，这是一个可扩展且开放式的平台，利用大语言模型结合人类参与，通过自动从流行的人类数字游戏平台获取并适配标准化、容器化的游戏环境变体，来合成新的代表性人类游戏。作为概念验证，我们基于苹果应用商店和Steam的排行榜生成了100个此类游戏，并在短时游戏片段中评估了七个前沿视觉语言模型。最佳模型在大多数游戏上的得分低于人类平均水平的10%，尤其是在挑战世界模型学习、记忆和规划能力的游戏上表现不佳。最后，我们提出了一系列后续步骤，以将AI游戏商店建设为一种实用的方法，用于衡量并推动机器向类人通用智能的进展。",
    "abstract_en": "Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \\textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a \"human game\" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the \"Multiverse of Human Games\". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines."
  },
  "2602.17558": {
    "title_zh": "RetouchIQ：基于指令的图像修饰多模态大语言模型智能体与通用奖励机制",
    "abstract_zh": "多模态大语言模型（MLLMs）的最新进展，在将视觉-语言推理能力扩展至基于专业工具的图像编辑领域方面展现出巨大潜力，实现了直观且富有创意的编辑方式。一个颇具前景的方向是利用强化学习（RL）使MLLMs能够在专业图像编辑软件中推理并执行最优工具使用方案。然而，由于缺乏能够反映创意编辑固有主观性的可靠、可验证的奖励信号，训练过程仍面临挑战。本研究提出RetouchIQ框架，该框架通过由通用奖励模型引导的MLLM智能体，实现基于指令的可执行图像编辑。RetouchIQ能够解读用户指定的编辑意图，并生成相应的可执行图像调整方案，从而在高层审美目标与精确参数控制之间建立桥梁。为超越传统基于规则、使用手工设计指标计算与固定参考图像相似度的奖励机制，我们提出一种通用奖励模型——一种经过RL微调的MLLM，能够根据具体情况通过一组生成的指标评估编辑结果。随后，该奖励模型通过多模态推理提供标量反馈，从而实现具有高质量、指令一致梯度的强化学习。我们构建了一个包含19万条指令-推理对的数据集，并建立了基于指令的图像编辑新基准。实验表明，RetouchIQ在语义一致性和感知质量上均显著优于以往基于MLLM和扩散模型的编辑系统。我们的研究结果证明了通用奖励驱动的MLLM智能体作为专业图像编辑灵活、可解释且可执行的辅助工具的潜力。",
    "abstract_en": "Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inherently subjective nature of creative editing. In this work, we introduce RetouchIQ, a framework that performs instruction-based executable image editing through MLLM agents guided by a generalist reward model. RetouchIQ interprets user-specified editing intentions and generates corresponding, executable image adjustments, bridging high-level aesthetic goals with precise parameter control. To move beyond conventional, rule-based rewards that compute similarity against a fixed reference image using handcrafted metrics, we propose a generalist reward model, an RL fine-tuned MLLM that evaluates edited results through a set of generated metrics on a case-by-case basis. Then, the reward model provides scalar feedback through multimodal reasoning, enabling reinforcement learning with high-quality, instruction-consistent gradients. We curate an extended dataset with 190k instruction-reasoning pairs and establish a new benchmark for instruction-based image editing. Experiments show that RetouchIQ substantially improves both semantic consistency and perceptual quality over previous MLLM-based and diffusion-based editing systems. Our findings demonstrate the potential of generalist reward-driven MLLM agents as flexible, explainable, and executable assistants for professional image editing."
  },
  "2602.17555": {
    "title_zh": "GraphThinker：通过事件图思维强化视频推理",
    "abstract_zh": "视频推理需要理解视频中事件之间的因果关系，然而这些关系通常是隐含的，且人工标注成本高昂。现有的多模态大语言模型（MLLMs）通常通过密集描述或视频摘要来推断事件关系以进行视频推理，但这种建模仍缺乏因果理解。由于缺乏对视频事件内部及跨事件因果结构的显式建模，这些模型在视频推理过程中容易出现幻觉。本文提出GraphThinker，一种基于强化微调的方法，通过构建结构化的事件级场景图并增强视觉基础，共同减少视频推理中的幻觉。具体而言，我们首先利用MLLM构建基于事件的视频场景图（EVSG），显式建模事件内部及事件间的关系，并将这些形成的场景图作为中间思维过程融入MLLM中。在强化微调过程中，我们还引入了视觉注意力奖励机制，以加强视频基础并进一步缓解幻觉。我们在RexTime和VidHalluc两个数据集上评估GraphThinker，结果显示相较于现有方法，它能够更准确地捕捉对象和事件关系，实现更精确的事件定位，从而有效减少视频推理中的幻觉。",
    "abstract_en": "Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods."
  },
  "2602.17535": {
    "title_zh": "LATA：面向医学视觉语言模型置信度校准的拉普拉斯辅助转导自适应方法",
    "abstract_zh": "医学视觉语言模型在医学影像零样本识别任务中表现优异，但其在领域偏移下的可靠性依赖于具备理论保证的校准不确定性。分形置信预测虽能提供有限样本覆盖保证，但预测集常因规模过大（效率低下）且类别间覆盖不均衡（类别条件覆盖差异高）而受限，尤其在少样本、不平衡场景中更为突出；若直接使用校准标签进行自适应，则会破坏数据交换性并导致理论保证失效。本文提出LATA（拉普拉斯辅助转导自适应），这是一种无需训练和标注的优化方法：通过对图像间k近邻图上的零样本概率进行少量连续置信传播平均场更新平滑处理，在联合校准集与测试集上操作，并通过确定性变换保持分形置信预测的有效性。我们进一步提出可感知失败的置信评分，将其嵌入视觉语言不确定性框架，通过实例级难度与标签合理性提升固定覆盖率下的预测集效率与类别均衡性。LATA具有黑盒特性（无需更新视觉语言模型）、计算轻量（窗口化转导、无需反向传播），并包含可选先验调节机制，既可完全无标注运行，也可通过单次使用校准边缘分布实现标签感知变体。在三种医学视觉语言模型与九项下游任务的实验中，LATA持续缩小预测集规模与类别条件覆盖差异，在维持或收紧目标覆盖率的同时，超越现有转导基线方法，显著逼近需使用标签的方法性能，且计算开销大幅降低。系统的消融实验与定性分析表明，LATA能在保持数据交换性的前提下有效锐化零样本预测。",
    "abstract_en": "Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and voids guarantees. We propose \\texttt{\\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation), a \\textit{training- and label-free} refinement that operates on the joint calibration and test pool by smoothing zero-shot probabilities over an image-image k-NN graph using a small number of CCCP mean-field updates, preserving SCP validity via a deterministic transform. We further introduce a \\textit{failure-aware} conformal score that plugs into the vision-language uncertainty (ViLU) framework, providing instance-level difficulty and label plausibility to improve prediction set efficiency and class-wise balance at fixed coverage. \\texttt{\\textbf{LATA}} is black-box (no VLM updates), compute-light (windowed transduction, no backprop), and includes an optional prior knob that can run strictly label-free or, if desired, in a label-informed variant using calibration marginals once. Across \\textbf{three} medical VLMs and \\textbf{nine} downstream tasks, \\texttt{\\textbf{LATA}} consistently reduces set size and CCV while matching or tightening target coverage, outperforming prior transductive baselines and narrowing the gap to label-using methods, while using far less compute. Comprehensive ablations and qualitative analyses show that \\texttt{\\textbf{LATA}} sharpens zero-shot predictions without compromising exchangeability."
  },
  "2602.17478": {
    "title_zh": "QuPAINT：面向量子材料发现的物理感知指令调优方法",
    "abstract_zh": "由于层依赖的对比度微妙、标注数据有限以及不同实验室和成像设置间的显著差异，从光学显微镜图像中表征二维量子材料具有挑战性。现有视觉模型因缺乏物理先验知识，且无法泛化至新材料或硬件条件，在此领域表现不佳。本研究提出了一种新的物理感知多模态框架，从数据和模型两方面应对这些限制。我们首先介绍了Synthia，一种基于物理的合成数据生成器，可模拟量子材料薄片在薄膜干涉下的真实光学响应。Synthia生成多样且高质量的样本，有助于减少对专家手动标注的依赖。我们引入了QMat-Instruct，这是首个针对量子材料的大规模指令数据集，包含多模态、物理信息化的问答对，旨在教导多模态大语言模型理解薄片的外观和厚度。随后，我们提出了物理感知指令调优方法QuPAINT，这是一种多模态架构，通过物理信息注意力模块融合视觉嵌入与光学先验，从而实现更鲁棒和更具区分性的薄片表征。最后，我们建立了QF-Bench，一个涵盖多种材料、基底和成像设置的综合性基准，为公平且可复现的评估提供了标准化协议。",
    "abstract_en": "Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum material flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the dependence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quantum materials, comprising multimodal, physics-informed question-answer pairs designed to teach Multimodal Large Language Models (MLLMs) to understand the appearance and thickness of flakes. Then, we propose Physics-Aware Instruction Tuning (QuPAINT), a multimodal architecture that incorporates a Physics-Informed Attention module to fuse visual embeddings with optical priors, enabling more robust and discriminative flake representations. Finally, we establish QF-Bench, a comprehensive benchmark spanning multiple materials, substrates, and imaging settings, offering standardized protocols for fair and reproducible evaluation."
  },
  "2602.17419": {
    "title_zh": "EAGLE：面向多模态大语言模型免调优工业异常检测的专家增强注意力引导方法",
    "abstract_zh": "工业异常检测对智能制造至关重要，但许多深度学习方法仅能输出二元决策且语义解释有限。多模态大语言模型（MLLMs）虽能生成细粒度的语言分析，但现有方法通常需要昂贵的微调，且相较于轻量级专用检测器，其异常检测精度并未持续提升。本文提出面向MLLMs工业异常检测的专家增强注意力引导框架（EAGLE），该免调优框架通过集成专家模型输出来引导MLLMs实现精准检测与可解释的异常描述。我们进一步研究EAGLE如何影响MLLMs内部机制，通过分析中间层对异常图像区域的注意力分布发现：成功的异常检测与异常区域注意力集中度的提升相关，而EAGLE能有效促进这种对齐。在MVTec-AD和VisA数据集上的实验表明，EAGLE无需参数更新即可提升多种MLLMs的异常检测性能，达到与基于微调方法相当的效果。代码已开源：https://github.com/shengtun/Eagle",
    "abstract_en": "Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial anomaly detection in MLLMs (EAGLE), a tuning-free framework that integrates outputs from expert model to guide MLLMs toward both accurate detection and interpretable anomaly descriptions. We further study how EAGLE affects MLLMs internals by examining the attention distribution of MLLMs to the anomalous image regions in the intermediate layers. We observe that successful anomaly detection is associated with increased attention concentration on anomalous regions, and EAGLE tends to encourage this alignment. Experiments on MVTec-AD and VisA show that EAGLE improves anomaly detection performance across multiple MLLMs without any parameter updates, achieving results comparable to fine-tuning based methods. Code is available at \\href{https://github.com/shengtun/Eagle}{https://github.com/shengtun/Eagle}"
  },
  "2602.17196": {
    "title_zh": "EntropyPrune：基于矩阵熵引导的多模态大语言模型视觉令牌剪枝",
    "abstract_zh": "多模态大语言模型（MLLMs）因每张图像需处理数百个视觉令牌而产生高昂推理成本。尽管令牌剪枝已被证明能有效加速推理，但何时何地进行剪枝仍主要依赖启发式方法。现有方法通常基于静态、经验选择的层，这限制了其可解释性和跨模型的可迁移性。本研究引入矩阵熵视角，识别出“熵坍缩层”（ECL），即视觉表示的信息内容在此层出现急剧且一致的下降，从而为选择剪枝阶段提供了原则性准则。基于此观察，我们提出EntropyPrune，一种新颖的矩阵熵引导令牌剪枝框架，该框架量化单个视觉令牌的信息价值，并在不依赖注意力图的情况下剪除冗余令牌。此外，为实现高效计算，我们利用对偶格拉姆矩阵的谱等价性，降低了熵计算的复杂度，理论加速比最高可达64倍。在多样化多模态基准上的广泛实验表明，EntropyPrune在准确性和效率上均持续优于最先进的剪枝方法。在LLaVA-1.5-7B模型上，我们的方法实现了68.2%的浮点运算量（FLOPs）减少，同时保持了96.0%的原始性能。此外，EntropyPrune能有效泛化至高分辨率和基于视频的模型，凸显了其在实用MLLM加速中的强鲁棒性和可扩展性。代码将公开于https://github.com/YahongWang1/EntropyPrune。",
    "abstract_en": "Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an \"Entropy Collapse Layer\" (ECL), where the information content of visual representations exhibits a sharp and consistent drop, which provides a principled criterion for selecting the pruning stage. Building on this observation, we propose EntropyPrune, a novel matrix-entropy-guided token pruning framework that quantifies the information value of individual visual tokens and prunes redundant ones without relying on attention maps. Moreover, to enable efficient computation, we exploit the spectral equivalence of dual Gram matrices, reducing the complexity of entropy computation and yielding up to a 64x theoretical speedup. Extensive experiments on diverse multimodal benchmarks demonstrate that EntropyPrune consistently outperforms state-of-the-art pruning methods in both accuracy and efficiency. On LLaVA-1.5-7B, our method achieves a 68.2% reduction in FLOPs while preserving 96.0% of the original performance. Furthermore, EntropyPrune generalizes effectively to high-resolution and video-based models, highlighting the strong robustness and scalability in practical MLLM acceleration. The code will be publicly available at https://github.com/YahongWang1/EntropyPrune."
  },
  "2602.17186": {
    "title_zh": "基于视觉信息增益的大型视觉语言模型选择性训练",
    "abstract_zh": "大型视觉语言模型（LVLMs）已取得显著进展，但常受语言偏见影响，即在不依赖视觉证据的情况下生成答案。先前研究尝试通过解码策略、架构调整或精选指令数据来缓解此问题，但通常缺乏对单个训练样本或标记实际从图像中获益程度的定量衡量。本文提出视觉信息增益（VIG），一种基于困惑度的度量指标，用于衡量视觉输入所提供的预测不确定性降低程度。VIG支持在样本和标记级别进行细粒度分析，有效突显颜色、空间关系和属性等视觉基础元素。基于此，我们提出一种VIG引导的选择性训练方案，优先处理高VIG样本和标记。该方法通过专注于视觉信息丰富的样本和标记，提升了视觉基础能力并减轻了语言偏见，在显著减少监督的情况下实现了更优的性能。",
    "abstract_en": "Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens."
  }
}