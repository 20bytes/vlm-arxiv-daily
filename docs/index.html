<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>VLM-Arxiv-Daily</title>
  <style>
    :root { --bg: #f7f7f5; --card: #ffffff; --text: #1f2937; --muted: #6b7280; --line: #e5e7eb; --accent: #0f766e; }
    * { box-sizing: border-box; }
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; color: var(--text); background: var(--bg); }
    .wrap { max-width: 1120px; margin: 0 auto; padding: 28px 20px 60px; }
    header { background: var(--card); border: 1px solid var(--line); border-radius: 12px; padding: 20px 24px; }
    h1 { margin: 0 0 8px; font-size: 28px; }
    .sub { color: var(--muted); margin: 0 0 8px; }
    .updated { font-weight: 600; }
    .toc { margin: 16px 0 0; padding: 0; list-style: none; display: flex; flex-wrap: wrap; gap: 10px; }
    .toc a { text-decoration: none; color: var(--accent); background: #e7f3f1; padding: 4px 10px; border-radius: 999px; font-size: 13px; }
    section { margin-top: 24px; }
    table { width: 100%; border-collapse: collapse; background: var(--card); border: 1px solid var(--line); border-radius: 12px; overflow: hidden; }
    th, td { padding: 10px 12px; border-bottom: 1px solid var(--line); vertical-align: top; font-size: 14px; }
    th { background: #f3f4f6; text-align: left; white-space: nowrap; }
    tr:last-child td { border-bottom: none; }
    .eval { display: inline-flex; gap: 8px; }
    .eval button { border: 1px solid var(--line); background: #fff; padding: 2px 8px; border-radius: 6px; cursor: pointer; font-size: 14px; }
    .eval button.active { border-color: var(--accent); background: #e7f3f1; }
    .legend { color: var(--muted); font-size: 13px; margin: 8px 0 0; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <header>
      <h1>ğŸ¤– VLM-Arxiv-Daily</h1>
      <p class="sub">æ¯æ—¥è‡ªåŠ¨è¿½è¸ª Vision-Language-Action (VLA)ã€Vision-Language Navigation (VLN) å’Œ Vision-Language Models (VLM) çš„æœ€æ–° arXiv è®ºæ–‡ã€‚</p>
      <p class="updated">Updated on 2026.03.01</p>
      <ul class="toc">
        <li><a href="#vla">VLA</a></li>
        <li><a href="#vln">VLN</a></li>
        <li><a href="#vlm">VLM</a></li>
      </ul>
    </header>
    <section id="vla">
      <h2>ğŸ“Œ VLA</h2>
      <table>
        <thead>
          <tr>
            <th>Publish Date (YYYY-MM-DD)</th>
            <th>Title</th>
            <th>Authors</th>
            <th>PDF</th>
            <th>HJFY</th>
            <th>è¯„ä¼°</th>
          </tr>
        </thead>
        <tbody>
          <tr data-arxiv-id="2602.23205">
            <td>2026-02-26</td>
            <td>EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents<br>EmbodMocapï¼šé¢å‘å…·èº«æ™ºèƒ½ä½“çš„é‡å¤–å››ç»´äºº-åœºæ™¯é‡å»º<br><a href="abstracts/2602.23205.html">æ‘˜è¦</a></td>
            <td>Taku Komura Team</td>
            <td><a href="http://arxiv.org/abs/2602.23205">2602.23205</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.23205v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.22988">
            <td>2026-02-26</td>
            <td>Residual Koopman Spectral Profiling for Predicting and Preventing Transformer Training Instability<br>åŸºäºæ®‹å·®åº“æ™®æ›¼è°±åˆ†æé¢„æµ‹ä¸é˜²æ­¢Transformerè®­ç»ƒä¸ç¨³å®šæ€§<br><a href="abstracts/2602.22988.html">æ‘˜è¦</a></td>
            <td>Yutaka Matsuo Team</td>
            <td><a href="http://arxiv.org/abs/2602.22988">2602.22988</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.22988v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.22952">
            <td>2026-02-26</td>
            <td>Automated Robotic Needle Puncture for Percutaneous Dilatational Tracheostomy<br>ç»çš®æ‰©å¼ æ°”ç®¡åˆ‡å¼€æœ¯çš„è‡ªåŠ¨åŒ–æœºå™¨äººé’ˆç©¿åˆºç³»ç»Ÿ<br><a href="abstracts/2602.22952.html">æ‘˜è¦</a></td>
            <td>Andrew Weightman Team</td>
            <td><a href="http://arxiv.org/abs/2602.22952">2602.22952</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.22952v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.22896">
            <td>2026-02-26</td>
            <td>DySL-VLA: Efficient Vision-Language-Action Model Inference via Dynamic-Static Layer-Skipping for Robot Manipulation<br>DySL-VLAï¼šé€šè¿‡åŠ¨æ€-é™æ€å±‚è·³è·ƒå®ç°æœºå™¨äººæ“ä½œä¸­é«˜æ•ˆè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹æ¨ç†<br><a href="abstracts/2602.22896.html">æ‘˜è¦</a></td>
            <td>Meng Li Team</td>
            <td><a href="http://arxiv.org/abs/2602.22896">2602.22896</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.22896v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.22862">
            <td>2026-02-26</td>
            <td>GraspLDP: Towards Generalizable Grasping Policy via Latent Diffusion<br>GraspLDPï¼šåŸºäºæ½œåœ¨æ‰©æ•£çš„é€šç”¨åŒ–æŠ“å–ç­–ç•¥ç ”ç©¶<br><a href="abstracts/2602.22862.html">æ‘˜è¦</a></td>
            <td>Di Huang Team</td>
            <td><a href="http://arxiv.org/abs/2602.22862">2602.22862</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.22862v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.22666">
            <td>2026-02-26</td>
            <td>ArtPro: Self-Supervised Articulated Object Reconstruction with Adaptive Integration of Mobility Proposals<br>ArtProï¼šåŸºäºè‡ªé€‚åº”è¿åŠ¨æè®®é›†æˆçš„è‡ªç›‘ç£å…³èŠ‚ç‰©ä½“é‡å»º<br><a href="abstracts/2602.22666.html">æ‘˜è¦</a></td>
            <td>Changhe Tu Team</td>
            <td><a href="http://arxiv.org/abs/2602.22666">2602.22666</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.22666v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.22663">
            <td>2026-02-26</td>
            <td>Rethinking the Practicality of Vision-language-action Model: A Comprehensive Benchmark and An Improved Baseline<br>é‡æ–°å®¡è§†è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„å®ç”¨æ€§ï¼šä¸€ä¸ªç»¼åˆæ€§åŸºå‡†ä¸æ”¹è¿›åŸºçº¿<br><a href="abstracts/2602.22663.html">æ‘˜è¦</a></td>
            <td>Haoang Li Team</td>
            <td><a href="http://arxiv.org/abs/2602.22663">2602.22663</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.22663v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.22579">
            <td>2026-02-26</td>
            <td>Metamorphic Testing of Vision-Language Action-Enabled Robots<br>è§†è§‰-è¯­è¨€-åŠ¨ä½œèµ‹èƒ½æœºå™¨äººçš„èœ•å˜æµ‹è¯•<br><a href="abstracts/2602.22579.html">æ‘˜è¦</a></td>
            <td>Aitor Arrieta Team</td>
            <td><a href="http://arxiv.org/abs/2602.22579">2602.22579</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.22579v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.22514">
            <td>2026-02-26</td>
            <td>SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation<br>SignVLAï¼šä¸€ç§æ— éœ€æ³¨é‡Šçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¡†æ¶ï¼Œç”¨äºå®æ—¶æ‰‹è¯­å¼•å¯¼çš„æœºå™¨äººæ“ä½œ<br><a href="abstracts/2602.22514.html">æ‘˜è¦</a></td>
            <td>Zezhi Tang Team</td>
            <td><a href="http://arxiv.org/abs/2602.22514">2602.22514</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.22514v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.22474">
            <td>2026-02-25</td>
            <td>When to Act, Ask, or Learn: Uncertainty-Aware Policy Steering<br>ä½•æ—¶æ‰§è¡Œã€è¯¢é—®æˆ–å­¦ä¹ ï¼šä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„ç­–ç•¥å¼•å¯¼<br><a href="abstracts/2602.22474.html">æ‘˜è¦</a></td>
            <td>Andrea Bajcsy Team</td>
            <td><a href="http://arxiv.org/abs/2602.22474">2602.22474</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.22474v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.21172">
            <td>2026-02-24</td>
            <td>NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning<br>NoRDï¼šä¸€ç§æ— éœ€æ¨ç†ã€æ•°æ®é«˜æ•ˆé©±åŠ¨çš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br><a href="abstracts/2602.21172.html">æ‘˜è¦</a></td>
            <td>Wei Zhan Team</td>
            <td><a href="http://arxiv.org/abs/2602.21172">2602.21172</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.21172v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.21161">
            <td>2026-02-24</td>
            <td>ActionReasoning: Robot Action Reasoning in 3D Space with LLM for Robotic Brick Stacking<br>è¡ŒåŠ¨æ¨ç†ï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„æœºå™¨äººä¸‰ç»´ç©ºé—´åŠ¨ä½œæ¨ç†ä¸ç –å—å †å åº”ç”¨<br><a href="abstracts/2602.21161.html">æ‘˜è¦</a></td>
            <td>Brian Sheil Team</td>
            <td><a href="http://arxiv.org/abs/2602.21161">2602.21161</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.21161v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.21157">
            <td>2026-02-24</td>
            <td>HALO: A Unified Vision-Language-Action Model for Embodied Multimodal Chain-of-Thought Reasoning<br>HALOï¼šé¢å‘å…·èº«å¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†çš„ç»Ÿä¸€è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br><a href="abstracts/2602.21157.html">æ‘˜è¦</a></td>
            <td>Song Guo Team</td>
            <td><a href="http://arxiv.org/abs/2602.21157">2602.21157</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.21157v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.21015">
            <td>2026-02-24</td>
            <td>From Perception to Action: An Interactive Benchmark for Vision Reasoning<br>ä»æ„ŸçŸ¥åˆ°è¡ŒåŠ¨ï¼šè§†è§‰æ¨ç†çš„äº¤äº’å¼åŸºå‡†æµ‹è¯•<br><a href="abstracts/2602.21015.html">æ‘˜è¦</a></td>
            <td>Roy Ka-Wei Lee Team</td>
            <td><a href="http://arxiv.org/abs/2602.21015">2602.21015</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.21015v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.21013">
            <td>2026-02-24</td>
            <td>Notes-to-Self: Scratchpad Augmented VLAs for Memory Dependent Manipulation Tasks<br>è‡ªæˆ‘ç¬”è®°ï¼šç”¨äºä¾èµ–è®°å¿†æ“ä½œä»»åŠ¡çš„ä¾¿ç­¾å¢å¼ºå‹è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹<br><a href="abstracts/2602.21013.html">æ‘˜è¦</a></td>
            <td>Roland Memisevic Team</td>
            <td><a href="http://arxiv.org/abs/2602.21013">2602.21013</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.21013v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.20979">
            <td>2026-02-24</td>
            <td>Toward an Agentic Infused Software Ecosystem<br>è¿ˆå‘èµ‹èƒ½ä»£ç†çš„è½¯ä»¶ç”Ÿæ€ç³»ç»Ÿ<br><a href="abstracts/2602.20979.html">æ‘˜è¦</a></td>
            <td>Mark Marron Team</td>
            <td><a href="http://arxiv.org/abs/2602.20979">2602.20979</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.20979v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.20715">
            <td>2026-02-24</td>
            <td>IG-RFT: An Interaction-Guided RL Framework for VLA Models in Long-Horizon Robotic Manipulation<br>IG-RFTï¼šé¢å‘é•¿æ—¶ç¨‹æœºå™¨äººæ“ä½œçš„äº¤äº’å¼•å¯¼å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œç”¨äºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br><a href="abstracts/2602.20715.html">æ‘˜è¦</a></td>
            <td>Huixu Dong Team</td>
            <td><a href="http://arxiv.org/abs/2602.20715">2602.20715</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.20715v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.20687">
            <td>2026-02-24</td>
            <td>How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective<br>åŸºç¡€æŠ€èƒ½å¦‚ä½•å½±å“åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„å…·èº«æ™ºèƒ½ä½“ï¼šä¸€ä¸ªåŸç”Ÿè§†è§’<br><a href="abstracts/2602.20687.html">æ‘˜è¦</a></td>
            <td>Tong Xu Team</td>
            <td><a href="http://arxiv.org/abs/2602.20687">2602.20687</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.20687v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.20659">
            <td>2026-02-24</td>
            <td>Recursive Belief Vision Language Model<br>é€’å½’ä¿¡å¿µè§†è§‰è¯­è¨€æ¨¡å‹<br><a href="abstracts/2602.20659.html">æ‘˜è¦</a></td>
            <td>Nirav Patel Team</td>
            <td><a href="http://arxiv.org/abs/2602.20659">2602.20659</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.20659v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.20577">
            <td>2026-02-24</td>
            <td>Efficient and Explainable End-to-End Autonomous Driving via Masked Vision-Language-Action Diffusion<br>åŸºäºæ©ç è§†è§‰-è¯­è¨€-åŠ¨ä½œæ‰©æ•£çš„é«˜æ•ˆå¯è§£é‡Šç«¯åˆ°ç«¯è‡ªåŠ¨é©¾é©¶<br><a href="abstracts/2602.20577.html">æ‘˜è¦</a></td>
            <td>Ziran Wang Team</td>
            <td><a href="http://arxiv.org/abs/2602.20577">2602.20577</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.20577v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17659">
            <td>2026-02-19</td>
            <td>When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs<br>å½“è§†è§‰å‡Œé©¾äºè¯­è¨€ä¹‹ä¸Šï¼šè¯„ä¼°ä¸ç¼“è§£è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹ä¸­çš„åäº‹å®å¤±è´¥<br><a href="abstracts/2602.17659.html">æ‘˜è¦</a></td>
            <td>Mingyu Ding Team</td>
            <td><a href="http://arxiv.org/abs/2602.17659">2602.17659</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17659v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17345">
            <td>2026-02-19</td>
            <td>What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?<br>ä»€ä¹ˆåœ¨ç ´åå…·èº«äººå·¥æ™ºèƒ½å®‰å…¨ï¼šå¤§è¯­è¨€æ¨¡å‹æ¼æ´ã€ä¿¡æ¯ç‰©ç†ç³»ç»Ÿç¼ºé™·ï¼Œè¿˜æ˜¯å…¶ä»–å› ç´ ï¼Ÿ<br><a href="abstracts/2602.17345.html">æ‘˜è¦</a></td>
            <td>Yue Zhang Team</td>
            <td><a href="http://arxiv.org/abs/2602.17345">2602.17345</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17345v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17259">
            <td>2026-02-19</td>
            <td>FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment<br>FRAPPEï¼šé€šè¿‡å¤šæœªæ¥è¡¨ç¤ºå¯¹é½å°†ä¸–ç•Œå»ºæ¨¡èå…¥é€šç”¨ç­–ç•¥<br><a href="abstracts/2602.17259.html">æ‘˜è¦</a></td>
            <td>Donglin Wang Team</td>
            <td><a href="http://arxiv.org/abs/2602.17259">2602.17259</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17259v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17245">
            <td>2026-02-19</td>
            <td>Web Verbs: Typed Abstractions for Reliable Task Composition on the Agentic Web<br>ç½‘ç»œåŠ¨è¯ï¼šé¢å‘æ™ºèƒ½ç½‘ç»œå¯é ä»»åŠ¡ç»„åˆçš„ç±»å‹åŒ–æŠ½è±¡<br><a href="abstracts/2602.17245.html">æ‘˜è¦</a></td>
            <td>Suman Nath Team</td>
            <td><a href="http://arxiv.org/abs/2602.17245">2602.17245</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17245v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17101">
            <td>2026-02-19</td>
            <td>Benchmarking the Effects of Object Pose Estimation and Reconstruction on Robotic Grasping Success<br>è¯„ä¼°ç‰©ä½“å§¿æ€ä¼°è®¡ä¸é‡å»ºå¯¹æœºå™¨äººæŠ“å–æˆåŠŸç‡å½±å“çš„åŸºå‡†ç ”ç©¶<br><a href="abstracts/2602.17101.html">æ‘˜è¦</a></td>
            <td>Torsten Sattler Team</td>
            <td><a href="http://arxiv.org/abs/2602.17101">2602.17101</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17101v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.16898">
            <td>2026-02-18</td>
            <td>MALLVI: a multi agent framework for integrated generalized robotics manipulation<br>MALLVIï¼šä¸€ç§é¢å‘é›†æˆé€šç”¨æœºå™¨äººæ“ä½œçš„å¤šæ™ºèƒ½ä½“æ¡†æ¶<br><a href="abstracts/2602.16898.html">æ‘˜è¦</a></td>
            <td>Babak Khalaj Team</td>
            <td><a href="http://arxiv.org/abs/2602.16898">2602.16898</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.16898v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.16710">
            <td>2026-02-18</td>
            <td>EgoScale: Scaling Dexterous Manipulation with Diverse Egocentric Human Data<br>EgoScaleï¼šåˆ©ç”¨å¤šæ ·åŒ–çš„è‡ªæˆ‘ä¸­å¿ƒäººç±»æ•°æ®æ‰©å±•çµå·§æ“ä½œèƒ½åŠ›<br><a href="abstracts/2602.16710.html">æ‘˜è¦</a></td>
            <td>Linxi Fan Team</td>
            <td><a href="http://arxiv.org/abs/2602.16710">2602.16710</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.16710v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.16444">
            <td>2026-02-19</td>
            <td>RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation<br>RoboGeneï¼šé€šè¿‡å¤šæ ·æ€§é©±åŠ¨çš„æ™ºèƒ½ä½“æ¡†æ¶æå‡è§†è§‰è¯­è¨€åŠ¨ä½œé¢„è®­ç»ƒï¼Œå®ç°çœŸå®ä¸–ç•Œä»»åŠ¡ç”Ÿæˆ<br><a href="abstracts/2602.16444.html">æ‘˜è¦</a></td>
            <td>Jian Tang Team</td>
            <td><a href="http://arxiv.org/abs/2602.16444">2602.16444</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.16444v2">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.15724">
            <td>2026-02-17</td>
            <td>Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation<br>å­¦ä¹ æ£€ç´¢å¯å¯¼èˆªå€™é€‰å¯¹è±¡ä»¥å®ç°é«˜æ•ˆçš„è§†è§‰ä¸è¯­è¨€å¯¼èˆª<br><a href="abstracts/2602.15724.html">æ‘˜è¦</a></td>
            <td>Lina Yao Team</td>
            <td><a href="http://arxiv.org/abs/2602.15724">2602.15724</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.15724v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.15682">
            <td>2026-02-17</td>
            <td>The Next Paradigm Is User-Centric Agent, Not Platform-Centric Service<br>ä¸‹ä¸€ä»£èŒƒå¼æ˜¯ç”¨æˆ·ä¸­å¿ƒæ™ºèƒ½ä½“ï¼Œè€Œéå¹³å°ä¸­å¿ƒæœåŠ¡<br><a href="abstracts/2602.15682.html">æ‘˜è¦</a></td>
            <td>Enhong Chen Team</td>
            <td><a href="http://arxiv.org/abs/2602.15682">2602.15682</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.15682v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.12281">
            <td>2026-02-12</td>
            <td>Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment<br>æ‰©å±•éªŒè¯åœ¨è§†è§‰-è¯­è¨€-åŠ¨ä½œå¯¹é½ä¸­æ¯”æ‰©å±•ç­–ç•¥å­¦ä¹ æ›´æœ‰æ•ˆ<br><a href="abstracts/2602.12281.html">æ‘˜è¦</a></td>
            <td>Marco Pavone Team</td>
            <td><a href="http://arxiv.org/abs/2602.12281">2602.12281</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.12281v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.12136">
            <td>2026-02-12</td>
            <td>Embodied AI Agents for Team Collaboration in Co-located Blue-Collar Work<br>é¢å‘å…±å€è“é¢†å·¥ä½œå›¢é˜Ÿåä½œçš„å…·èº«äººå·¥æ™ºèƒ½ä½“<br><a href="abstracts/2602.12136.html">æ‘˜è¦</a></td>
            <td>Thomas Olsson Team</td>
            <td><a href="http://arxiv.org/abs/2602.12136">2602.12136</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.12136v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.12099">
            <td>2026-02-12</td>
            <td>GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning<br>GigaBrain-0.5M*ï¼šä¸€ç§åŸºäºä¸–ç•Œæ¨¡å‹å¼ºåŒ–å­¦ä¹ è®­ç»ƒçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br><a href="abstracts/2602.12099.html">æ‘˜è¦</a></td>
            <td>Zheng Zhu Team</td>
            <td><a href="http://arxiv.org/abs/2602.12099">2602.12099</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.12099v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.12063">
            <td>2026-02-12</td>
            <td>VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model<br>VLAWï¼šè§†è§‰-è¯­è¨€-åŠ¨ä½œç­–ç•¥ä¸ä¸–ç•Œæ¨¡å‹çš„è¿­ä»£ååŒæ”¹è¿›<br><a href="abstracts/2602.12063.html">æ‘˜è¦</a></td>
            <td>Chelsea Finn Team</td>
            <td><a href="http://arxiv.org/abs/2602.12063">2602.12063</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.12063v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.12062">
            <td>2026-02-12</td>
            <td>HoloBrain-0 Technical Report<br>HoloBrain-0æŠ€æœ¯æŠ¥å‘Š<br><a href="abstracts/2602.12062.html">æ‘˜è¦</a></td>
            <td>Zhizhong Su Team</td>
            <td><a href="http://arxiv.org/abs/2602.12062">2602.12062</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.12062v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.12032">
            <td>2026-02-12</td>
            <td>When would Vision-Proprioception Policies Fail in Robotic Manipulation?<br>è§†è§‰-æœ¬ä½“æ„ŸçŸ¥ç­–ç•¥åœ¨æœºå™¨äººæ“ä½œä¸­ä½•æ—¶ä¼šå¤±æ•ˆï¼Ÿ<br><a href="abstracts/2602.12032.html">æ‘˜è¦</a></td>
            <td>Di Hu Team</td>
            <td><a href="http://arxiv.org/abs/2602.12032">2602.12032</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.12032v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.11934">
            <td>2026-02-12</td>
            <td>Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control<br>Robot-DIFTï¼šæå–æ‰©æ•£ç‰¹å¾ä»¥å®ç°å‡ ä½•ä¸€è‡´çš„è§†è§‰è¿åŠ¨æ§åˆ¶<br><a href="abstracts/2602.11934.html">æ‘˜è¦</a></td>
            <td>Georgia Chalvatzaki Team</td>
            <td><a href="http://arxiv.org/abs/2602.11934">2602.11934</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.11934v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.11832">
            <td>2026-02-12</td>
            <td>JEPA-VLA: Video Predictive Embedding is Needed for VLA Models<br>JEPA-VLAï¼šè§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹éœ€è¦è§†é¢‘é¢„æµ‹æ€§åµŒå…¥<br><a href="abstracts/2602.11832.html">æ‘˜è¦</a></td>
            <td>Mingsheng Long Team</td>
            <td><a href="http://arxiv.org/abs/2602.11832">2602.11832</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.11832v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.11660">
            <td>2026-02-12</td>
            <td>Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes<br>Clutt3R-Segï¼šé¢å‘æ‚ä¹±åœºæ™¯ä¸­è¯­è¨€é©±åŠ¨æŠ“å–çš„ç¨€ç–è§†è§’ä¸‰ç»´å®ä¾‹åˆ†å‰²<br><a href="abstracts/2602.11660.html">æ‘˜è¦</a></td>
            <td>Ayoung Kim Team</td>
            <td><a href="http://arxiv.org/abs/2602.11660">2602.11660</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.11660v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.11643">
            <td>2026-02-12</td>
            <td>ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning<br>ViTaSï¼šé¢å‘è§†è§‰è¿åŠ¨å­¦ä¹ çš„è§†è§‰è§¦è§‰è½¯èåˆå¯¹æ¯”å­¦ä¹ <br><a href="abstracts/2602.11643.html">æ‘˜è¦</a></td>
            <td>Huazhe Xu Team</td>
            <td><a href="http://arxiv.org/abs/2602.11643">2602.11643</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.11643v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09878">
            <td>2026-02-10</td>
            <td>MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation<br>MVISTA-4Dï¼šå…·æœ‰æµ‹è¯•æ—¶åŠ¨ä½œæ¨ç†èƒ½åŠ›çš„è§†å›¾ä¸€è‡´4Dä¸–ç•Œæ¨¡å‹ï¼Œç”¨äºæœºå™¨äººæ“ä½œ<br><a href="abstracts/2602.09878.html">æ‘˜è¦</a></td>
            <td>Xiangyu Yue Team</td>
            <td><a href="http://arxiv.org/abs/2602.09878">2602.09878</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09878v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09856">
            <td>2026-02-10</td>
            <td>Code2World: A GUI World Model via Renderable Code Generation<br>Code2Worldï¼šé€šè¿‡å¯æ¸²æŸ“ä»£ç ç”Ÿæˆçš„GUIä¸–ç•Œæ¨¡å‹<br><a href="abstracts/2602.09856.html">æ‘˜è¦</a></td>
            <td>Kevin Qinghong Lin Team</td>
            <td><a href="http://arxiv.org/abs/2602.09856">2602.09856</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09856v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09849">
            <td>2026-02-10</td>
            <td>BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation<br>BagelVLAï¼šé€šè¿‡äº¤é”™è§†è§‰-è¯­è¨€-åŠ¨ä½œç”Ÿæˆå¢å¼ºé•¿æ—¶ç¨‹æ“ä½œèƒ½åŠ›<br><a href="abstracts/2602.09849.html">æ‘˜è¦</a></td>
            <td>Jianyu Chen Team</td>
            <td><a href="http://arxiv.org/abs/2602.09849">2602.09849</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09849v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09765">
            <td>2026-02-10</td>
            <td>NavDreamer: Video Models as Zero-Shot 3D Navigators<br>NavDreamerï¼šè§†é¢‘æ¨¡å‹ä½œä¸ºé›¶æ ·æœ¬ä¸‰ç»´å¯¼èˆªå™¨<br><a href="abstracts/2602.09765.html">æ‘˜è¦</a></td>
            <td>Fei Gao Team</td>
            <td><a href="http://arxiv.org/abs/2602.09765">2602.09765</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09765v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09722">
            <td>2026-02-10</td>
            <td>Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization<br>é‡æ–°å®¡è§†è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹çš„è§„æ¨¡åŒ–ï¼šå¯¹é½ã€æ··åˆä¸æ­£åˆ™åŒ–<br><a href="abstracts/2602.09722.html">æ‘˜è¦</a></td>
            <td>Qin Jin Team</td>
            <td><a href="http://arxiv.org/abs/2602.09722">2602.09722</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09722v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09657">
            <td>2026-02-10</td>
            <td>AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild<br>AutoFlyï¼šé¢å‘é‡å¤–æ— äººæœºè‡ªä¸»å¯¼èˆªçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br><a href="abstracts/2602.09657.html">æ‘˜è¦</a></td>
            <td>Hui Xiong Team</td>
            <td><a href="http://arxiv.org/abs/2602.09657">2602.09657</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09657v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09638">
            <td>2026-02-10</td>
            <td>VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model<br>VideoAffordï¼šåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»äºº-ç‰©äº¤äº’è§†é¢‘ä¸­å®ç°ä¸‰ç»´åŠŸèƒ½å¯åŠæ€§æ¥åœ°<br><a href="abstracts/2602.09638.html">æ‘˜è¦</a></td>
            <td>Hui Xiong Team</td>
            <td><a href="http://arxiv.org/abs/2602.09638">2602.09638</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09638v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09600">
            <td>2026-02-10</td>
            <td>Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures<br>Hand2Worldï¼šåŸºäºè‡ªç”±ç©ºé—´æ‰‹åŠ¿çš„è‡ªå›å½’ç¬¬ä¸€äººç§°äº¤äº’ç”Ÿæˆ<br><a href="abstracts/2602.09600.html">æ‘˜è¦</a></td>
            <td>Xingang Pan Team</td>
            <td><a href="http://arxiv.org/abs/2602.09600">2602.09600</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09600v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09583">
            <td>2026-02-10</td>
            <td>Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation<br>é¢å‘å¯å˜å½¢ç‰©ä½“æ“ä½œçš„åå¥½å¯¹é½è§†è§‰è¿åŠ¨æ‰©æ•£ç­–ç•¥<br><a href="abstracts/2602.09583.html">æ‘˜è¦</a></td>
            <td>Danica Kragic Team</td>
            <td><a href="http://arxiv.org/abs/2602.09583">2602.09583</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09583v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09534">
            <td>2026-02-10</td>
            <td>AUHead: Realistic Emotional Talking Head Generation via Action Units Control<br>AUHeadï¼šåŸºäºåŠ¨ä½œå•å…ƒæ§åˆ¶çš„é€¼çœŸæƒ…æ„Ÿè¯´è¯å¤´éƒ¨ç”Ÿæˆ<br><a href="abstracts/2602.09534.html">æ‘˜è¦</a></td>
            <td>Tat-Seng Chua Team</td>
            <td><a href="http://arxiv.org/abs/2602.09534">2602.09534</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09534v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04880">
            <td>2026-02-04</td>
            <td>Capturing Visual Environment Structure Correlates with Control Performance<br>æ•æ‰è§†è§‰ç¯å¢ƒç»“æ„ä¸æ§åˆ¶æ€§èƒ½çš„ç›¸å…³æ€§<br><a href="abstracts/2602.04880.html">æ‘˜è¦</a></td>
            <td>Yu-Xiong Wang Team</td>
            <td><a href="http://arxiv.org/abs/2602.04880">2602.04880</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04880v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04877">
            <td>2026-02-04</td>
            <td>CoWTracker: Tracking by Warping instead of Correlation<br>CoWTrackerï¼šé€šè¿‡å˜å½¢è€Œéç›¸å…³æ€§è¿›è¡Œè·Ÿè¸ª<br><a href="abstracts/2602.04877.html">æ‘˜è¦</a></td>
            <td>Andrea Vedaldi Team</td>
            <td><a href="http://arxiv.org/abs/2602.04877">2602.04877</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04877v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04635">
            <td>2026-02-04</td>
            <td>Relational Scene Graphs for Object Grounding of Natural Language Commands<br>é¢å‘è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­ç‰©ä½“å®šä½çš„å…³ç³»åœºæ™¯å›¾<br><a href="abstracts/2602.04635.html">æ‘˜è¦</a></td>
            <td>Ville Kyrki Team</td>
            <td><a href="http://arxiv.org/abs/2602.04635">2602.04635</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04635v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04600">
            <td>2026-02-04</td>
            <td>Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data<br>è¡ŒåŠ¨ã€æ„ŸçŸ¥ã€å†è¡ŒåŠ¨ï¼šä»å¤§è§„æ¨¡ç¬¬ä¸€äººç§°äººç±»æ•°æ®ä¸­å­¦ä¹ éé©¬å°”å¯å¤«ä¸»åŠ¨æ„ŸçŸ¥ç­–ç•¥<br><a href="abstracts/2602.04600.html">æ‘˜è¦</a></td>
            <td>Wenzhao Lian Team</td>
            <td><a href="http://arxiv.org/abs/2602.04600">2602.04600</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04600v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04522">
            <td>2026-02-04</td>
            <td>A Unified Complementarity-based Approach for Rigid-Body Manipulation and Motion Prediction<br>åŸºäºäº’è¡¥æ€§çš„ç»Ÿä¸€æ–¹æ³•åœ¨åˆšä½“æ“ä½œä¸è¿åŠ¨é¢„æµ‹ä¸­çš„åº”ç”¨<br><a href="abstracts/2602.04522.html">æ‘˜è¦</a></td>
            <td>Riddhiman Laha Team</td>
            <td><a href="http://arxiv.org/abs/2602.04522">2602.04522</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04522v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04515">
            <td>2026-02-04</td>
            <td>EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models<br>EgoActorï¼šé€šè¿‡è§†è§‰è¯­è¨€æ¨¡å‹å°†ä»»åŠ¡è§„åˆ’è½åœ°ä¸ºå…·èº«æœºå™¨äººçš„ç©ºé—´æ„ŸçŸ¥è‡ªæˆ‘ä¸­å¿ƒåŠ¨ä½œ<br><a href="abstracts/2602.04515.html">æ‘˜è¦</a></td>
            <td>BÃ¶rje F. Karlsson Team</td>
            <td><a href="http://arxiv.org/abs/2602.04515">2602.04515</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04515v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04411">
            <td>2026-02-04</td>
            <td>Self-evolving Embodied AI<br>è‡ªæ¼”åŒ–çš„å…·èº«äººå·¥æ™ºèƒ½<br><a href="abstracts/2602.04411.html">æ‘˜è¦</a></td>
            <td>Wenwu Zhu Team</td>
            <td><a href="http://arxiv.org/abs/2602.04411">2602.04411</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04411v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04315">
            <td>2026-02-04</td>
            <td>GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning<br>GeneralVLAï¼šå…·å¤‡çŸ¥è¯†å¼•å¯¼è½¨è¿¹è§„åˆ’çš„é€šç”¨è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br><a href="abstracts/2602.04315.html">æ‘˜è¦</a></td>
            <td>Hao Tang Team</td>
            <td><a href="http://arxiv.org/abs/2602.04315">2602.04315</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04315v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04243">
            <td>2026-02-04</td>
            <td>Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation<br>è§†è§’è‡³å…³é‡è¦ï¼šåˆ©ç”¨æ©ç è‡ªç¼–ç å™¨åŠ¨æ€ä¼˜åŒ–è§†è§‰æ“æ§çš„è§†è§’<br><a href="abstracts/2602.04243.html">æ‘˜è¦</a></td>
            <td>Wenzhao Lian Team</td>
            <td><a href="http://arxiv.org/abs/2602.04243">2602.04243</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04243v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04231">
            <td>2026-02-04</td>
            <td>GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning<br>GeoLanGï¼šåŸºäºç»Ÿä¸€RGB-Då¤šæ¨¡æ€å­¦ä¹ çš„å‡ ä½•æ„ŸçŸ¥è¯­è¨€å¼•å¯¼æŠ“å–<br><a href="abstracts/2602.04231.html">æ‘˜è¦</a></td>
            <td>Hongliang Ren Team</td>
            <td><a href="http://arxiv.org/abs/2602.04231">2602.04231</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04231v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02459">
            <td>2026-02-02</td>
            <td>TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments<br>TIC-VLAï¼šä¸€ç§ç”¨äºåŠ¨æ€ç¯å¢ƒä¸­æœºå™¨äººå¯¼èˆªçš„æ€æ§ä¸€ä½“åŒ–è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br><a href="abstracts/2602.02459.html">æ‘˜è¦</a></td>
            <td>Jiaqi Ma Team</td>
            <td><a href="http://arxiv.org/abs/2602.02459">2602.02459</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02459v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02454">
            <td>2026-02-02</td>
            <td>World-Gymnast: Training Robots with Reinforcement Learning in a World Model<br>ä¸–ç•Œä½“æ“å®¶ï¼šåœ¨ä¸–ç•Œæ¨¡å‹ä¸­é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœºå™¨äºº<br><a href="abstracts/2602.02454.html">æ‘˜è¦</a></td>
            <td>Sherry Yang Team</td>
            <td><a href="http://arxiv.org/abs/2602.02454">2602.02454</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02454v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02402">
            <td>2026-02-02</td>
            <td>SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation<br>SoMAï¼šé¢å‘æœºå™¨äººè½¯ä½“æ“ä½œçš„çœŸå®åˆ°ä»¿çœŸç¥ç»æ¨¡æ‹Ÿå™¨<br><a href="abstracts/2602.02402.html">æ‘˜è¦</a></td>
            <td>Jiangmiao Pang Team</td>
            <td><a href="http://arxiv.org/abs/2602.02402">2602.02402</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02402v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02212">
            <td>2026-02-02</td>
            <td>MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models<br>MAIN-VLAï¼šä¸ºè§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹å»ºæ¨¡æ„å›¾ä¸ç¯å¢ƒçš„æŠ½è±¡<br><a href="abstracts/2602.02212.html">æ‘˜è¦</a></td>
            <td>Lemiao Qiu Team</td>
            <td><a href="http://arxiv.org/abs/2602.02212">2602.02212</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02212v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02142">
            <td>2026-02-02</td>
            <td>FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation<br>FD-VLAï¼šç”¨äºæ¥è§¦ä¸°å¯Œæ“ä½œçš„åŠ›è’¸é¦è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br><a href="abstracts/2602.02142.html">æ‘˜è¦</a></td>
            <td>Haiyue Zhu Team</td>
            <td><a href="http://arxiv.org/abs/2602.02142">2602.02142</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02142v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02063">
            <td>2026-02-02</td>
            <td>See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers<br>See2Refineï¼šè§†è§‰-è¯­è¨€åé¦ˆæå‡åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„eHMIè¡Œä¸ºè®¾è®¡èƒ½åŠ›<br><a href="abstracts/2602.02063.html">æ‘˜è¦</a></td>
            <td>Takeo Igarashi Team</td>
            <td><a href="http://arxiv.org/abs/2602.02063">2602.02063</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02063v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.01834">
            <td>2026-02-02</td>
            <td>Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models<br>é¢å‘è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹æ¨ç†æ—¶å®‰å…¨æ€§çš„æ¦‚å¿µè¯å…¸å­¦ä¹ æ–¹æ³•<br><a href="abstracts/2602.01834.html">æ‘˜è¦</a></td>
            <td>Di Wang Team</td>
            <td><a href="http://arxiv.org/abs/2602.01834">2602.01834</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.01834v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.01811">
            <td>2026-02-02</td>
            <td>From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models<br>ä»ç²¾ç¡®è®¤çŸ¥åˆ°ç²¾å‡†æ‰§è¡Œï¼šé¢å‘è§†è§‰è¯­è¨€åŠ¨ä½œæ¨¡å‹çš„é€šç”¨è‡ªæ ¡æ­£ä¸ç»ˆæ­¢æ¡†æ¶<br><a href="abstracts/2602.01811.html">æ‘˜è¦</a></td>
            <td>Jianzong Wang Team</td>
            <td><a href="http://arxiv.org/abs/2602.01811">2602.01811</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.01811v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.01662">
            <td>2026-02-02</td>
            <td>AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act<br>AgenticLabï¼šä¸€ä¸ªèƒ½å¤Ÿè§‚å¯Ÿã€æ€è€ƒä¸è¡ŒåŠ¨çš„çœŸå®ä¸–ç•Œæœºå™¨äººæ™ºèƒ½ä½“å¹³å°<br><a href="abstracts/2602.01662.html">æ‘˜è¦</a></td>
            <td>Yu She Team</td>
            <td><a href="http://arxiv.org/abs/2602.01662">2602.01662</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.01662v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.01644">
            <td>2026-02-02</td>
            <td>From Perception to Action: Spatial AI Agents and World Models<br>ä»æ„ŸçŸ¥åˆ°è¡ŒåŠ¨ï¼šç©ºé—´äººå·¥æ™ºèƒ½ä»£ç†ä¸ä¸–ç•Œæ¨¡å‹<br><a href="abstracts/2602.01644.html">æ‘˜è¦</a></td>
            <td>Esteban Rojas Team</td>
            <td><a href="http://arxiv.org/abs/2602.01644">2602.01644</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.01644v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.23087">
            <td>2026-01-30</td>
            <td>Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation</td>
            <td>Wu Songwei et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.23087">2601.23087</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.23065">
            <td>2026-01-30</td>
            <td>EAG-PT: Emission-Aware Gaussians and Path Tracing for Indoor Scene Reconstruction and Editing</td>
            <td>Xijie Yang et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.23065">2601.23065</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22988">
            <td>2026-01-30</td>
            <td>Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation</td>
            <td>Di Zhang et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22988">2601.22988</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22948">
            <td>2026-01-30</td>
            <td>Alignment among Language, Vision and Action Representations</td>
            <td>Nicola Milano et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22948">2601.22948</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22868">
            <td>2026-01-30</td>
            <td>When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection</td>
            <td>Shashank Mishra et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22868">2601.22868</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22714">
            <td>2026-01-30</td>
            <td>Vision-Language Models Unlock Task-Centric Latent Actions</td>
            <td>Alexander Nikulin et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22714">2601.22714</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22701">
            <td>2026-01-30</td>
            <td>Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference</td>
            <td>Emilien BirÃ© et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22701">2601.22701</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22467">
            <td>2026-01-30</td>
            <td>CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control</td>
            <td>Jiaqi Shi et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22467">2601.22467</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22356">
            <td>2026-01-29</td>
            <td>PoSafeNet: Safe Learning with Poset-Structured Neural Nets</td>
            <td>Kiwan Wong et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22356">2601.22356</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22153">
            <td>2026-01-29</td>
            <td>DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation</td>
            <td>Haozhe Xie et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22153">2601.22153</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22046">
            <td>2026-01-29</td>
            <td>PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction</td>
            <td>Changjian Jiang et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22046">2601.22046</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22018">
            <td>2026-01-29</td>
            <td>PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy</td>
            <td>Jinhao Zhang et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22018">2601.22018</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.21998">
            <td>2026-01-29</td>
            <td>Causal World Modeling for Robot Control</td>
            <td>Lin Li et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.21998">2601.21998</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.21971">
            <td>2026-01-29</td>
            <td>MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts</td>
            <td>Lorenzo Mazza et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.21971">2601.21971</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.21926">
            <td>2026-01-29</td>
            <td>Information Filtering via Variational Regularization for Robot Manipulation</td>
            <td>Jinhao Zhang et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.21926">2601.21926</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.21751">
            <td>2026-01-29</td>
            <td>Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation</td>
            <td>Jiankun Peng et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.21751">2601.21751</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.21712">
            <td>2026-01-29</td>
            <td>CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation</td>
            <td>Xuanran Zhai et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.21712">2601.21712</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.21602">
            <td>2026-01-29</td>
            <td>AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation</td>
            <td>Jianli Sun et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.21602">2601.21602</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.21570">
            <td>2026-01-29</td>
            <td>EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots</td>
            <td>Zixing Lei et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.21570">2601.21570</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
        </tbody>
      </table>
      <p class="legend">è¯„ä¼°çŠ¶æ€ä¿å­˜åœ¨æµè§ˆå™¨æœ¬åœ°ï¼ˆlocalStorageï¼‰ï¼Œæ¢è®¾å¤‡/æµè§ˆå™¨ä¸ä¼šåŒæ­¥ã€‚</p>
    </section>
    <section id="vln">
      <h2>ğŸ“Œ VLN</h2>
      <table>
        <thead>
          <tr>
            <th>Publish Date (YYYY-MM-DD)</th>
            <th>Title</th>
            <th>Authors</th>
            <th>PDF</th>
            <th>HJFY</th>
            <th>è¯„ä¼°</th>
          </tr>
        </thead>
        <tbody>
          <tr data-arxiv-id="2602.18424">
            <td>2026-02-20</td>
            <td>CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation<br>CapNavï¼šåŸºäºèƒ½åŠ›æ¡ä»¶å®¤å†…å¯¼èˆªçš„è§†è§‰è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•<br><a href="abstracts/2602.18424.html">æ‘˜è¦</a></td>
            <td>Jon Froehlich Team</td>
            <td><a href="http://arxiv.org/abs/2602.18424">2602.18424</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.18424v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.15724">
            <td>2026-02-17</td>
            <td>Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation<br>å­¦ä¹ æ£€ç´¢å¯å¯¼èˆªå€™é€‰å¯¹è±¡ä»¥å®ç°é«˜æ•ˆçš„è§†è§‰ä¸è¯­è¨€å¯¼èˆª<br><a href="abstracts/2602.15724.html">æ‘˜è¦</a></td>
            <td>Lina Yao Team</td>
            <td><a href="http://arxiv.org/abs/2602.15724">2602.15724</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.15724v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.15400">
            <td>2026-02-17</td>
            <td>One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation<br>ä¸€æ™ºä½“å¼•é¢†å…¨å±€ï¼šé€šè¿‡æ˜¾å¼ä¸–ç•Œè¡¨å¾èµ‹èƒ½å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å®ç°è§†è§‰ä¸è¯­è¨€å¯¼èˆª<br><a href="abstracts/2602.15400.html">æ‘˜è¦</a></td>
            <td>Qi Wu Team</td>
            <td><a href="http://arxiv.org/abs/2602.15400">2602.15400</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.15400v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.14401">
            <td>2026-02-16</td>
            <td>pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI<br>pFedNaviï¼šé¢å‘å…·èº«AIçš„ç»“æ„æ„ŸçŸ¥ä¸ªæ€§åŒ–è”é‚¦è§†è§‰è¯­è¨€å¯¼èˆª<br><a href="abstracts/2602.14401.html">æ‘˜è¦</a></td>
            <td>Haibing Guan Team</td>
            <td><a href="http://arxiv.org/abs/2602.14401">2602.14401</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.14401v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.11598">
            <td>2026-02-12</td>
            <td>ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation<br>ABot-N0ï¼šé¢å‘é€šç”¨å…·èº«å¯¼èˆªçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œåŸºç¡€æ¨¡å‹æŠ€æœ¯æŠ¥å‘Š<br><a href="abstracts/2602.11598.html">æ‘˜è¦</a></td>
            <td>Mu Xu Team</td>
            <td><a href="http://arxiv.org/abs/2602.11598">2602.11598</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.11598v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09972">
            <td>2026-02-10</td>
            <td>Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning<br>Hydra-Navï¼šåŸºäºè‡ªé€‚åº”åŒè¿‡ç¨‹æ¨ç†çš„ç›®æ ‡å¯¼èˆª<br><a href="abstracts/2602.09972.html">æ‘˜è¦</a></td>
            <td>Yiming Gan Team</td>
            <td><a href="http://arxiv.org/abs/2602.09972">2602.09972</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09972v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09657">
            <td>2026-02-10</td>
            <td>AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild<br>AutoFlyï¼šé¢å‘é‡å¤–æ— äººæœºè‡ªä¸»å¯¼èˆªçš„è§†è§‰-è¯­è¨€-åŠ¨ä½œæ¨¡å‹<br><a href="abstracts/2602.09657.html">æ‘˜è¦</a></td>
            <td>Hui Xiong Team</td>
            <td><a href="http://arxiv.org/abs/2602.09657">2602.09657</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09657v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.08236">
            <td>2026-02-09</td>
            <td>When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning<br>ä½•æ—¶æƒ³è±¡ä¸æƒ³è±¡å¤šå°‘ï¼šåŸºäºä¸–ç•Œæ¨¡å‹çš„è‡ªé€‚åº”æµ‹è¯•æ—¶ç¼©æ”¾ç”¨äºè§†è§‰ç©ºé—´æ¨ç†<br><a href="abstracts/2602.08236.html">æ‘˜è¦</a></td>
            <td>Mohit Bansal Team</td>
            <td><a href="http://arxiv.org/abs/2602.08236">2602.08236</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.08236v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.07629">
            <td>2026-02-10</td>
            <td>LCLA: Language-Conditioned Latent Alignment for Vision-Language Navigation<br>LCLAï¼šé¢å‘è§†è§‰è¯­è¨€å¯¼èˆªçš„è¯­è¨€æ¡ä»¶åŒ–æ½œåœ¨å¯¹é½æ¡†æ¶<br><a href="abstracts/2602.07629.html">æ‘˜è¦</a></td>
            <td>Soumik Sarkar Team</td>
            <td><a href="http://arxiv.org/abs/2602.07629">2602.07629</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.07629v2">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.06427">
            <td>2026-02-06</td>
            <td>Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters<br>å¼¥åˆå®¤å†…å¤–é¸¿æ²Ÿï¼šé¢å‘æœ€åå‡ ç±³çš„è§†è§‰ä¸­å¿ƒåŒ–æŒ‡ä»¤å¼•å¯¼å…·èº«å¯¼èˆª<br><a href="abstracts/2602.06427.html">æ‘˜è¦</a></td>
            <td>Mu Xu Team</td>
            <td><a href="http://arxiv.org/abs/2602.06427">2602.06427</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.06427v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.06356">
            <td>2026-02-06</td>
            <td>Nipping the Drift in the Bud: Retrospective Rectification for Robust Vision-Language Navigation<br>é˜²å¾®æœæ¸ï¼šåŸºäºå›æº¯ä¿®æ­£çš„é²æ£’è§†è§‰è¯­è¨€å¯¼èˆª<br><a href="abstracts/2602.06356.html">æ‘˜è¦</a></td>
            <td>Weiying Xie Team</td>
            <td><a href="http://arxiv.org/abs/2602.06356">2602.06356</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.06356v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.05827">
            <td>2026-02-05</td>
            <td>Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation<br>ç¨€ç–è§†é¢‘ç”Ÿæˆæ¨åŠ¨ç°å®ä¸–ç•Œè¶…è§†è·è§†è§‰è¯­è¨€å¯¼èˆª<br><a href="abstracts/2602.05827.html">æ‘˜è¦</a></td>
            <td>Hongyang Li Team</td>
            <td><a href="http://arxiv.org/abs/2602.05827">2602.05827</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.05827v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.05789">
            <td>2026-02-05</td>
            <td>Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation<br>ä»–è€…ä¸­å¿ƒæ„ŸçŸ¥å™¨ï¼šé€šè¿‡æ¡†æ¶å®ä¾‹åŒ–ä»ä»–è€…è§†è§‰å…ˆéªŒä¸­è§£è€¦ä»–è€…ä¸­å¿ƒæ¨ç†<br><a href="abstracts/2602.05789.html">æ‘˜è¦</a></td>
            <td>Weiming Zhang Team</td>
            <td><a href="http://arxiv.org/abs/2602.05789">2602.05789</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.05789v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.05467">
            <td>2026-02-05</td>
            <td>MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation<br>MerNavï¼šä¸€ç§é«˜åº¦å¯æ³›åŒ–çš„è®°å¿†-æ‰§è¡Œ-å›é¡¾æ¡†æ¶ï¼Œç”¨äºé›¶æ ·æœ¬ç›®æ ‡å¯¼èˆª<br><a href="abstracts/2602.05467.html">æ‘˜è¦</a></td>
            <td>Mu Xu Team</td>
            <td><a href="http://arxiv.org/abs/2602.05467">2602.05467</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.05467v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02220">
            <td>2026-02-02</td>
            <td>LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation<br>LangMapï¼šé¢å‘å¼€æ”¾è¯æ±‡ç›®æ ‡å¯¼èˆªçš„åˆ†å±‚åŸºå‡†<br><a href="abstracts/2602.02220.html">æ‘˜è¦</a></td>
            <td>Anton van den Hengel Team</td>
            <td><a href="http://arxiv.org/abs/2602.02220">2602.02220</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02220v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.00551">
            <td>2026-01-31</td>
            <td>APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation<br>APEXï¼šä¸€ç§ç”¨äºå¼‚æ­¥ç©ºä¸­ç›®æ ‡å¯¼èˆªçš„è§£è€¦è®°å¿†å‹æ¢ç´¢å™¨<br><a href="abstracts/2602.00551.html">æ‘˜è¦</a></td>
            <td>Shuo Yang Team</td>
            <td><a href="http://arxiv.org/abs/2602.00551">2602.00551</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.00551v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.00222">
            <td>2026-02-03</td>
            <td>MapDream: Task-Driven Map Learning for Vision-Language Navigation<br>MapDreamï¼šé¢å‘è§†è§‰è¯­è¨€å¯¼èˆªçš„ä»»åŠ¡é©±åŠ¨åœ°å›¾å­¦ä¹ <br><a href="abstracts/2602.00222.html">æ‘˜è¦</a></td>
            <td>Zhaoxin Fan Team</td>
            <td><a href="http://arxiv.org/abs/2602.00222">2602.00222</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.00222v2">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.21751">
            <td>2026-01-29</td>
            <td>Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation<br>åŠ¨æ€æ‹“æ‰‘æ„ŸçŸ¥ï¼šæ‰“ç ´è§†è§‰è¯­è¨€å¯¼èˆªä¸­çš„ç²’åº¦åƒµåŒ–<br><a href="abstracts/2601.21751.html">æ‘˜è¦</a></td>
            <td>Xiaoming Wang Team</td>
            <td><a href="http://arxiv.org/abs/2601.21751">2601.21751</a></td>
            <td><a href="https://hjfy.top/arxiv/2601.21751v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.18492">
            <td>2026-01-26</td>
            <td>DV-VLN: Dual Verification for Reliable LLM-Based Vision-and-Language Navigation<br>DV-VLNï¼šåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ä¸è¯­è¨€å¯¼èˆªåŒé‡éªŒè¯å¯é æ¡†æ¶<br><a href="abstracts/2601.18492.html">æ‘˜è¦</a></td>
            <td>Shoujun Zhou Team</td>
            <td><a href="http://arxiv.org/abs/2601.18492">2601.18492</a></td>
            <td><a href="https://hjfy.top/arxiv/2601.18492v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.18188">
            <td>2026-01-26</td>
            <td>\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation<br>NaVIDAï¼šåŸºäºé€†åŠ¨åŠ›å­¦å¢å¼ºçš„è§†è§‰è¯­è¨€å¯¼èˆª<br><a href="abstracts/2601.18188.html">æ‘˜è¦</a></td>
            <td>Feng Zheng Team</td>
            <td><a href="http://arxiv.org/abs/2601.18188">2601.18188</a></td>
            <td><a href="https://hjfy.top/arxiv/2601.18188v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.15614">
            <td>2026-01-22</td>
            <td>AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning<br>AIONï¼šåŸºäºåŒç­–ç•¥å¼ºåŒ–å­¦ä¹ çš„ç©ºä¸­å®¤å†…ç›®æ ‡å¯¼èˆªç³»ç»Ÿ<br><a href="abstracts/2601.15614.html">æ‘˜è¦</a></td>
            <td>Lin Zhao Team</td>
            <td><a href="http://arxiv.org/abs/2601.15614">2601.15614</a></td>
            <td><a href="https://hjfy.top/arxiv/2601.15614v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.13976">
            <td>2026-01-23</td>
            <td>FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation<br>FantasyVLNï¼šé¢å‘è§†è§‰è¯­è¨€å¯¼èˆªçš„ç»Ÿä¸€å¤šæ¨¡æ€æ€ç»´é“¾æ¨ç†æ¡†æ¶<br><a href="abstracts/2601.13976.html">æ‘˜è¦</a></td>
            <td>Yonggang Qi Team</td>
            <td><a href="http://arxiv.org/abs/2601.13976">2601.13976</a></td>
            <td><a href="https://hjfy.top/arxiv/2601.13976v2">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.12766">
            <td>2026-01-19</td>
            <td>Spatial-VLN: Zero-Shot Vision-and-Language Navigation With Explicit Spatial Perception and Exploration<br>Spatial-VLNï¼šå…·å¤‡æ˜¾å¼ç©ºé—´æ„ŸçŸ¥ä¸æ¢ç´¢èƒ½åŠ›çš„é›¶æ ·æœ¬è§†è§‰è¯­è¨€å¯¼èˆª<br><a href="abstracts/2601.12766.html">æ‘˜è¦</a></td>
            <td>Feitian Zhang Team</td>
            <td><a href="http://arxiv.org/abs/2601.12766">2601.12766</a></td>
            <td><a href="https://hjfy.top/arxiv/2601.12766v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.09111">
            <td>2026-01-14</td>
            <td>Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning<br>è¿ˆå‘å¼€æ”¾ç¯å¢ƒä¸æŒ‡ä»¤ï¼šåŸºäºå¿«æ…¢äº¤äº’æ¨ç†çš„é€šç”¨è§†è§‰è¯­è¨€å¯¼èˆª<br><a href="abstracts/2601.09111.html">æ‘˜è¦</a></td>
            <td>Yahong Han Team</td>
            <td><a href="http://arxiv.org/abs/2601.09111">2601.09111</a></td>
            <td><a href="https://hjfy.top/arxiv/2601.09111v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.08868">
            <td>2026-01-11</td>
            <td>Residual Cross-Modal Fusion Networks for Audio-Visual Navigation</td>
            <td>Yi Wang et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.08868">2601.08868</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.08665">
            <td>2026-01-13</td>
            <td>VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory</td>
            <td>Shaoan Wang et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.08665">2601.08665</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.07375">
            <td>2026-01-12</td>
            <td>GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap</td>
            <td>Farzad Shami et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.07375">2601.07375</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
        </tbody>
      </table>
      <p class="legend">è¯„ä¼°çŠ¶æ€ä¿å­˜åœ¨æµè§ˆå™¨æœ¬åœ°ï¼ˆlocalStorageï¼‰ï¼Œæ¢è®¾å¤‡/æµè§ˆå™¨ä¸ä¼šåŒæ­¥ã€‚</p>
    </section>
    <section id="vlm">
      <h2>ğŸ“Œ VLM</h2>
      <table>
        <thead>
          <tr>
            <th>Publish Date (YYYY-MM-DD)</th>
            <th>Title</th>
            <th>Authors</th>
            <th>PDF</th>
            <th>HJFY</th>
            <th>è¯„ä¼°</th>
          </tr>
        </thead>
        <tbody>
          <tr data-arxiv-id="2602.23363">
            <td>2026-02-26</td>
            <td>MediX-R1: Open Ended Medical Reinforcement Learning<br>MediX-R1ï¼šå¼€æ”¾å¼åŒ»å­¦å¼ºåŒ–å­¦ä¹ æ¡†æ¶<br><a href="abstracts/2602.23363.html">æ‘˜è¦</a></td>
            <td>Hisham Cholakkal Team</td>
            <td><a href="http://arxiv.org/abs/2602.23363">2602.23363</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.23363v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.23351">
            <td>2026-02-26</td>
            <td>Scale Can&#x27;t Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning<br>è§„æ¨¡æ— æ³•å…‹æœè¯­ç”¨å­¦ï¼šæŠ¥å‘Šåå·®å¯¹è§†è§‰-è¯­è¨€æ¨ç†çš„å½±å“<br><a href="abstracts/2602.23351.html">æ‘˜è¦</a></td>
            <td>Ranjay Krishna Team</td>
            <td><a href="http://arxiv.org/abs/2602.23351">2602.23351</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.23351v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.23339">
            <td>2026-02-26</td>
            <td>Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?<br>æ£€ç´¢ä¸åˆ†å‰²ï¼šå°‘é‡ç¤ºä¾‹è¶³ä»¥å¼¥åˆå¼€æ”¾è¯æ±‡åˆ†å‰²ä¸­çš„ç›‘ç£é¸¿æ²Ÿå—ï¼Ÿ<br><a href="abstracts/2602.23339.html">æ‘˜è¦</a></td>
            <td>Giorgos Tolias Team</td>
            <td><a href="http://arxiv.org/abs/2602.23339">2602.23339</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.23339v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.23276">
            <td>2026-02-26</td>
            <td>CXReasonAgent: Evidence-Grounded Diagnostic Reasoning Agent for Chest X-rays<br>CXReasonAgentï¼šåŸºäºè¯æ®çš„èƒ¸éƒ¨Xå…‰è¯Šæ–­æ¨ç†æ™ºèƒ½ä½“<br><a href="abstracts/2602.23276.html">æ‘˜è¦</a></td>
            <td>Edward Choi Team</td>
            <td><a href="http://arxiv.org/abs/2602.23276">2602.23276</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.23276v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.23229">
            <td>2026-02-26</td>
            <td>Large Multimodal Models as General In-Context Classifiers<br>å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ä½œä¸ºé€šç”¨ä¸Šä¸‹æ–‡å†…åˆ†ç±»å™¨<br><a href="abstracts/2602.23229.html">æ‘˜è¦</a></td>
            <td>Elisa Ricci Team</td>
            <td><a href="http://arxiv.org/abs/2602.23229">2602.23229</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.23229v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.23228">
            <td>2026-02-26</td>
            <td>MovieTeller: Tool-augmented Movie Synopsis with ID Consistent Progressive Abstraction<br>MovieTellerï¼šåŸºäºå·¥å…·å¢å¼ºçš„ç”µå½±å‰§æƒ…æ‘˜è¦ä¸IDä¸€è‡´æ¸è¿›å¼æŠ½è±¡<br><a href="abstracts/2602.23228.html">æ‘˜è¦</a></td>
            <td>Gaoang Wang Team</td>
            <td><a href="http://arxiv.org/abs/2602.23228">2602.23228</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.23228v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.23153">
            <td>2026-02-26</td>
            <td>Efficient Encoder-Free Fourier-based 3D Large Multimodal Model<br>é«˜æ•ˆæ— ç¼–ç å™¨çš„åŸºäºå‚…é‡Œå¶å˜æ¢çš„3Då¤§å‹å¤šæ¨¡æ€æ¨¡å‹<br><a href="abstracts/2602.23153.html">æ‘˜è¦</a></td>
            <td>Fabio Poiesi Team</td>
            <td><a href="http://arxiv.org/abs/2602.23153">2602.23153</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.23153v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.23088">
            <td>2026-02-26</td>
            <td>Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy<br>ä»¥è¨€æ„å½¢ï¼šå¼±ç›‘ç£è§†è§‰-è¯­è¨€å»ºæ¨¡ç”¨äºäººè„‘æ˜¾å¾®æˆåƒ<br><a href="abstracts/2602.23088.html">æ‘˜è¦</a></td>
            <td>Christian Schiffer Team</td>
            <td><a href="http://arxiv.org/abs/2602.23088">2602.23088</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.23088v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.23013">
            <td>2026-02-26</td>
            <td>SubspaceAD: Training-Free Few-Shot Anomaly Detection via Subspace Modeling<br>SubspaceADï¼šåŸºäºå­ç©ºé—´å»ºæ¨¡çš„æ— è®­ç»ƒå°‘æ ·æœ¬å¼‚å¸¸æ£€æµ‹æ–¹æ³•<br><a href="abstracts/2602.23013.html">æ‘˜è¦</a></td>
            <td>Egor Bondarev Team</td>
            <td><a href="http://arxiv.org/abs/2602.23013">2602.23013</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.23013v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.22963">
            <td>2026-02-26</td>
            <td>FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning<br>FactGuardï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„æ™ºèƒ½ä½“è§†é¢‘è™šå‡ä¿¡æ¯æ£€æµ‹<br><a href="abstracts/2602.22963.html">æ‘˜è¦</a></td>
            <td>Zhaoqi Wang Team</td>
            <td><a href="http://arxiv.org/abs/2602.22963">2602.22963</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.22963v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.21186">
            <td>2026-02-24</td>
            <td>Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning<br>Spa3Rï¼šé¢å‘ä¸‰ç»´è§†è§‰æ¨ç†çš„é¢„æµ‹æ€§ç©ºé—´åœºå»ºæ¨¡<br><a href="abstracts/2602.21186.html">æ‘˜è¦</a></td>
            <td>Xinggang Wang Team</td>
            <td><a href="http://arxiv.org/abs/2602.21186">2602.21186</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.21186v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.21175">
            <td>2026-02-24</td>
            <td>Seeing Through Words: Controlling Visual Retrieval Quality with Language Models<br>é€è¿‡æ–‡å­—çœ‹è§ï¼šåˆ©ç”¨è¯­è¨€æ¨¡å‹æ§åˆ¶è§†è§‰æ£€ç´¢è´¨é‡<br><a href="abstracts/2602.21175.html">æ‘˜è¦</a></td>
            <td>Yun Fu Team</td>
            <td><a href="http://arxiv.org/abs/2602.21175">2602.21175</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.21175v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.21142">
            <td>2026-02-24</td>
            <td>LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis<br>LUMENï¼šç”¨äºé¢„åä¸è¯Šæ–­çš„çºµå‘å¤šæ¨¡æ€æ”¾å°„å­¦æ¨¡å‹<br><a href="abstracts/2602.21142.html">æ‘˜è¦</a></td>
            <td>Marius George Linguraru Team</td>
            <td><a href="http://arxiv.org/abs/2602.21142">2602.21142</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.21142v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.21054">
            <td>2026-02-24</td>
            <td>VAUQ: Vision-Aware Uncertainty Quantification for LVLM Self-Evaluation<br>VAUQï¼šé¢å‘LVLMè‡ªè¯„ä¼°çš„è§†è§‰æ„ŸçŸ¥ä¸ç¡®å®šæ€§é‡åŒ–<br><a href="abstracts/2602.21054.html">æ‘˜è¦</a></td>
            <td>Sharon Li Team</td>
            <td><a href="http://arxiv.org/abs/2602.21054">2602.21054</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.21054v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.21053">
            <td>2026-02-24</td>
            <td>OCR-Agent: Agentic OCR with Capability and Memory Reflection<br>OCR-Agentï¼šå…·å¤‡èƒ½åŠ›ä¸è®°å¿†åæ€çš„æ™ºèƒ½OCRä»£ç†<br><a href="abstracts/2602.21053.html">æ‘˜è¦</a></td>
            <td>Ying Cai Team</td>
            <td><a href="http://arxiv.org/abs/2602.21053">2602.21053</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.21053v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.21035">
            <td>2026-02-24</td>
            <td>Not Just What&#x27;s There: Enabling CLIP to Comprehend Negated Visual Descriptions Without Fine-tuning<br>ä¸æ­¢äºæ‰€è§ï¼šæ— éœ€å¾®è°ƒï¼Œè®©CLIPç†è§£å¦å®šçš„è§†è§‰æè¿°<br><a href="abstracts/2602.21035.html">æ‘˜è¦</a></td>
            <td>Zejiang He Team</td>
            <td><a href="http://arxiv.org/abs/2602.21035">2602.21035</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.21035v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.21015">
            <td>2026-02-24</td>
            <td>From Perception to Action: An Interactive Benchmark for Vision Reasoning<br>ä»æ„ŸçŸ¥åˆ°è¡ŒåŠ¨ï¼šè§†è§‰æ¨ç†çš„äº¤äº’å¼åŸºå‡†æµ‹è¯•<br><a href="abstracts/2602.21015.html">æ‘˜è¦</a></td>
            <td>Roy Ka-Wei Lee Team</td>
            <td><a href="http://arxiv.org/abs/2602.21015">2602.21015</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.21015v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.20980">
            <td>2026-02-24</td>
            <td>CrystaL: Spontaneous Emergence of Visual Latents in MLLMs<br>CrystaLï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­è§†è§‰æ½œåœ¨ç‰¹å¾çš„è‡ªå‘æ¶Œç°<br><a href="abstracts/2602.20980.html">æ‘˜è¦</a></td>
            <td>Xiang Li Team</td>
            <td><a href="http://arxiv.org/abs/2602.20980">2602.20980</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.20980v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.20972">
            <td>2026-02-24</td>
            <td>Are Multimodal Large Language Models Good Annotators for Image Tagging?<br>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ˜¯å›¾åƒæ ‡æ³¨çš„ä¼˜ç§€æ³¨é‡Šè€…å—ï¼Ÿ<br><a href="abstracts/2602.20972.html">æ‘˜è¦</a></td>
            <td>Masashi Sugiyama Team</td>
            <td><a href="http://arxiv.org/abs/2602.20972">2602.20972</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.20972v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.20913">
            <td>2026-02-24</td>
            <td>LongVideo-R1: Smart Navigation for Low-cost Long Video Understanding<br>LongVideo-R1ï¼šé¢å‘ä½æˆæœ¬é•¿è§†é¢‘ç†è§£çš„æ™ºèƒ½å¯¼èˆªæ–¹æ³•<br><a href="abstracts/2602.20913.html">æ‘˜è¦</a></td>
            <td>Qixiang Ye Team</td>
            <td><a href="http://arxiv.org/abs/2602.20913">2602.20913</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.20913v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17645">
            <td>2026-02-19</td>
            <td>Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting<br>é€šè¿‡ç»†ç²’åº¦ç»†èŠ‚å®šä½æ¨åŠ¨é»‘ç›’å¤§è§†è§‰è¯­è¨€æ¨¡å‹æ”»å‡»å‰æ²¿<br><a href="abstracts/2602.17645.html">æ‘˜è¦</a></td>
            <td>Zhiqiang Shen Team</td>
            <td><a href="http://arxiv.org/abs/2602.17645">2602.17645</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17645v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17625">
            <td>2026-02-19</td>
            <td>Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning<br>æŠ—ç¾éš¾æ€§é—å¿˜çš„å•æ¬¡å¢é‡è”é‚¦å­¦ä¹ <br><a href="abstracts/2602.17625.html">æ‘˜è¦</a></td>
            <td>Monowar Bhuyan Team</td>
            <td><a href="http://arxiv.org/abs/2602.17625">2602.17625</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17625v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17594">
            <td>2026-02-19</td>
            <td>AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games<br>AIæ¸¸æˆå•†åº—ï¼šé€šè¿‡äººç±»æ¸¸æˆå®ç°æœºå™¨é€šç”¨æ™ºèƒ½çš„å¯æ‰©å±•ã€å¼€æ”¾å¼è¯„ä¼°<br><a href="abstracts/2602.17594.html">æ‘˜è¦</a></td>
            <td>Joshua B. Tenenbaum Team</td>
            <td><a href="http://arxiv.org/abs/2602.17594">2602.17594</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17594v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17558">
            <td>2026-02-19</td>
            <td>RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward<br>RetouchIQï¼šåŸºäºæŒ‡ä»¤çš„å›¾åƒä¿®é¥°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹æ™ºèƒ½ä½“ä¸é€šç”¨å¥–åŠ±æœºåˆ¶<br><a href="abstracts/2602.17558.html">æ‘˜è¦</a></td>
            <td>Handong Zhao Team</td>
            <td><a href="http://arxiv.org/abs/2602.17558">2602.17558</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17558v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17555">
            <td>2026-02-19</td>
            <td>GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking<br>GraphThinkerï¼šé€šè¿‡äº‹ä»¶å›¾æ€ç»´å¼ºåŒ–è§†é¢‘æ¨ç†<br><a href="abstracts/2602.17555.html">æ‘˜è¦</a></td>
            <td>Shaogang Gong Team</td>
            <td><a href="http://arxiv.org/abs/2602.17555">2602.17555</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17555v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17535">
            <td>2026-02-19</td>
            <td>LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs<br>LATAï¼šé¢å‘åŒ»å­¦è§†è§‰è¯­è¨€æ¨¡å‹ç½®ä¿¡åº¦æ ¡å‡†çš„æ‹‰æ™®æ‹‰æ–¯è¾…åŠ©è½¬å¯¼è‡ªé€‚åº”æ–¹æ³•<br><a href="abstracts/2602.17535.html">æ‘˜è¦</a></td>
            <td>Zongyuan Ge Team</td>
            <td><a href="http://arxiv.org/abs/2602.17535">2602.17535</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17535v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17478">
            <td>2026-02-19</td>
            <td>QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery<br>QuPAINTï¼šé¢å‘é‡å­ææ–™å‘ç°çš„ç‰©ç†æ„ŸçŸ¥æŒ‡ä»¤è°ƒä¼˜æ–¹æ³•<br><a href="abstracts/2602.17478.html">æ‘˜è¦</a></td>
            <td>Khoa Luu Team</td>
            <td><a href="http://arxiv.org/abs/2602.17478">2602.17478</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17478v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17419">
            <td>2026-02-19</td>
            <td>EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models<br>EAGLEï¼šé¢å‘å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹å…è°ƒä¼˜å·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„ä¸“å®¶å¢å¼ºæ³¨æ„åŠ›å¼•å¯¼æ–¹æ³•<br><a href="abstracts/2602.17419.html">æ‘˜è¦</a></td>
            <td>Seon Han Choi Team</td>
            <td><a href="http://arxiv.org/abs/2602.17419">2602.17419</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17419v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17196">
            <td>2026-02-19</td>
            <td>EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models<br>EntropyPruneï¼šåŸºäºçŸ©é˜µç†µå¼•å¯¼çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹è§†è§‰ä»¤ç‰Œå‰ªæ<br><a href="abstracts/2602.17196.html">æ‘˜è¦</a></td>
            <td>Lianghua He Team</td>
            <td><a href="http://arxiv.org/abs/2602.17196">2602.17196</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17196v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.17186">
            <td>2026-02-19</td>
            <td>Selective Training for Large Vision Language Models via Visual Information Gain<br>åŸºäºè§†è§‰ä¿¡æ¯å¢ç›Šçš„å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹é€‰æ‹©æ€§è®­ç»ƒ<br><a href="abstracts/2602.17186.html">æ‘˜è¦</a></td>
            <td>Sangheum Hwang Team</td>
            <td><a href="http://arxiv.org/abs/2602.17186">2602.17186</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.17186v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.12281">
            <td>2026-02-12</td>
            <td>Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment<br>æ‰©å±•éªŒè¯åœ¨è§†è§‰-è¯­è¨€-åŠ¨ä½œå¯¹é½ä¸­æ¯”æ‰©å±•ç­–ç•¥å­¦ä¹ æ›´æœ‰æ•ˆ<br><a href="abstracts/2602.12281.html">æ‘˜è¦</a></td>
            <td>Marco Pavone Team</td>
            <td><a href="http://arxiv.org/abs/2602.12281">2602.12281</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.12281v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.12203">
            <td>2026-02-12</td>
            <td>ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images<br>ExStrucTinyï¼šé¢å‘æ–‡æ¡£å›¾åƒä¸­æ¨¡å¼å¯å˜ç»“æ„åŒ–ä¿¡æ¯æå–çš„åŸºå‡†æ•°æ®é›†<br><a href="abstracts/2602.12203.html">æ‘˜è¦</a></td>
            <td>Manuela Veloso Team</td>
            <td><a href="http://arxiv.org/abs/2602.12203">2602.12203</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.12203v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.12196">
            <td>2026-02-12</td>
            <td>Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education<br>è§†è§‰æ¨ç†åŸºå‡†ï¼šè¯„ä¼°å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹åœ¨åŸºç¡€æ•™è‚²è¯¾å ‚çœŸå®è§†è§‰é—®é¢˜ä¸Šçš„è¡¨ç°<br><a href="abstracts/2602.12196.html">æ‘˜è¦</a></td>
            <td>Oliver G. B. Garrod Team</td>
            <td><a href="http://arxiv.org/abs/2602.12196">2602.12196</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.12196v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.12159">
            <td>2026-02-12</td>
            <td>3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting<br>3DGSNavï¼šé€šè¿‡ä¸»åŠ¨3Dé«˜æ–¯æ³¼æº…å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹åœ¨ç‰©ä½“å¯¼èˆªä¸­çš„æ¨ç†èƒ½åŠ›<br><a href="abstracts/2602.12159.html">æ‘˜è¦</a></td>
            <td>Xinyi Yu Team</td>
            <td><a href="http://arxiv.org/abs/2602.12159">2602.12159</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.12159v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.12092">
            <td>2026-02-12</td>
            <td>DeepSight: An All-in-One LM Safety Toolkit<br>DeepSightï¼šä¸€ä½“åŒ–å¤§å‹æ¨¡å‹å®‰å…¨å·¥å…·ç®±<br><a href="abstracts/2602.12092.html">æ‘˜è¦</a></td>
            <td>Xia Hu Team</td>
            <td><a href="http://arxiv.org/abs/2602.12092">2602.12092</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.12092v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.12065">
            <td>2026-02-12</td>
            <td>Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning<br>å¯ä¾›æ€§å›¾åŒ–ä»»åŠ¡ä¸–ç•Œï¼šé¢å‘å¯æ‰©å±•å…·èº«å­¦ä¹ çš„è‡ªæ¼”åŒ–ä»»åŠ¡ç”Ÿæˆ<br><a href="abstracts/2602.12065.html">æ‘˜è¦</a></td>
            <td>Changshui Zhang Team</td>
            <td><a href="http://arxiv.org/abs/2602.12065">2602.12065</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.12065v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.12002">
            <td>2026-02-12</td>
            <td>Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation<br>æœ¬åœ°è§†è§‰è¯­è¨€æ¨¡å‹èƒ½å¦è¶…è¶Šè§†è§‰Transformeræå‡æ´»åŠ¨è¯†åˆ«èƒ½åŠ›ï¼Ÿâ€”â€”ä»¥æ–°ç”Ÿå„¿å¤è‹ä¸ºä¾‹çš„ç ”ç©¶<br><a href="abstracts/2602.12002.html">æ‘˜è¦</a></td>
            <td>Ã˜yvind Meinich-Bache Team</td>
            <td><a href="http://arxiv.org/abs/2602.12002">2602.12002</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.12002v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.11980">
            <td>2026-02-12</td>
            <td>Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation<br>ç©ºé—´æ€ç»´é“¾ï¼šè¿æ¥ç†è§£ä¸ç”Ÿæˆæ¨¡å‹ä»¥å®ç°ç©ºé—´æ¨ç†ç”Ÿæˆ<br><a href="abstracts/2602.11980.html">æ‘˜è¦</a></td>
            <td>Long Chen Team</td>
            <td><a href="http://arxiv.org/abs/2602.11980">2602.11980</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.11980v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.11960">
            <td>2026-02-12</td>
            <td>Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion<br>è¯„ä¼°è§†è§‰è¯­è¨€æ¨¡å‹åœ¨æ³•è¯­PDFè½¬Markdownä»»åŠ¡ä¸­çš„æ€§èƒ½åŸºå‡†<br><a href="abstracts/2602.11960.html">æ‘˜è¦</a></td>
            <td>Nicolas Mery Team</td>
            <td><a href="http://arxiv.org/abs/2602.11960">2602.11960</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.11960v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.11957">
            <td>2026-02-12</td>
            <td>Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization<br>åŒLLMæ˜¯å¦ä¼˜äºå•ä¸€æ¨¡å‹ï¼Ÿä¸€ç§ç”¨äºåŒ»è¯å†…å®¹ä¼˜åŒ–çš„å¸ˆç”ŸåŒå¤´LLMæ¶æ„<br><a href="abstracts/2602.11957.html">æ‘˜è¦</a></td>
            <td>Anubhav Girdhar Team</td>
            <td><a href="http://arxiv.org/abs/2602.11957">2602.11957</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.11957v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09850">
            <td>2026-02-10</td>
            <td>Reason-IAD: Knowledge-Guided Dynamic Latent Reasoning for Explainable Industrial Anomaly Detection<br>Reason-IADï¼šé¢å‘å¯è§£é‡Šå·¥ä¸šå¼‚å¸¸æ£€æµ‹çš„çŸ¥è¯†å¼•å¯¼åŠ¨æ€æ½œåœ¨æ¨ç†æ¡†æ¶<br><a href="abstracts/2602.09850.html">æ‘˜è¦</a></td>
            <td>Xiaochun Cao Team</td>
            <td><a href="http://arxiv.org/abs/2602.09850">2602.09850</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09850v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09843">
            <td>2026-02-10</td>
            <td>Kelix Technique Report<br>KelixæŠ€æœ¯æŠ¥å‘Š<br><a href="abstracts/2602.09843.html">æ‘˜è¦</a></td>
            <td>Ziqi Wang Team</td>
            <td><a href="http://arxiv.org/abs/2602.09843">2602.09843</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09843v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09825">
            <td>2026-02-10</td>
            <td>SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding<br>SAKEDï¼šé€šè¿‡ç¨³å®šæ€§æ„ŸçŸ¥çš„çŸ¥è¯†å¢å¼ºè§£ç ç¼“è§£å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜<br><a href="abstracts/2602.09825.html">æ‘˜è¦</a></td>
            <td>Xudong Jiang Team</td>
            <td><a href="http://arxiv.org/abs/2602.09825">2602.09825</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09825v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09701">
            <td>2026-02-10</td>
            <td>GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation<br>GenSeg-R1ï¼šåŸºäºå¼ºåŒ–å­¦ä¹ çš„è§†è§‰è¯­è¨€ç»†ç²’åº¦æŒ‡ä»£åˆ†å‰²<br><a href="abstracts/2602.09701.html">æ‘˜è¦</a></td>
            <td>Uma Mahesh Team</td>
            <td><a href="http://arxiv.org/abs/2602.09701">2602.09701</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09701v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09638">
            <td>2026-02-10</td>
            <td>VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model<br>VideoAffordï¼šåŸºäºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä»äºº-ç‰©äº¤äº’è§†é¢‘ä¸­å®ç°ä¸‰ç»´åŠŸèƒ½å¯åŠæ€§æ¥åœ°<br><a href="abstracts/2602.09638.html">æ‘˜è¦</a></td>
            <td>Hui Xiong Team</td>
            <td><a href="http://arxiv.org/abs/2602.09638">2602.09638</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09638v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09611">
            <td>2026-02-10</td>
            <td>AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models<br>AGMarkï¼šé¢å‘å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„æ³¨æ„åŠ›å¼•å¯¼åŠ¨æ€æ°´å°æŠ€æœ¯<br><a href="abstracts/2602.09611.html">æ‘˜è¦</a></td>
            <td>Linlin Wang Team</td>
            <td><a href="http://arxiv.org/abs/2602.09611">2602.09611</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09611v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09609">
            <td>2026-02-10</td>
            <td>Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing<br>Tele-Omniï¼šé¢å‘è§†é¢‘ç”Ÿæˆä¸ç¼–è¾‘çš„ç»Ÿä¸€å¤šæ¨¡æ€æ¡†æ¶<br><a href="abstracts/2602.09609.html">æ‘˜è¦</a></td>
            <td>Xuelong Li Team</td>
            <td><a href="http://arxiv.org/abs/2602.09609">2602.09609</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09609v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09586">
            <td>2026-02-10</td>
            <td>Delving into Spectral Clustering with Vision-Language Representations<br>æ¢ç´¢åŸºäºè§†è§‰-è¯­è¨€è¡¨å¾çš„å…‰è°±èšç±»æ–¹æ³•<br><a href="abstracts/2602.09586.html">æ‘˜è¦</a></td>
            <td>Zhen Fang Team</td>
            <td><a href="http://arxiv.org/abs/2602.09586">2602.09586</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09586v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09541">
            <td>2026-02-10</td>
            <td>Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination<br>æ‰‹æœ¯åˆ€ï¼šé€šè¿‡æ··åˆé«˜æ–¯æ¡¥ç²¾ç»†å¯¹é½æ³¨æ„åŠ›æ¿€æ´»æµå½¢ä»¥ç¼“è§£å¤šæ¨¡æ€å¹»è§‰<br><a href="abstracts/2602.09541.html">æ‘˜è¦</a></td>
            <td>Koichi Shirahata Team</td>
            <td><a href="http://arxiv.org/abs/2602.09541">2602.09541</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09541v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.09531">
            <td>2026-02-10</td>
            <td>DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment<br>DR.Expertsï¼šé¢å‘ç›²å›¾åƒè´¨é‡è¯„ä¼°çš„å¤±çœŸæ„ŸçŸ¥ä¸“å®¶å·®åˆ†ç»†åŒ–æ–¹æ³•<br><a href="abstracts/2602.09531.html">æ‘˜è¦</a></td>
            <td>Runze Hu Team</td>
            <td><a href="http://arxiv.org/abs/2602.09531">2602.09531</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.09531v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04864">
            <td>2026-02-04</td>
            <td>When LLaVA Meets Objects: Token Composition for Vision-Language-Models<br>å½“LLaVAé‡è§ç‰©ä½“ï¼šè§†è§‰è¯­è¨€æ¨¡å‹çš„ä»¤ç‰Œç»„åˆ<br><a href="abstracts/2602.04864.html">æ‘˜è¦</a></td>
            <td>Hilde Kuehne Team</td>
            <td><a href="http://arxiv.org/abs/2602.04864">2602.04864</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04864v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04849">
            <td>2026-02-04</td>
            <td>El Agente Estructural: An Artificially Intelligent Molecular Editor<br>ç»“æ„æ™ºèƒ½ä½“ï¼šä¸€ç§äººå·¥æ™ºèƒ½åˆ†å­ç¼–è¾‘å™¨<br><a href="abstracts/2602.04849.html">æ‘˜è¦</a></td>
            <td>Varinia Bernales Team</td>
            <td><a href="http://arxiv.org/abs/2602.04849">2602.04849</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04849v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04802">
            <td>2026-02-04</td>
            <td>VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?<br>VISTA-Benchï¼šè§†è§‰è¯­è¨€æ¨¡å‹çœŸçš„èƒ½åƒç†è§£çº¯æ–‡æœ¬ä¸€æ ·ç†è§£å›¾åƒä¸­çš„æ–‡æœ¬å—ï¼Ÿ<br><a href="abstracts/2602.04802.html">æ‘˜è¦</a></td>
            <td>Huchuan Lu Team</td>
            <td><a href="http://arxiv.org/abs/2602.04802">2602.04802</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04802v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04739">
            <td>2026-02-04</td>
            <td>Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases<br>å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„å¯¹é½æ¼‚ç§»ï¼šå¯¹å…«ä¸ªæ¨¡å‹ç‰ˆæœ¬æœ‰å®³æ€§çš„ä¸¤é˜¶æ®µçºµå‘è¯„ä¼°<br><a href="abstracts/2602.04739.html">æ‘˜è¦</a></td>
            <td>Emily Dix Team</td>
            <td><a href="http://arxiv.org/abs/2602.04739">2602.04739</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04739v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04712">
            <td>2026-02-04</td>
            <td>SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation<br>SAR-RAGï¼šé€šè¿‡è¯­ä¹‰æœç´¢ã€æ£€ç´¢ä¸å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„è‡ªåŠ¨ç›®æ ‡è¯†åˆ«è§†è§‰é—®ç­”<br><a href="abstracts/2602.04712.html">æ‘˜è¦</a></td>
            <td>Andreas Spanias Team</td>
            <td><a href="http://arxiv.org/abs/2602.04712">2602.04712</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04712v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04699">
            <td>2026-02-04</td>
            <td>Annotation Free Spacecraft Detection and Segmentation using Vision Language Models<br>åŸºäºè§†è§‰è¯­è¨€æ¨¡å‹çš„æ— æ ‡æ³¨èˆªå¤©å™¨æ£€æµ‹ä¸åˆ†å‰²<br><a href="abstracts/2602.04699.html">æ‘˜è¦</a></td>
            <td>Djamila Aouada Team</td>
            <td><a href="http://arxiv.org/abs/2602.04699">2602.04699</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04699v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04672">
            <td>2026-02-04</td>
            <td>AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation<br>AGILEï¼šåŸºäºæ™ºèƒ½ä½“ç”Ÿæˆä»è§†é¢‘é‡å»ºæ‰‹-ç‰©äº¤äº’<br><a href="abstracts/2602.04672.html">æ‘˜è¦</a></td>
            <td>Chunhua Shen Team</td>
            <td><a href="http://arxiv.org/abs/2602.04672">2602.04672</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04672v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04657">
            <td>2026-02-04</td>
            <td>PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective<br>PIO-FVLMï¼šä»æ¨ç†ç›®æ ‡è§†è§’é‡æ–°å®¡è§†ç”¨äºVLMåŠ é€Ÿçš„æ— è®­ç»ƒè§†è§‰ä»¤ç‰Œç¼©å‡<br><a href="abstracts/2602.04657.html">æ‘˜è¦</a></td>
            <td>Chunhua Shen Team</td>
            <td><a href="http://arxiv.org/abs/2602.04657">2602.04657</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04657v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04635">
            <td>2026-02-04</td>
            <td>Relational Scene Graphs for Object Grounding of Natural Language Commands<br>é¢å‘è‡ªç„¶è¯­è¨€æŒ‡ä»¤ä¸­ç‰©ä½“å®šä½çš„å…³ç³»åœºæ™¯å›¾<br><a href="abstracts/2602.04635.html">æ‘˜è¦</a></td>
            <td>Ville Kyrki Team</td>
            <td><a href="http://arxiv.org/abs/2602.04635">2602.04635</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04635v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.04617">
            <td>2026-02-04</td>
            <td>LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation<br>LEADï¼šé¢å‘å¿ å®æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„å±‚çº§ä¸“å®¶å¯¹é½è§£ç <br><a href="abstracts/2602.04617.html">æ‘˜è¦</a></td>
            <td>Yan Song Team</td>
            <td><a href="http://arxiv.org/abs/2602.04617">2602.04617</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.04617v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02468">
            <td>2026-02-02</td>
            <td>Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts<br>Avenir-Webï¼šåŸºäºæ··åˆå®šä½ä¸“å®¶çš„äººç±»ç»éªŒæ¨¡ä»¿å¼å¤šæ¨¡æ€ç½‘ç»œä»£ç†<br><a href="abstracts/2602.02468.html">æ‘˜è¦</a></td>
            <td>Mengdi Wang Team</td>
            <td><a href="http://arxiv.org/abs/2602.02468">2602.02468</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02468v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02465">
            <td>2026-02-02</td>
            <td>MentisOculi: Revealing the Limits of Reasoning with Mental Imagery<br>MentisOculiï¼šæ­ç¤ºå¿ƒæ™ºæ„è±¡æ¨ç†çš„å±€é™æ€§<br><a href="abstracts/2602.02465.html">æ‘˜è¦</a></td>
            <td>Wieland Brendel Team</td>
            <td><a href="http://arxiv.org/abs/2602.02465">2602.02465</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02465v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02456">
            <td>2026-02-02</td>
            <td>Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning<br>é¢å‘ä»»åŠ¡æ¨ç†çš„å…³ç³»æ„ŸçŸ¥åˆ†å±‚ä¸‰ç»´åœºæ™¯å›¾<br><a href="abstracts/2602.02456.html">æ‘˜è¦</a></td>
            <td>Kostas Alexis Team</td>
            <td><a href="http://arxiv.org/abs/2602.02456">2602.02456</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02456v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02454">
            <td>2026-02-02</td>
            <td>World-Gymnast: Training Robots with Reinforcement Learning in a World Model<br>ä¸–ç•Œä½“æ“å®¶ï¼šåœ¨ä¸–ç•Œæ¨¡å‹ä¸­é€šè¿‡å¼ºåŒ–å­¦ä¹ è®­ç»ƒæœºå™¨äºº<br><a href="abstracts/2602.02454.html">æ‘˜è¦</a></td>
            <td>Sherry Yang Team</td>
            <td><a href="http://arxiv.org/abs/2602.02454">2602.02454</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02454v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02408">
            <td>2026-02-02</td>
            <td>ReasonEdit: Editing Vision-Language Models using Human Reasoning<br>ReasonEditï¼šåŸºäºäººç±»æ¨ç†çš„è§†è§‰è¯­è¨€æ¨¡å‹ç¼–è¾‘<br><a href="abstracts/2602.02408.html">æ‘˜è¦</a></td>
            <td>Thomas Hartvigsen Team</td>
            <td><a href="http://arxiv.org/abs/2602.02408">2602.02408</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02408v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02341">
            <td>2026-02-02</td>
            <td>LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization<br>LongVPOï¼šä»é”šå®šçº¿ç´¢åˆ°è‡ªæˆ‘æ¨ç†çš„é•¿è§†é¢‘åå¥½ä¼˜åŒ–<br><a href="abstracts/2602.02341.html">æ‘˜è¦</a></td>
            <td>Limin Wang Team</td>
            <td><a href="http://arxiv.org/abs/2602.02341">2602.02341</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02341v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02185">
            <td>2026-02-02</td>
            <td>Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models<br>Vision-DeepResearchåŸºå‡†ï¼šé‡æ–°æ€è€ƒå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹çš„è§†è§‰ä¸æ–‡æœ¬æœç´¢èƒ½åŠ›<br><a href="abstracts/2602.02185.html">æ‘˜è¦</a></td>
            <td>Shaosheng Cao Team</td>
            <td><a href="http://arxiv.org/abs/2602.02185">2602.02185</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02185v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02063">
            <td>2026-02-02</td>
            <td>See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers<br>See2Refineï¼šè§†è§‰-è¯­è¨€åé¦ˆæå‡åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„eHMIè¡Œä¸ºè®¾è®¡èƒ½åŠ›<br><a href="abstracts/2602.02063.html">æ‘˜è¦</a></td>
            <td>Takeo Igarashi Team</td>
            <td><a href="http://arxiv.org/abs/2602.02063">2602.02063</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02063v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02043">
            <td>2026-02-02</td>
            <td>Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models<br>Auto-Compï¼šé¢å‘å¯¹æ¯”å¼è§†è§‰è¯­è¨€æ¨¡å‹å¯æ‰©å±•ç»„åˆæ€§æ¢æµ‹çš„è‡ªåŠ¨åŒ–æµç¨‹<br><a href="abstracts/2602.02043.html">æ‘˜è¦</a></td>
            <td>Toshihiko Yamasaki Team</td>
            <td><a href="http://arxiv.org/abs/2602.02043">2602.02043</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02043v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2602.02033">
            <td>2026-02-02</td>
            <td>One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation<br>ä¸€å›¾å¤šé…ï¼šåœ¨å¤§è§„æ¨¡å¹¿å‘Šå›¾åƒç”Ÿæˆä¸­åè°ƒå¤šæ ·åŒ–çš„ç¾¤ä½“ç‚¹å‡»åå¥½<br><a href="abstracts/2602.02033.html">æ‘˜è¦</a></td>
            <td>Jian Liang Team</td>
            <td><a href="http://arxiv.org/abs/2602.02033">2602.02033</a></td>
            <td><a href="https://hjfy.top/arxiv/2602.02033v1">HJFY</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.23281">
            <td>2026-01-30</td>
            <td>User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments</td>
            <td>Junfeng Lin et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.23281">2601.23281</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.23253">
            <td>2026-01-30</td>
            <td>Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models</td>
            <td>Yi Zhang et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.23253">2601.23253</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.23251">
            <td>2026-01-30</td>
            <td>Structured Over Scale: Learning Spatial Reasoning from Educational Video</td>
            <td>Bishoy Galoaa et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.23251">2601.23251</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.23224">
            <td>2026-01-30</td>
            <td>Video-o3: Native Interleaved Clue Seeking for Long Video Multi-Hop Reasoning</td>
            <td>Xiangyu Zeng et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.23224">2601.23224</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.23220">
            <td>2026-01-30</td>
            <td>Med-Scout: Curing MLLMs&#x27; Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training</td>
            <td>Anglin Liu et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.23220">2601.23220</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.23179">
            <td>2026-01-30</td>
            <td>Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization</td>
            <td>Hui Lu et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.23179">2601.23179</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.23149">
            <td>2026-01-30</td>
            <td>Hearing is Believing? Evaluating and Analyzing Audio Language Model Sycophancy with SYAUDIO</td>
            <td>Junchi Yao et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.23149">2601.23149</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.23041">
            <td>2026-01-30</td>
            <td>One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs</td>
            <td>Youxu Shi et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.23041">2601.23041</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22959">
            <td>2026-01-30</td>
            <td>Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models</td>
            <td>Anmin Wang et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22959">2601.22959</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22948">
            <td>2026-01-30</td>
            <td>Alignment among Language, Vision and Action Representations</td>
            <td>Nicola Milano et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22948">2601.22948</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22155">
            <td>2026-01-29</td>
            <td>UEval: A Benchmark for Unified Multimodal Generation</td>
            <td>Bo Li et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22155">2601.22155</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22150">
            <td>2026-01-29</td>
            <td>Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions</td>
            <td>Xiaoxiao Sun et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22150">2601.22150</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22114">
            <td>2026-01-29</td>
            <td>SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence</td>
            <td>Saoud Aldowaish et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22114">2601.22114</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22069">
            <td>2026-01-29</td>
            <td>VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning</td>
            <td>Yibo Wang et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22069">2601.22069</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22060">
            <td>2026-01-29</td>
            <td>Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models</td>
            <td>Wenxuan Huang et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22060">2601.22060</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22054">
            <td>2026-01-29</td>
            <td>MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources</td>
            <td>Baorui Ma et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22054">2601.22054</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.22020">
            <td>2026-01-29</td>
            <td>Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning</td>
            <td>Chengyi Cai et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.22020">2601.22020</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.21998">
            <td>2026-01-29</td>
            <td>Causal World Modeling for Robot Control</td>
            <td>Lin Li et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.21998">2601.21998</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.21944">
            <td>2026-01-29</td>
            <td>Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models</td>
            <td>Konstantinos P. Panousis et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.21944">2601.21944</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
          <tr data-arxiv-id="2601.21915">
            <td>2026-01-29</td>
            <td>VideoAesBench: Benchmarking the Video Aesthetics Perception Capabilities of Large Multimodal Models</td>
            <td>Yunhao Li et.al.</td>
            <td><a href="http://arxiv.org/abs/2601.21915">2601.21915</a></td>
            <td><a href="">null</a></td>
            <td>
              <div class="eval">
                <button type="button" data-value="read">âœ…</button>
                <button type="button" data-value="skip">âŒ</button>
                <button type="button" data-value="star">â­</button>
              </div>
            </td>
          </tr>
        </tbody>
      </table>
      <p class="legend">è¯„ä¼°çŠ¶æ€ä¿å­˜åœ¨æµè§ˆå™¨æœ¬åœ°ï¼ˆlocalStorageï¼‰ï¼Œæ¢è®¾å¤‡/æµè§ˆå™¨ä¸ä¼šåŒæ­¥ã€‚</p>
    </section>
  </div>
  <script>
    const storageKey = (id) => `vlm_arxiv_daily_eval:${id}`;
    const rows = document.querySelectorAll('tr[data-arxiv-id]');
    rows.forEach((row) => {
      const id = row.getAttribute('data-arxiv-id');
      const buttons = row.querySelectorAll('button[data-value]');
      const saved = localStorage.getItem(storageKey(id));
      if (saved) {
        buttons.forEach((btn) => {
          if (btn.dataset.value === saved) btn.classList.add('active');
        });
      }
      buttons.forEach((btn) => {
        btn.addEventListener('click', () => {
          const val = btn.dataset.value;
          localStorage.setItem(storageKey(id), val);
          buttons.forEach((b) => b.classList.remove('active'));
          btn.classList.add('active');
        });
      });
    });
  </script>
</body>
</html>
