{"VLA": {"2601.22153": "|**2026-01-29**|**DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation**|Haozhe Xie et.al.|[2601.22153](http://arxiv.org/abs/2601.22153)|null|\n", "2601.22046": "|**2026-01-29**|**PLANING: A Loosely Coupled Triangle-Gaussian Framework for Streaming 3D Reconstruction**|Changjian Jiang et.al.|[2601.22046](http://arxiv.org/abs/2601.22046)|null|\n", "2601.22018": "|**2026-01-29**|**PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy**|Jinhao Zhang et.al.|[2601.22018](http://arxiv.org/abs/2601.22018)|null|\n", "2601.21998": "|**2026-01-29**|**Causal World Modeling for Robot Control**|Lin Li et.al.|[2601.21998](http://arxiv.org/abs/2601.21998)|null|\n", "2601.21971": "|**2026-01-29**|**MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts**|Lorenzo Mazza et.al.|[2601.21971](http://arxiv.org/abs/2601.21971)|null|\n", "2601.21926": "|**2026-01-29**|**Information Filtering via Variational Regularization for Robot Manipulation**|Jinhao Zhang et.al.|[2601.21926](http://arxiv.org/abs/2601.21926)|null|\n", "2601.21751": "|**2026-01-29**|**Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation**|Jiankun Peng et.al.|[2601.21751](http://arxiv.org/abs/2601.21751)|null|\n", "2601.21712": "|**2026-01-29**|**CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation**|Xuanran Zhai et.al.|[2601.21712](http://arxiv.org/abs/2601.21712)|null|\n", "2601.21602": "|**2026-01-29**|**AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation**|Jianli Sun et.al.|[2601.21602](http://arxiv.org/abs/2601.21602)|null|\n", "2601.21570": "|**2026-01-29**|**EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots**|Zixing Lei et.al.|[2601.21570](http://arxiv.org/abs/2601.21570)|null|\n", "2601.23087": "|**2026-01-30**|**Temporally Coherent Imitation Learning via Latent Action Flow Matching for Robotic Manipulation**|Wu Songwei et.al.|[2601.23087](http://arxiv.org/abs/2601.23087)|null|\n", "2601.23065": "|**2026-01-30**|**EAG-PT: Emission-Aware Gaussians and Path Tracing for Indoor Scene Reconstruction and Editing**|Xijie Yang et.al.|[2601.23065](http://arxiv.org/abs/2601.23065)|null|\n", "2601.22988": "|**2026-01-30**|**Learning Geometrically-Grounded 3D Visual Representations for View-Generalizable Robotic Manipulation**|Di Zhang et.al.|[2601.22988](http://arxiv.org/abs/2601.22988)|null|\n", "2601.22948": "|**2026-01-30**|**Alignment among Language, Vision and Action Representations**|Nicola Milano et.al.|[2601.22948](http://arxiv.org/abs/2601.22948)|null|\n", "2601.22868": "|**2026-01-30**|**When Anomalies Depend on Context: Learning Conditional Compatibility for Anomaly Detection**|Shashank Mishra et.al.|[2601.22868](http://arxiv.org/abs/2601.22868)|null|\n", "2601.22714": "|**2026-01-30**|**Vision-Language Models Unlock Task-Centric Latent Actions**|Alexander Nikulin et.al.|[2601.22714](http://arxiv.org/abs/2601.22714)|null|\n", "2601.22701": "|**2026-01-30**|**Best-of-Q: Improving VLM agents with Q-function Action Ranking at Inference**|Emilien Bir\u00e9 et.al.|[2601.22701](http://arxiv.org/abs/2601.22701)|null|\n", "2601.22467": "|**2026-01-30**|**CARE: Multi-Task Pretraining for Latent Continuous Action Representation in Robot Control**|Jiaqi Shi et.al.|[2601.22467](http://arxiv.org/abs/2601.22467)|null|\n", "2601.22356": "|**2026-01-29**|**PoSafeNet: Safe Learning with Poset-Structured Neural Nets**|Kiwan Wong et.al.|[2601.22356](http://arxiv.org/abs/2601.22356)|null|\n", "2602.02459": "|**2026-02-02**|**TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments<br>TIC-VLA\uff1a\u4e00\u79cd\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5bfc\u822a\u7684\u601d\u63a7\u4e00\u4f53\u5316\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b<br>[\u6458\u8981](abstracts/2602.02459.html)**|Jiaqi Ma Team|[2602.02459](http://arxiv.org/abs/2602.02459)|[HJFY](https://hjfy.top/arxiv/2602.02459v1)|\n", "2602.02454": "|**2026-02-02**|**World-Gymnast: Training Robots with Reinforcement Learning in a World Model<br>\u4e16\u754c\u4f53\u64cd\u5bb6\uff1a\u5728\u4e16\u754c\u6a21\u578b\u4e2d\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u673a\u5668\u4eba<br>[\u6458\u8981](abstracts/2602.02454.html)**|Sherry Yang Team|[2602.02454](http://arxiv.org/abs/2602.02454)|[HJFY](https://hjfy.top/arxiv/2602.02454v1)|\n", "2602.02402": "|**2026-02-02**|**SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation<br>SoMA\uff1a\u9762\u5411\u673a\u5668\u4eba\u8f6f\u4f53\u64cd\u4f5c\u7684\u771f\u5b9e\u5230\u4eff\u771f\u795e\u7ecf\u6a21\u62df\u5668<br>[\u6458\u8981](abstracts/2602.02402.html)**|Jiangmiao Pang Team|[2602.02402](http://arxiv.org/abs/2602.02402)|[HJFY](https://hjfy.top/arxiv/2602.02402v1)|\n", "2602.02212": "|**2026-02-02**|**MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models<br>MAIN-VLA\uff1a\u4e3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5efa\u6a21\u610f\u56fe\u4e0e\u73af\u5883\u7684\u62bd\u8c61<br>[\u6458\u8981](abstracts/2602.02212.html)**|Lemiao Qiu Team|[2602.02212](http://arxiv.org/abs/2602.02212)|[HJFY](https://hjfy.top/arxiv/2602.02212v1)|\n", "2602.02142": "|**2026-02-02**|**FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation<br>FD-VLA\uff1a\u7528\u4e8e\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u7684\u529b\u84b8\u998f\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b<br>[\u6458\u8981](abstracts/2602.02142.html)**|Haiyue Zhu Team|[2602.02142](http://arxiv.org/abs/2602.02142)|[HJFY](https://hjfy.top/arxiv/2602.02142v1)|\n", "2602.02063": "|**2026-02-02**|**See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers<br>See2Refine\uff1a\u89c6\u89c9-\u8bed\u8a00\u53cd\u9988\u63d0\u5347\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684eHMI\u884c\u4e3a\u8bbe\u8ba1\u80fd\u529b<br>[\u6458\u8981](abstracts/2602.02063.html)**|Takeo Igarashi Team|[2602.02063](http://arxiv.org/abs/2602.02063)|[HJFY](https://hjfy.top/arxiv/2602.02063v1)|\n", "2602.01834": "|**2026-02-02**|**Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models<br>\u9762\u5411\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u63a8\u7406\u65f6\u5b89\u5168\u6027\u7684\u6982\u5ff5\u8bcd\u5178\u5b66\u4e60\u65b9\u6cd5<br>[\u6458\u8981](abstracts/2602.01834.html)**|Di Wang Team|[2602.01834](http://arxiv.org/abs/2602.01834)|[HJFY](https://hjfy.top/arxiv/2602.01834v1)|\n", "2602.01811": "|**2026-02-02**|**From Knowing to Doing Precisely: A General Self-Correction and Termination Framework for VLA models<br>\u4ece\u7cbe\u786e\u8ba4\u77e5\u5230\u7cbe\u51c6\u6267\u884c\uff1a\u9762\u5411\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u7684\u901a\u7528\u81ea\u6821\u6b63\u4e0e\u7ec8\u6b62\u6846\u67b6<br>[\u6458\u8981](abstracts/2602.01811.html)**|Jianzong Wang Team|[2602.01811](http://arxiv.org/abs/2602.01811)|[HJFY](https://hjfy.top/arxiv/2602.01811v1)|\n", "2602.01662": "|**2026-02-02**|**AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act<br>AgenticLab\uff1a\u4e00\u4e2a\u80fd\u591f\u89c2\u5bdf\u3001\u601d\u8003\u4e0e\u884c\u52a8\u7684\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u667a\u80fd\u4f53\u5e73\u53f0<br>[\u6458\u8981](abstracts/2602.01662.html)**|Yu She Team|[2602.01662](http://arxiv.org/abs/2602.01662)|[HJFY](https://hjfy.top/arxiv/2602.01662v1)|\n", "2602.01644": "|**2026-02-02**|**From Perception to Action: Spatial AI Agents and World Models<br>\u4ece\u611f\u77e5\u5230\u884c\u52a8\uff1a\u7a7a\u95f4\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u4e0e\u4e16\u754c\u6a21\u578b<br>[\u6458\u8981](abstracts/2602.01644.html)**|Esteban Rojas Team|[2602.01644](http://arxiv.org/abs/2602.01644)|[HJFY](https://hjfy.top/arxiv/2602.01644v1)|\n", "2602.04880": "|**2026-02-04**|**Capturing Visual Environment Structure Correlates with Control Performance<br>\u6355\u6349\u89c6\u89c9\u73af\u5883\u7ed3\u6784\u4e0e\u63a7\u5236\u6027\u80fd\u7684\u76f8\u5173\u6027<br>[\u6458\u8981](abstracts/2602.04880.html)**|Yu-Xiong Wang Team|[2602.04880](http://arxiv.org/abs/2602.04880)|[HJFY](https://hjfy.top/arxiv/2602.04880v1)|\n", "2602.04877": "|**2026-02-04**|**CoWTracker: Tracking by Warping instead of Correlation<br>CoWTracker\uff1a\u901a\u8fc7\u53d8\u5f62\u800c\u975e\u76f8\u5173\u6027\u8fdb\u884c\u8ddf\u8e2a<br>[\u6458\u8981](abstracts/2602.04877.html)**|Andrea Vedaldi Team|[2602.04877](http://arxiv.org/abs/2602.04877)|[HJFY](https://hjfy.top/arxiv/2602.04877v1)|\n", "2602.04635": "|**2026-02-04**|**Relational Scene Graphs for Object Grounding of Natural Language Commands<br>\u9762\u5411\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e2d\u7269\u4f53\u5b9a\u4f4d\u7684\u5173\u7cfb\u573a\u666f\u56fe<br>[\u6458\u8981](abstracts/2602.04635.html)**|Ville Kyrki Team|[2602.04635](http://arxiv.org/abs/2602.04635)|[HJFY](https://hjfy.top/arxiv/2602.04635v1)|\n", "2602.04600": "|**2026-02-04**|**Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data<br>\u884c\u52a8\u3001\u611f\u77e5\u3001\u518d\u884c\u52a8\uff1a\u4ece\u5927\u89c4\u6a21\u7b2c\u4e00\u4eba\u79f0\u4eba\u7c7b\u6570\u636e\u4e2d\u5b66\u4e60\u975e\u9a6c\u5c14\u53ef\u592b\u4e3b\u52a8\u611f\u77e5\u7b56\u7565<br>[\u6458\u8981](abstracts/2602.04600.html)**|Wenzhao Lian Team|[2602.04600](http://arxiv.org/abs/2602.04600)|[HJFY](https://hjfy.top/arxiv/2602.04600v1)|\n", "2602.04522": "|**2026-02-04**|**A Unified Complementarity-based Approach for Rigid-Body Manipulation and Motion Prediction<br>\u57fa\u4e8e\u4e92\u8865\u6027\u7684\u7edf\u4e00\u65b9\u6cd5\u5728\u521a\u4f53\u64cd\u4f5c\u4e0e\u8fd0\u52a8\u9884\u6d4b\u4e2d\u7684\u5e94\u7528<br>[\u6458\u8981](abstracts/2602.04522.html)**|Riddhiman Laha Team|[2602.04522](http://arxiv.org/abs/2602.04522)|[HJFY](https://hjfy.top/arxiv/2602.04522v1)|\n", "2602.04515": "|**2026-02-04**|**EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models<br>EgoActor\uff1a\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06\u4efb\u52a1\u89c4\u5212\u843d\u5730\u4e3a\u5177\u8eab\u673a\u5668\u4eba\u7684\u7a7a\u95f4\u611f\u77e5\u81ea\u6211\u4e2d\u5fc3\u52a8\u4f5c<br>[\u6458\u8981](abstracts/2602.04515.html)**|B\u00f6rje F. Karlsson Team|[2602.04515](http://arxiv.org/abs/2602.04515)|[HJFY](https://hjfy.top/arxiv/2602.04515v1)|\n", "2602.04411": "|**2026-02-04**|**Self-evolving Embodied AI<br>\u81ea\u6f14\u5316\u7684\u5177\u8eab\u4eba\u5de5\u667a\u80fd<br>[\u6458\u8981](abstracts/2602.04411.html)**|Wenwu Zhu Team|[2602.04411](http://arxiv.org/abs/2602.04411)|[HJFY](https://hjfy.top/arxiv/2602.04411v1)|\n", "2602.04315": "|**2026-02-04**|**GeneralVLA: Generalizable Vision-Language-Action Models with Knowledge-Guided Trajectory Planning<br>GeneralVLA\uff1a\u5177\u5907\u77e5\u8bc6\u5f15\u5bfc\u8f68\u8ff9\u89c4\u5212\u7684\u901a\u7528\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b<br>[\u6458\u8981](abstracts/2602.04315.html)**|Hao Tang Team|[2602.04315](http://arxiv.org/abs/2602.04315)|[HJFY](https://hjfy.top/arxiv/2602.04315v1)|\n", "2602.04243": "|**2026-02-04**|**Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation<br>\u89c6\u89d2\u81f3\u5173\u91cd\u8981\uff1a\u5229\u7528\u63a9\u7801\u81ea\u7f16\u7801\u5668\u52a8\u6001\u4f18\u5316\u89c6\u89c9\u64cd\u63a7\u7684\u89c6\u89d2<br>[\u6458\u8981](abstracts/2602.04243.html)**|Wenzhao Lian Team|[2602.04243](http://arxiv.org/abs/2602.04243)|[HJFY](https://hjfy.top/arxiv/2602.04243v1)|\n", "2602.04231": "|**2026-02-04**|**GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning<br>GeoLanG\uff1a\u57fa\u4e8e\u7edf\u4e00RGB-D\u591a\u6a21\u6001\u5b66\u4e60\u7684\u51e0\u4f55\u611f\u77e5\u8bed\u8a00\u5f15\u5bfc\u6293\u53d6<br>[\u6458\u8981](abstracts/2602.04231.html)**|Hongliang Ren Team|[2602.04231](http://arxiv.org/abs/2602.04231)|[HJFY](https://hjfy.top/arxiv/2602.04231v1)|\n", "2602.09878": "|**2026-02-10**|**MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation<br>MVISTA-4D\uff1a\u5177\u6709\u6d4b\u8bd5\u65f6\u52a8\u4f5c\u63a8\u7406\u80fd\u529b\u7684\u89c6\u56fe\u4e00\u81f44D\u4e16\u754c\u6a21\u578b\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c<br>[\u6458\u8981](abstracts/2602.09878.html)**|Xiangyu Yue Team|[2602.09878](http://arxiv.org/abs/2602.09878)|[HJFY](https://hjfy.top/arxiv/2602.09878v1)|\n", "2602.09856": "|**2026-02-10**|**Code2World: A GUI World Model via Renderable Code Generation<br>Code2World\uff1a\u901a\u8fc7\u53ef\u6e32\u67d3\u4ee3\u7801\u751f\u6210\u7684GUI\u4e16\u754c\u6a21\u578b<br>[\u6458\u8981](abstracts/2602.09856.html)**|Kevin Qinghong Lin Team|[2602.09856](http://arxiv.org/abs/2602.09856)|[HJFY](https://hjfy.top/arxiv/2602.09856v1)|\n", "2602.09849": "|**2026-02-10**|**BagelVLA: Enhancing Long-Horizon Manipulation via Interleaved Vision-Language-Action Generation<br>BagelVLA\uff1a\u901a\u8fc7\u4ea4\u9519\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u751f\u6210\u589e\u5f3a\u957f\u65f6\u7a0b\u64cd\u4f5c\u80fd\u529b<br>[\u6458\u8981](abstracts/2602.09849.html)**|Jianyu Chen Team|[2602.09849](http://arxiv.org/abs/2602.09849)|[HJFY](https://hjfy.top/arxiv/2602.09849v1)|\n", "2602.09765": "|**2026-02-10**|**NavDreamer: Video Models as Zero-Shot 3D Navigators<br>NavDreamer\uff1a\u89c6\u9891\u6a21\u578b\u4f5c\u4e3a\u96f6\u6837\u672c\u4e09\u7ef4\u5bfc\u822a\u5668<br>[\u6458\u8981](abstracts/2602.09765.html)**|Fei Gao Team|[2602.09765](http://arxiv.org/abs/2602.09765)|[HJFY](https://hjfy.top/arxiv/2602.09765v1)|\n", "2602.09722": "|**2026-02-10**|**Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization<br>\u91cd\u65b0\u5ba1\u89c6\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u89c4\u6a21\u5316\uff1a\u5bf9\u9f50\u3001\u6df7\u5408\u4e0e\u6b63\u5219\u5316<br>[\u6458\u8981](abstracts/2602.09722.html)**|Qin Jin Team|[2602.09722](http://arxiv.org/abs/2602.09722)|[HJFY](https://hjfy.top/arxiv/2602.09722v1)|\n", "2602.09657": "|**2026-02-10**|**AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild<br>AutoFly\uff1a\u9762\u5411\u91ce\u5916\u65e0\u4eba\u673a\u81ea\u4e3b\u5bfc\u822a\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b<br>[\u6458\u8981](abstracts/2602.09657.html)**|Hui Xiong Team|[2602.09657](http://arxiv.org/abs/2602.09657)|[HJFY](https://hjfy.top/arxiv/2602.09657v1)|\n", "2602.09638": "|**2026-02-10**|**VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model<br>VideoAfford\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u4eba-\u7269\u4ea4\u4e92\u89c6\u9891\u4e2d\u5b9e\u73b0\u4e09\u7ef4\u529f\u80fd\u53ef\u53ca\u6027\u63a5\u5730<br>[\u6458\u8981](abstracts/2602.09638.html)**|Hui Xiong Team|[2602.09638](http://arxiv.org/abs/2602.09638)|[HJFY](https://hjfy.top/arxiv/2602.09638v1)|\n", "2602.09600": "|**2026-02-10**|**Hand2World: Autoregressive Egocentric Interaction Generation via Free-Space Hand Gestures<br>Hand2World\uff1a\u57fa\u4e8e\u81ea\u7531\u7a7a\u95f4\u624b\u52bf\u7684\u81ea\u56de\u5f52\u7b2c\u4e00\u4eba\u79f0\u4ea4\u4e92\u751f\u6210<br>[\u6458\u8981](abstracts/2602.09600.html)**|Xingang Pan Team|[2602.09600](http://arxiv.org/abs/2602.09600)|[HJFY](https://hjfy.top/arxiv/2602.09600v1)|\n", "2602.09583": "|**2026-02-10**|**Preference Aligned Visuomotor Diffusion Policies for Deformable Object Manipulation<br>\u9762\u5411\u53ef\u53d8\u5f62\u7269\u4f53\u64cd\u4f5c\u7684\u504f\u597d\u5bf9\u9f50\u89c6\u89c9\u8fd0\u52a8\u6269\u6563\u7b56\u7565<br>[\u6458\u8981](abstracts/2602.09583.html)**|Danica Kragic Team|[2602.09583](http://arxiv.org/abs/2602.09583)|[HJFY](https://hjfy.top/arxiv/2602.09583v1)|\n", "2602.09534": "|**2026-02-10**|**AUHead: Realistic Emotional Talking Head Generation via Action Units Control<br>AUHead\uff1a\u57fa\u4e8e\u52a8\u4f5c\u5355\u5143\u63a7\u5236\u7684\u903c\u771f\u60c5\u611f\u8bf4\u8bdd\u5934\u90e8\u751f\u6210<br>[\u6458\u8981](abstracts/2602.09534.html)**|Tat-Seng Chua Team|[2602.09534](http://arxiv.org/abs/2602.09534)|[HJFY](https://hjfy.top/arxiv/2602.09534v1)|\n", "2602.12281": "|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment<br>\u6269\u5c55\u9a8c\u8bc1\u5728\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u5bf9\u9f50\u4e2d\u6bd4\u6269\u5c55\u7b56\u7565\u5b66\u4e60\u66f4\u6709\u6548<br>[\u6458\u8981](abstracts/2602.12281.html)**|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|[HJFY](https://hjfy.top/arxiv/2602.12281v1)|\n", "2602.12136": "|**2026-02-12**|**Embodied AI Agents for Team Collaboration in Co-located Blue-Collar Work<br>\u9762\u5411\u5171\u5740\u84dd\u9886\u5de5\u4f5c\u56e2\u961f\u534f\u4f5c\u7684\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u4f53<br>[\u6458\u8981](abstracts/2602.12136.html)**|Thomas Olsson Team|[2602.12136](http://arxiv.org/abs/2602.12136)|[HJFY](https://hjfy.top/arxiv/2602.12136v1)|\n", "2602.12099": "|**2026-02-12**|**GigaBrain-0.5M*: a VLA That Learns From World Model-Based Reinforcement Learning<br>GigaBrain-0.5M*\uff1a\u4e00\u79cd\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b<br>[\u6458\u8981](abstracts/2602.12099.html)**|Zheng Zhu Team|[2602.12099](http://arxiv.org/abs/2602.12099)|[HJFY](https://hjfy.top/arxiv/2602.12099v1)|\n", "2602.12063": "|**2026-02-12**|**VLAW: Iterative Co-Improvement of Vision-Language-Action Policy and World Model<br>VLAW\uff1a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7b56\u7565\u4e0e\u4e16\u754c\u6a21\u578b\u7684\u8fed\u4ee3\u534f\u540c\u6539\u8fdb<br>[\u6458\u8981](abstracts/2602.12063.html)**|Chelsea Finn Team|[2602.12063](http://arxiv.org/abs/2602.12063)|[HJFY](https://hjfy.top/arxiv/2602.12063v1)|\n", "2602.12062": "|**2026-02-12**|**HoloBrain-0 Technical Report<br>HoloBrain-0\u6280\u672f\u62a5\u544a<br>[\u6458\u8981](abstracts/2602.12062.html)**|Zhizhong Su Team|[2602.12062](http://arxiv.org/abs/2602.12062)|[HJFY](https://hjfy.top/arxiv/2602.12062v1)|\n", "2602.12032": "|**2026-02-12**|**When would Vision-Proprioception Policies Fail in Robotic Manipulation?<br>\u89c6\u89c9-\u672c\u4f53\u611f\u77e5\u7b56\u7565\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u4f55\u65f6\u4f1a\u5931\u6548\uff1f<br>[\u6458\u8981](abstracts/2602.12032.html)**|Di Hu Team|[2602.12032](http://arxiv.org/abs/2602.12032)|[HJFY](https://hjfy.top/arxiv/2602.12032v1)|\n", "2602.11934": "|**2026-02-12**|**Robot-DIFT: Distilling Diffusion Features for Geometrically Consistent Visuomotor Control<br>Robot-DIFT\uff1a\u63d0\u53d6\u6269\u6563\u7279\u5f81\u4ee5\u5b9e\u73b0\u51e0\u4f55\u4e00\u81f4\u7684\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236<br>[\u6458\u8981](abstracts/2602.11934.html)**|Georgia Chalvatzaki Team|[2602.11934](http://arxiv.org/abs/2602.11934)|[HJFY](https://hjfy.top/arxiv/2602.11934v1)|\n", "2602.11832": "|**2026-02-12**|**JEPA-VLA: Video Predictive Embedding is Needed for VLA Models<br>JEPA-VLA\uff1a\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u9700\u8981\u89c6\u9891\u9884\u6d4b\u6027\u5d4c\u5165<br>[\u6458\u8981](abstracts/2602.11832.html)**|Mingsheng Long Team|[2602.11832](http://arxiv.org/abs/2602.11832)|[HJFY](https://hjfy.top/arxiv/2602.11832v1)|\n", "2602.11660": "|**2026-02-12**|**Clutt3R-Seg: Sparse-view 3D Instance Segmentation for Language-grounded Grasping in Cluttered Scenes<br>Clutt3R-Seg\uff1a\u9762\u5411\u6742\u4e71\u573a\u666f\u4e2d\u8bed\u8a00\u9a71\u52a8\u6293\u53d6\u7684\u7a00\u758f\u89c6\u89d2\u4e09\u7ef4\u5b9e\u4f8b\u5206\u5272<br>[\u6458\u8981](abstracts/2602.11660.html)**|Ayoung Kim Team|[2602.11660](http://arxiv.org/abs/2602.11660)|[HJFY](https://hjfy.top/arxiv/2602.11660v1)|\n", "2602.11643": "|**2026-02-12**|**ViTaS: Visual Tactile Soft Fusion Contrastive Learning for Visuomotor Learning<br>ViTaS\uff1a\u9762\u5411\u89c6\u89c9\u8fd0\u52a8\u5b66\u4e60\u7684\u89c6\u89c9\u89e6\u89c9\u8f6f\u878d\u5408\u5bf9\u6bd4\u5b66\u4e60<br>[\u6458\u8981](abstracts/2602.11643.html)**|Huazhe Xu Team|[2602.11643](http://arxiv.org/abs/2602.11643)|[HJFY](https://hjfy.top/arxiv/2602.11643v1)|\n"}, "VLN": {"2601.21751": "|**2026-01-29**|**Dynamic Topology Awareness: Breaking the Granularity Rigidity in Vision-Language Navigation<br>\u52a8\u6001\u62d3\u6251\u611f\u77e5\uff1a\u6253\u7834\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4e2d\u7684\u7c92\u5ea6\u50f5\u5316<br>[\u6458\u8981](abstracts/2601.21751.html)**|Xiaoming Wang Team|[2601.21751](http://arxiv.org/abs/2601.21751)|[HJFY](https://hjfy.top/arxiv/2601.21751v1)|\n", "2601.18492": "|**2026-01-26**|**DV-VLN: Dual Verification for Reliable LLM-Based Vision-and-Language Navigation<br>DV-VLN\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u53cc\u91cd\u9a8c\u8bc1\u53ef\u9760\u6846\u67b6<br>[\u6458\u8981](abstracts/2601.18492.html)**|Shoujun Zhou Team|[2601.18492](http://arxiv.org/abs/2601.18492)|[HJFY](https://hjfy.top/arxiv/2601.18492v1)|\n", "2601.18188": "|**2026-01-26**|**\\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation<br>NaVIDA\uff1a\u57fa\u4e8e\u9006\u52a8\u529b\u5b66\u589e\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a<br>[\u6458\u8981](abstracts/2601.18188.html)**|Feng Zheng Team|[2601.18188](http://arxiv.org/abs/2601.18188)|[HJFY](https://hjfy.top/arxiv/2601.18188v1)|\n", "2601.15614": "|**2026-01-22**|**AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning<br>AION\uff1a\u57fa\u4e8e\u53cc\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u7684\u7a7a\u4e2d\u5ba4\u5185\u76ee\u6807\u5bfc\u822a\u7cfb\u7edf<br>[\u6458\u8981](abstracts/2601.15614.html)**|Lin Zhao Team|[2601.15614](http://arxiv.org/abs/2601.15614)|[HJFY](https://hjfy.top/arxiv/2601.15614v1)|\n", "2601.13976": "|**2026-01-23**|**FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation<br>FantasyVLN\uff1a\u9762\u5411\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u7edf\u4e00\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\u6846\u67b6<br>[\u6458\u8981](abstracts/2601.13976.html)**|Yonggang Qi Team|[2601.13976](http://arxiv.org/abs/2601.13976)|[HJFY](https://hjfy.top/arxiv/2601.13976v2)|\n", "2601.12766": "|**2026-01-19**|**Spatial-VLN: Zero-Shot Vision-and-Language Navigation With Explicit Spatial Perception and Exploration<br>Spatial-VLN\uff1a\u5177\u5907\u663e\u5f0f\u7a7a\u95f4\u611f\u77e5\u4e0e\u63a2\u7d22\u80fd\u529b\u7684\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a<br>[\u6458\u8981](abstracts/2601.12766.html)**|Feitian Zhang Team|[2601.12766](http://arxiv.org/abs/2601.12766)|[HJFY](https://hjfy.top/arxiv/2601.12766v1)|\n", "2601.09111": "|**2026-01-14**|**Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning<br>\u8fc8\u5411\u5f00\u653e\u73af\u5883\u4e0e\u6307\u4ee4\uff1a\u57fa\u4e8e\u5feb\u6162\u4ea4\u4e92\u63a8\u7406\u7684\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a<br>[\u6458\u8981](abstracts/2601.09111.html)**|Yahong Han Team|[2601.09111](http://arxiv.org/abs/2601.09111)|[HJFY](https://hjfy.top/arxiv/2601.09111v1)|\n", "2601.08665": "|**2026-01-13**|**VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory**|Shaoan Wang et.al.|[2601.08665](http://arxiv.org/abs/2601.08665)|null|\n", "2601.07375": "|**2026-01-12**|**GROKE: Vision-Free Navigation Instruction Evaluation via Graph Reasoning on OpenStreetMap**|Farzad Shami et.al.|[2601.07375](http://arxiv.org/abs/2601.07375)|null|\n", "2601.08868": "|**2026-01-11**|**Residual Cross-Modal Fusion Networks for Audio-Visual Navigation**|Yi Wang et.al.|[2601.08868](http://arxiv.org/abs/2601.08868)|null|\n", "2602.02220": "|**2026-02-02**|**LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation<br>LangMap\uff1a\u9762\u5411\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u5bfc\u822a\u7684\u5206\u5c42\u57fa\u51c6<br>[\u6458\u8981](abstracts/2602.02220.html)**|Anton van den Hengel Team|[2602.02220](http://arxiv.org/abs/2602.02220)|[HJFY](https://hjfy.top/arxiv/2602.02220v1)|\n", "2602.00551": "|**2026-01-31**|**APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation<br>APEX\uff1a\u4e00\u79cd\u7528\u4e8e\u5f02\u6b65\u7a7a\u4e2d\u76ee\u6807\u5bfc\u822a\u7684\u89e3\u8026\u8bb0\u5fc6\u578b\u63a2\u7d22\u5668<br>[\u6458\u8981](abstracts/2602.00551.html)**|Shuo Yang Team|[2602.00551](http://arxiv.org/abs/2602.00551)|[HJFY](https://hjfy.top/arxiv/2602.00551v1)|\n", "2602.00222": "|**2026-02-03**|**MapDream: Task-Driven Map Learning for Vision-Language Navigation<br>MapDream\uff1a\u9762\u5411\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u4efb\u52a1\u9a71\u52a8\u5730\u56fe\u5b66\u4e60<br>[\u6458\u8981](abstracts/2602.00222.html)**|Zhaoxin Fan Team|[2602.00222](http://arxiv.org/abs/2602.00222)|[HJFY](https://hjfy.top/arxiv/2602.00222v2)|\n", "2602.09657": "|**2026-02-10**|**AutoFly: Vision-Language-Action Model for UAV Autonomous Navigation in the Wild<br>AutoFly\uff1a\u9762\u5411\u91ce\u5916\u65e0\u4eba\u673a\u81ea\u4e3b\u5bfc\u822a\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b<br>[\u6458\u8981](abstracts/2602.09657.html)**|Hui Xiong Team|[2602.09657](http://arxiv.org/abs/2602.09657)|[HJFY](https://hjfy.top/arxiv/2602.09657v1)|\n", "2602.08236": "|**2026-02-09**|**When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning<br>\u4f55\u65f6\u60f3\u8c61\u4e0e\u60f3\u8c61\u591a\u5c11\uff1a\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u81ea\u9002\u5e94\u6d4b\u8bd5\u65f6\u7f29\u653e\u7528\u4e8e\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406<br>[\u6458\u8981](abstracts/2602.08236.html)**|Mohit Bansal Team|[2602.08236](http://arxiv.org/abs/2602.08236)|[HJFY](https://hjfy.top/arxiv/2602.08236v1)|\n", "2602.07629": "|**2026-02-10**|**LCLA: Language-Conditioned Latent Alignment for Vision-Language Navigation<br>LCLA\uff1a\u9762\u5411\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7684\u8bed\u8a00\u6761\u4ef6\u5316\u6f5c\u5728\u5bf9\u9f50\u6846\u67b6<br>[\u6458\u8981](abstracts/2602.07629.html)**|Soumik Sarkar Team|[2602.07629](http://arxiv.org/abs/2602.07629)|[HJFY](https://hjfy.top/arxiv/2602.07629v2)|\n", "2602.06427": "|**2026-02-06**|**Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters<br>\u5f25\u5408\u5ba4\u5185\u5916\u9e3f\u6c9f\uff1a\u9762\u5411\u6700\u540e\u51e0\u7c73\u7684\u89c6\u89c9\u4e2d\u5fc3\u5316\u6307\u4ee4\u5f15\u5bfc\u5177\u8eab\u5bfc\u822a<br>[\u6458\u8981](abstracts/2602.06427.html)**|Mu Xu Team|[2602.06427](http://arxiv.org/abs/2602.06427)|[HJFY](https://hjfy.top/arxiv/2602.06427v1)|\n", "2602.06356": "|**2026-02-06**|**Nipping the Drift in the Bud: Retrospective Rectification for Robust Vision-Language Navigation<br>\u9632\u5fae\u675c\u6e10\uff1a\u57fa\u4e8e\u56de\u6eaf\u4fee\u6b63\u7684\u9c81\u68d2\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a<br>[\u6458\u8981](abstracts/2602.06356.html)**|Weiying Xie Team|[2602.06356](http://arxiv.org/abs/2602.06356)|[HJFY](https://hjfy.top/arxiv/2602.06356v1)|\n", "2602.05827": "|**2026-02-05**|**Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation<br>\u7a00\u758f\u89c6\u9891\u751f\u6210\u63a8\u52a8\u73b0\u5b9e\u4e16\u754c\u8d85\u89c6\u8ddd\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a<br>[\u6458\u8981](abstracts/2602.05827.html)**|Hongyang Li Team|[2602.05827](http://arxiv.org/abs/2602.05827)|[HJFY](https://hjfy.top/arxiv/2602.05827v1)|\n", "2602.05789": "|**2026-02-05**|**Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation<br>\u4ed6\u8005\u4e2d\u5fc3\u611f\u77e5\u5668\uff1a\u901a\u8fc7\u6846\u67b6\u5b9e\u4f8b\u5316\u4ece\u4ed6\u8005\u89c6\u89c9\u5148\u9a8c\u4e2d\u89e3\u8026\u4ed6\u8005\u4e2d\u5fc3\u63a8\u7406<br>[\u6458\u8981](abstracts/2602.05789.html)**|Weiming Zhang Team|[2602.05789](http://arxiv.org/abs/2602.05789)|[HJFY](https://hjfy.top/arxiv/2602.05789v1)|\n", "2602.05467": "|**2026-02-05**|**MerNav: A Highly Generalizable Memory-Execute-Review Framework for Zero-Shot Object Goal Navigation<br>MerNav\uff1a\u4e00\u79cd\u9ad8\u5ea6\u53ef\u6cdb\u5316\u7684\u8bb0\u5fc6-\u6267\u884c-\u56de\u987e\u6846\u67b6\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u76ee\u6807\u5bfc\u822a<br>[\u6458\u8981](abstracts/2602.05467.html)**|Mu Xu Team|[2602.05467](http://arxiv.org/abs/2602.05467)|[HJFY](https://hjfy.top/arxiv/2602.05467v1)|\n", "2602.11598": "|**2026-02-12**|**ABot-N0: Technical Report on the VLA Foundation Model for Versatile Embodied Navigation<br>ABot-N0\uff1a\u9762\u5411\u901a\u7528\u5177\u8eab\u5bfc\u822a\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u57fa\u7840\u6a21\u578b\u6280\u672f\u62a5\u544a<br>[\u6458\u8981](abstracts/2602.11598.html)**|Mu Xu Team|[2602.11598](http://arxiv.org/abs/2602.11598)|[HJFY](https://hjfy.top/arxiv/2602.11598v1)|\n", "2602.09972": "|**2026-02-10**|**Hydra-Nav: Object Navigation via Adaptive Dual-Process Reasoning<br>Hydra-Nav\uff1a\u57fa\u4e8e\u81ea\u9002\u5e94\u53cc\u8fc7\u7a0b\u63a8\u7406\u7684\u76ee\u6807\u5bfc\u822a<br>[\u6458\u8981](abstracts/2602.09972.html)**|Yiming Gan Team|[2602.09972](http://arxiv.org/abs/2602.09972)|[HJFY](https://hjfy.top/arxiv/2602.09972v1)|\n"}, "VLM": {"2601.22155": "|**2026-01-29**|**UEval: A Benchmark for Unified Multimodal Generation**|Bo Li et.al.|[2601.22155](http://arxiv.org/abs/2601.22155)|null|\n", "2601.22150": "|**2026-01-29**|**Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions**|Xiaoxiao Sun et.al.|[2601.22150](http://arxiv.org/abs/2601.22150)|null|\n", "2601.22114": "|**2026-01-29**|**SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence**|Saoud Aldowaish et.al.|[2601.22114](http://arxiv.org/abs/2601.22114)|null|\n", "2601.22069": "|**2026-01-29**|**VTC-R1: Vision-Text Compression for Efficient Long-Context Reasoning**|Yibo Wang et.al.|[2601.22069](http://arxiv.org/abs/2601.22069)|null|\n", "2601.22060": "|**2026-01-29**|**Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models**|Wenxuan Huang et.al.|[2601.22060](http://arxiv.org/abs/2601.22060)|null|\n", "2601.22054": "|**2026-01-29**|**MetricAnything: Scaling Metric Depth Pretraining with Noisy Heterogeneous Sources**|Baorui Ma et.al.|[2601.22054](http://arxiv.org/abs/2601.22054)|null|\n", "2601.22020": "|**2026-01-29**|**Visual-Guided Key-Token Regularization for Multimodal Large Language Model Unlearning**|Chengyi Cai et.al.|[2601.22020](http://arxiv.org/abs/2601.22020)|null|\n", "2601.21998": "|**2026-01-29**|**Causal World Modeling for Robot Control**|Lin Li et.al.|[2601.21998](http://arxiv.org/abs/2601.21998)|null|\n", "2601.21944": "|**2026-01-29**|**Clarity: The Flexibility-Interpretability Trade-Off in Sparsity-aware Concept Bottleneck Models**|Konstantinos P. Panousis et.al.|[2601.21944](http://arxiv.org/abs/2601.21944)|null|\n", "2601.21915": "|**2026-01-29**|**VideoAesBench: Benchmarking the Video Aesthetics Perception Capabilities of Large Multimodal Models**|Yunhao Li et.al.|[2601.21915](http://arxiv.org/abs/2601.21915)|null|\n", "2601.23281": "|**2026-01-30**|**User Prompting Strategies and Prompt Enhancement Methods for Open-Set Object Detection in XR Environments**|Junfeng Lin et.al.|[2601.23281](http://arxiv.org/abs/2601.23281)|null|\n", "2601.23253": "|**2026-01-30**|**Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models**|Yi Zhang et.al.|[2601.23253](http://arxiv.org/abs/2601.23253)|null|\n", "2601.23251": "|**2026-01-30**|**Structured Over Scale: Learning Spatial Reasoning from Educational Video**|Bishoy Galoaa et.al.|[2601.23251](http://arxiv.org/abs/2601.23251)|null|\n", "2601.23224": "|**2026-01-30**|**Video-o3: Native Interleaved Clue Seeking for Long Video Multi-Hop Reasoning**|Xiangyu Zeng et.al.|[2601.23224](http://arxiv.org/abs/2601.23224)|null|\n", "2601.23220": "|**2026-01-30**|**Med-Scout: Curing MLLMs' Geometric Blindness in Medical Perception via Geometry-Aware RL Post-Training**|Anglin Liu et.al.|[2601.23220](http://arxiv.org/abs/2601.23220)|null|\n", "2601.23179": "|**2026-01-30**|**Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization**|Hui Lu et.al.|[2601.23179](http://arxiv.org/abs/2601.23179)|null|\n", "2601.23149": "|**2026-01-30**|**Hearing is Believing? Evaluating and Analyzing Audio Language Model Sycophancy with SYAUDIO**|Junchi Yao et.al.|[2601.23149](http://arxiv.org/abs/2601.23149)|null|\n", "2601.23041": "|**2026-01-30**|**One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs**|Youxu Shi et.al.|[2601.23041](http://arxiv.org/abs/2601.23041)|null|\n", "2601.22959": "|**2026-01-30**|**Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models**|Anmin Wang et.al.|[2601.22959](http://arxiv.org/abs/2601.22959)|null|\n", "2601.22948": "|**2026-01-30**|**Alignment among Language, Vision and Action Representations**|Nicola Milano et.al.|[2601.22948](http://arxiv.org/abs/2601.22948)|null|\n", "2602.02468": "|**2026-02-02**|**Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts<br>Avenir-Web\uff1a\u57fa\u4e8e\u6df7\u5408\u5b9a\u4f4d\u4e13\u5bb6\u7684\u4eba\u7c7b\u7ecf\u9a8c\u6a21\u4eff\u5f0f\u591a\u6a21\u6001\u7f51\u7edc\u4ee3\u7406<br>[\u6458\u8981](abstracts/2602.02468.html)**|Mengdi Wang Team|[2602.02468](http://arxiv.org/abs/2602.02468)|[HJFY](https://hjfy.top/arxiv/2602.02468v1)|\n", "2602.02465": "|**2026-02-02**|**MentisOculi: Revealing the Limits of Reasoning with Mental Imagery<br>MentisOculi\uff1a\u63ed\u793a\u5fc3\u667a\u610f\u8c61\u63a8\u7406\u7684\u5c40\u9650\u6027<br>[\u6458\u8981](abstracts/2602.02465.html)**|Wieland Brendel Team|[2602.02465](http://arxiv.org/abs/2602.02465)|[HJFY](https://hjfy.top/arxiv/2602.02465v1)|\n", "2602.02456": "|**2026-02-02**|**Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning<br>\u9762\u5411\u4efb\u52a1\u63a8\u7406\u7684\u5173\u7cfb\u611f\u77e5\u5206\u5c42\u4e09\u7ef4\u573a\u666f\u56fe<br>[\u6458\u8981](abstracts/2602.02456.html)**|Kostas Alexis Team|[2602.02456](http://arxiv.org/abs/2602.02456)|[HJFY](https://hjfy.top/arxiv/2602.02456v1)|\n", "2602.02454": "|**2026-02-02**|**World-Gymnast: Training Robots with Reinforcement Learning in a World Model<br>\u4e16\u754c\u4f53\u64cd\u5bb6\uff1a\u5728\u4e16\u754c\u6a21\u578b\u4e2d\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u673a\u5668\u4eba<br>[\u6458\u8981](abstracts/2602.02454.html)**|Sherry Yang Team|[2602.02454](http://arxiv.org/abs/2602.02454)|[HJFY](https://hjfy.top/arxiv/2602.02454v1)|\n", "2602.02408": "|**2026-02-02**|**ReasonEdit: Editing Vision-Language Models using Human Reasoning<br>ReasonEdit\uff1a\u57fa\u4e8e\u4eba\u7c7b\u63a8\u7406\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7f16\u8f91<br>[\u6458\u8981](abstracts/2602.02408.html)**|Thomas Hartvigsen Team|[2602.02408](http://arxiv.org/abs/2602.02408)|[HJFY](https://hjfy.top/arxiv/2602.02408v1)|\n", "2602.02341": "|**2026-02-02**|**LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization<br>LongVPO\uff1a\u4ece\u951a\u5b9a\u7ebf\u7d22\u5230\u81ea\u6211\u63a8\u7406\u7684\u957f\u89c6\u9891\u504f\u597d\u4f18\u5316<br>[\u6458\u8981](abstracts/2602.02341.html)**|Limin Wang Team|[2602.02341](http://arxiv.org/abs/2602.02341)|[HJFY](https://hjfy.top/arxiv/2602.02341v1)|\n", "2602.02185": "|**2026-02-02**|**Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models<br>Vision-DeepResearch\u57fa\u51c6\uff1a\u91cd\u65b0\u601d\u8003\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u4e0e\u6587\u672c\u641c\u7d22\u80fd\u529b<br>[\u6458\u8981](abstracts/2602.02185.html)**|Shaosheng Cao Team|[2602.02185](http://arxiv.org/abs/2602.02185)|[HJFY](https://hjfy.top/arxiv/2602.02185v1)|\n", "2602.02063": "|**2026-02-02**|**See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers<br>See2Refine\uff1a\u89c6\u89c9-\u8bed\u8a00\u53cd\u9988\u63d0\u5347\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684eHMI\u884c\u4e3a\u8bbe\u8ba1\u80fd\u529b<br>[\u6458\u8981](abstracts/2602.02063.html)**|Takeo Igarashi Team|[2602.02063](http://arxiv.org/abs/2602.02063)|[HJFY](https://hjfy.top/arxiv/2602.02063v1)|\n", "2602.02043": "|**2026-02-02**|**Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models<br>Auto-Comp\uff1a\u9762\u5411\u5bf9\u6bd4\u5f0f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53ef\u6269\u5c55\u7ec4\u5408\u6027\u63a2\u6d4b\u7684\u81ea\u52a8\u5316\u6d41\u7a0b<br>[\u6458\u8981](abstracts/2602.02043.html)**|Toshihiko Yamasaki Team|[2602.02043](http://arxiv.org/abs/2602.02043)|[HJFY](https://hjfy.top/arxiv/2602.02043v1)|\n", "2602.02033": "|**2026-02-02**|**One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation<br>\u4e00\u56fe\u591a\u914d\uff1a\u5728\u5927\u89c4\u6a21\u5e7f\u544a\u56fe\u50cf\u751f\u6210\u4e2d\u534f\u8c03\u591a\u6837\u5316\u7684\u7fa4\u4f53\u70b9\u51fb\u504f\u597d<br>[\u6458\u8981](abstracts/2602.02033.html)**|Jian Liang Team|[2602.02033](http://arxiv.org/abs/2602.02033)|[HJFY](https://hjfy.top/arxiv/2602.02033v1)|\n", "2602.04864": "|**2026-02-04**|**When LLaVA Meets Objects: Token Composition for Vision-Language-Models<br>\u5f53LLaVA\u9047\u89c1\u7269\u4f53\uff1a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4ee4\u724c\u7ec4\u5408<br>[\u6458\u8981](abstracts/2602.04864.html)**|Hilde Kuehne Team|[2602.04864](http://arxiv.org/abs/2602.04864)|[HJFY](https://hjfy.top/arxiv/2602.04864v1)|\n", "2602.04849": "|**2026-02-04**|**El Agente Estructural: An Artificially Intelligent Molecular Editor<br>\u7ed3\u6784\u667a\u80fd\u4f53\uff1a\u4e00\u79cd\u4eba\u5de5\u667a\u80fd\u5206\u5b50\u7f16\u8f91\u5668<br>[\u6458\u8981](abstracts/2602.04849.html)**|Varinia Bernales Team|[2602.04849](http://arxiv.org/abs/2602.04849)|[HJFY](https://hjfy.top/arxiv/2602.04849v1)|\n", "2602.04802": "|**2026-02-04**|**VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?<br>VISTA-Bench\uff1a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u771f\u7684\u80fd\u50cf\u7406\u89e3\u7eaf\u6587\u672c\u4e00\u6837\u7406\u89e3\u56fe\u50cf\u4e2d\u7684\u6587\u672c\u5417\uff1f<br>[\u6458\u8981](abstracts/2602.04802.html)**|Huchuan Lu Team|[2602.04802](http://arxiv.org/abs/2602.04802)|[HJFY](https://hjfy.top/arxiv/2602.04802v1)|\n", "2602.04739": "|**2026-02-04**|**Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases<br>\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5bf9\u9f50\u6f02\u79fb\uff1a\u5bf9\u516b\u4e2a\u6a21\u578b\u7248\u672c\u6709\u5bb3\u6027\u7684\u4e24\u9636\u6bb5\u7eb5\u5411\u8bc4\u4f30<br>[\u6458\u8981](abstracts/2602.04739.html)**|Emily Dix Team|[2602.04739](http://arxiv.org/abs/2602.04739)|[HJFY](https://hjfy.top/arxiv/2602.04739v1)|\n", "2602.04712": "|**2026-02-04**|**SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation<br>SAR-RAG\uff1a\u901a\u8fc7\u8bed\u4e49\u641c\u7d22\u3001\u68c0\u7d22\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u81ea\u52a8\u76ee\u6807\u8bc6\u522b\u89c6\u89c9\u95ee\u7b54<br>[\u6458\u8981](abstracts/2602.04712.html)**|Andreas Spanias Team|[2602.04712](http://arxiv.org/abs/2602.04712)|[HJFY](https://hjfy.top/arxiv/2602.04712v1)|\n", "2602.04699": "|**2026-02-04**|**Annotation Free Spacecraft Detection and Segmentation using Vision Language Models<br>\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u6807\u6ce8\u822a\u5929\u5668\u68c0\u6d4b\u4e0e\u5206\u5272<br>[\u6458\u8981](abstracts/2602.04699.html)**|Djamila Aouada Team|[2602.04699](http://arxiv.org/abs/2602.04699)|[HJFY](https://hjfy.top/arxiv/2602.04699v1)|\n", "2602.04672": "|**2026-02-04**|**AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation<br>AGILE\uff1a\u57fa\u4e8e\u667a\u80fd\u4f53\u751f\u6210\u4ece\u89c6\u9891\u91cd\u5efa\u624b-\u7269\u4ea4\u4e92<br>[\u6458\u8981](abstracts/2602.04672.html)**|Chunhua Shen Team|[2602.04672](http://arxiv.org/abs/2602.04672)|[HJFY](https://hjfy.top/arxiv/2602.04672v1)|\n", "2602.04657": "|**2026-02-04**|**PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective<br>PIO-FVLM\uff1a\u4ece\u63a8\u7406\u76ee\u6807\u89c6\u89d2\u91cd\u65b0\u5ba1\u89c6\u7528\u4e8eVLM\u52a0\u901f\u7684\u65e0\u8bad\u7ec3\u89c6\u89c9\u4ee4\u724c\u7f29\u51cf<br>[\u6458\u8981](abstracts/2602.04657.html)**|Chunhua Shen Team|[2602.04657](http://arxiv.org/abs/2602.04657)|[HJFY](https://hjfy.top/arxiv/2602.04657v1)|\n", "2602.04635": "|**2026-02-04**|**Relational Scene Graphs for Object Grounding of Natural Language Commands<br>\u9762\u5411\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e2d\u7269\u4f53\u5b9a\u4f4d\u7684\u5173\u7cfb\u573a\u666f\u56fe<br>[\u6458\u8981](abstracts/2602.04635.html)**|Ville Kyrki Team|[2602.04635](http://arxiv.org/abs/2602.04635)|[HJFY](https://hjfy.top/arxiv/2602.04635v1)|\n", "2602.04617": "|**2026-02-04**|**LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation<br>LEAD\uff1a\u9762\u5411\u5fe0\u5b9e\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u7684\u5c42\u7ea7\u4e13\u5bb6\u5bf9\u9f50\u89e3\u7801<br>[\u6458\u8981](abstracts/2602.04617.html)**|Yan Song Team|[2602.04617](http://arxiv.org/abs/2602.04617)|[HJFY](https://hjfy.top/arxiv/2602.04617v1)|\n", "2602.09850": "|**2026-02-10**|**Reason-IAD: Knowledge-Guided Dynamic Latent Reasoning for Explainable Industrial Anomaly Detection<br>Reason-IAD\uff1a\u9762\u5411\u53ef\u89e3\u91ca\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u7684\u77e5\u8bc6\u5f15\u5bfc\u52a8\u6001\u6f5c\u5728\u63a8\u7406\u6846\u67b6<br>[\u6458\u8981](abstracts/2602.09850.html)**|Xiaochun Cao Team|[2602.09850](http://arxiv.org/abs/2602.09850)|[HJFY](https://hjfy.top/arxiv/2602.09850v1)|\n", "2602.09843": "|**2026-02-10**|**Kelix Technique Report<br>Kelix\u6280\u672f\u62a5\u544a<br>[\u6458\u8981](abstracts/2602.09843.html)**|Ziqi Wang Team|[2602.09843](http://arxiv.org/abs/2602.09843)|[HJFY](https://hjfy.top/arxiv/2602.09843v1)|\n", "2602.09825": "|**2026-02-10**|**SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding<br>SAKED\uff1a\u901a\u8fc7\u7a33\u5b9a\u6027\u611f\u77e5\u7684\u77e5\u8bc6\u589e\u5f3a\u89e3\u7801\u7f13\u89e3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898<br>[\u6458\u8981](abstracts/2602.09825.html)**|Xudong Jiang Team|[2602.09825](http://arxiv.org/abs/2602.09825)|[HJFY](https://hjfy.top/arxiv/2602.09825v1)|\n", "2602.09701": "|**2026-02-10**|**GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation<br>GenSeg-R1\uff1a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u89c6\u89c9\u8bed\u8a00\u7ec6\u7c92\u5ea6\u6307\u4ee3\u5206\u5272<br>[\u6458\u8981](abstracts/2602.09701.html)**|Uma Mahesh Team|[2602.09701](http://arxiv.org/abs/2602.09701)|[HJFY](https://hjfy.top/arxiv/2602.09701v1)|\n", "2602.09638": "|**2026-02-10**|**VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model<br>VideoAfford\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u4eba-\u7269\u4ea4\u4e92\u89c6\u9891\u4e2d\u5b9e\u73b0\u4e09\u7ef4\u529f\u80fd\u53ef\u53ca\u6027\u63a5\u5730<br>[\u6458\u8981](abstracts/2602.09638.html)**|Hui Xiong Team|[2602.09638](http://arxiv.org/abs/2602.09638)|[HJFY](https://hjfy.top/arxiv/2602.09638v1)|\n", "2602.09611": "|**2026-02-10**|**AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models<br>AGMark\uff1a\u9762\u5411\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5f15\u5bfc\u52a8\u6001\u6c34\u5370\u6280\u672f<br>[\u6458\u8981](abstracts/2602.09611.html)**|Linlin Wang Team|[2602.09611](http://arxiv.org/abs/2602.09611)|[HJFY](https://hjfy.top/arxiv/2602.09611v1)|\n", "2602.09609": "|**2026-02-10**|**Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing<br>Tele-Omni\uff1a\u9762\u5411\u89c6\u9891\u751f\u6210\u4e0e\u7f16\u8f91\u7684\u7edf\u4e00\u591a\u6a21\u6001\u6846\u67b6<br>[\u6458\u8981](abstracts/2602.09609.html)**|Xuelong Li Team|[2602.09609](http://arxiv.org/abs/2602.09609)|[HJFY](https://hjfy.top/arxiv/2602.09609v1)|\n", "2602.09586": "|**2026-02-10**|**Delving into Spectral Clustering with Vision-Language Representations<br>\u63a2\u7d22\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u8868\u5f81\u7684\u5149\u8c31\u805a\u7c7b\u65b9\u6cd5<br>[\u6458\u8981](abstracts/2602.09586.html)**|Zhen Fang Team|[2602.09586](http://arxiv.org/abs/2602.09586)|[HJFY](https://hjfy.top/arxiv/2602.09586v1)|\n", "2602.09541": "|**2026-02-10**|**Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination<br>\u624b\u672f\u5200\uff1a\u901a\u8fc7\u6df7\u5408\u9ad8\u65af\u6865\u7cbe\u7ec6\u5bf9\u9f50\u6ce8\u610f\u529b\u6fc0\u6d3b\u6d41\u5f62\u4ee5\u7f13\u89e3\u591a\u6a21\u6001\u5e7b\u89c9<br>[\u6458\u8981](abstracts/2602.09541.html)**|Koichi Shirahata Team|[2602.09541](http://arxiv.org/abs/2602.09541)|[HJFY](https://hjfy.top/arxiv/2602.09541v1)|\n", "2602.09531": "|**2026-02-10**|**DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment<br>DR.Experts\uff1a\u9762\u5411\u76f2\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u7684\u5931\u771f\u611f\u77e5\u4e13\u5bb6\u5dee\u5206\u7ec6\u5316\u65b9\u6cd5<br>[\u6458\u8981](abstracts/2602.09531.html)**|Runze Hu Team|[2602.09531](http://arxiv.org/abs/2602.09531)|[HJFY](https://hjfy.top/arxiv/2602.09531v1)|\n", "2602.12281": "|**2026-02-12**|**Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment<br>\u6269\u5c55\u9a8c\u8bc1\u5728\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u5bf9\u9f50\u4e2d\u6bd4\u6269\u5c55\u7b56\u7565\u5b66\u4e60\u66f4\u6709\u6548<br>[\u6458\u8981](abstracts/2602.12281.html)**|Marco Pavone Team|[2602.12281](http://arxiv.org/abs/2602.12281)|[HJFY](https://hjfy.top/arxiv/2602.12281v1)|\n", "2602.12203": "|**2026-02-12**|**ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images<br>ExStrucTiny\uff1a\u9762\u5411\u6587\u6863\u56fe\u50cf\u4e2d\u6a21\u5f0f\u53ef\u53d8\u7ed3\u6784\u5316\u4fe1\u606f\u63d0\u53d6\u7684\u57fa\u51c6\u6570\u636e\u96c6<br>[\u6458\u8981](abstracts/2602.12203.html)**|Manuela Veloso Team|[2602.12203](http://arxiv.org/abs/2602.12203)|[HJFY](https://hjfy.top/arxiv/2602.12203v1)|\n", "2602.12196": "|**2026-02-12**|**Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education<br>\u89c6\u89c9\u63a8\u7406\u57fa\u51c6\uff1a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u57fa\u7840\u6559\u80b2\u8bfe\u5802\u771f\u5b9e\u89c6\u89c9\u95ee\u9898\u4e0a\u7684\u8868\u73b0<br>[\u6458\u8981](abstracts/2602.12196.html)**|Oliver G. B. Garrod Team|[2602.12196](http://arxiv.org/abs/2602.12196)|[HJFY](https://hjfy.top/arxiv/2602.12196v1)|\n", "2602.12159": "|**2026-02-12**|**3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting<br>3DGSNav\uff1a\u901a\u8fc7\u4e3b\u52a83D\u9ad8\u65af\u6cfc\u6e85\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7269\u4f53\u5bfc\u822a\u4e2d\u7684\u63a8\u7406\u80fd\u529b<br>[\u6458\u8981](abstracts/2602.12159.html)**|Xinyi Yu Team|[2602.12159](http://arxiv.org/abs/2602.12159)|[HJFY](https://hjfy.top/arxiv/2602.12159v1)|\n", "2602.12092": "|**2026-02-12**|**DeepSight: An All-in-One LM Safety Toolkit<br>DeepSight\uff1a\u4e00\u4f53\u5316\u5927\u578b\u6a21\u578b\u5b89\u5168\u5de5\u5177\u7bb1<br>[\u6458\u8981](abstracts/2602.12092.html)**|Xia Hu Team|[2602.12092](http://arxiv.org/abs/2602.12092)|[HJFY](https://hjfy.top/arxiv/2602.12092v1)|\n", "2602.12065": "|**2026-02-12**|**Affordance-Graphed Task Worlds: Self-Evolving Task Generation for Scalable Embodied Learning<br>\u53ef\u4f9b\u6027\u56fe\u5316\u4efb\u52a1\u4e16\u754c\uff1a\u9762\u5411\u53ef\u6269\u5c55\u5177\u8eab\u5b66\u4e60\u7684\u81ea\u6f14\u5316\u4efb\u52a1\u751f\u6210<br>[\u6458\u8981](abstracts/2602.12065.html)**|Changshui Zhang Team|[2602.12065](http://arxiv.org/abs/2602.12065)|[HJFY](https://hjfy.top/arxiv/2602.12065v1)|\n", "2602.12002": "|**2026-02-12**|**Can Local Vision-Language Models improve Activity Recognition over Vision Transformers? -- Case Study on Newborn Resuscitation<br>\u672c\u5730\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u8d85\u8d8a\u89c6\u89c9Transformer\u63d0\u5347\u6d3b\u52a8\u8bc6\u522b\u80fd\u529b\uff1f\u2014\u2014\u4ee5\u65b0\u751f\u513f\u590d\u82cf\u4e3a\u4f8b\u7684\u7814\u7a76<br>[\u6458\u8981](abstracts/2602.12002.html)**|\u00d8yvind Meinich-Bache Team|[2602.12002](http://arxiv.org/abs/2602.12002)|[HJFY](https://hjfy.top/arxiv/2602.12002v1)|\n", "2602.11980": "|**2026-02-12**|**Spatial Chain-of-Thought: Bridging Understanding and Generation Models for Spatial Reasoning Generation<br>\u7a7a\u95f4\u601d\u7ef4\u94fe\uff1a\u8fde\u63a5\u7406\u89e3\u4e0e\u751f\u6210\u6a21\u578b\u4ee5\u5b9e\u73b0\u7a7a\u95f4\u63a8\u7406\u751f\u6210<br>[\u6458\u8981](abstracts/2602.11980.html)**|Long Chen Team|[2602.11980](http://arxiv.org/abs/2602.11980)|[HJFY](https://hjfy.top/arxiv/2602.11980v1)|\n", "2602.11960": "|**2026-02-12**|**Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion<br>\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u8bedPDF\u8f6cMarkdown\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u57fa\u51c6<br>[\u6458\u8981](abstracts/2602.11960.html)**|Nicolas Mery Team|[2602.11960](http://arxiv.org/abs/2602.11960)|[HJFY](https://hjfy.top/arxiv/2602.11960v1)|\n", "2602.11957": "|**2026-02-12**|**Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization<br>\u53ccLLM\u662f\u5426\u4f18\u4e8e\u5355\u4e00\u6a21\u578b\uff1f\u4e00\u79cd\u7528\u4e8e\u533b\u836f\u5185\u5bb9\u4f18\u5316\u7684\u5e08\u751f\u53cc\u5934LLM\u67b6\u6784<br>[\u6458\u8981](abstracts/2602.11957.html)**|Anubhav Girdhar Team|[2602.11957](http://arxiv.org/abs/2602.11957)|[HJFY](https://hjfy.top/arxiv/2602.11957v1)|\n"}}