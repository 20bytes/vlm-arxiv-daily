<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning&lt;br&gt;面向任务推理的关系感知分层三维场景图&lt;br&gt;[摘要](abstracts/2602.02456.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph&#x27;s semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.</p>
      <h2>摘要 (ZH)</h2>
      <p>以结构化方式表示和理解三维环境对于自主智能体导航和推理其周围环境至关重要。传统的同步定位与建图（SLAM）方法虽能生成度量重建并可扩展为度量-语义建图，但缺乏更高层次的抽象和关系推理能力。为弥补这一不足，三维场景图作为一种能够捕捉层次结构和物体关系的强大表示方法应运而生。本研究提出了一种增强型分层三维场景图，它在多个抽象层次上整合了开放词汇特征，并支持物体关系推理。我们的方法利用视觉语言模型（VLM）来推断语义关系。特别地，我们引入了一个任务推理模块，该模块结合大型语言模型（LLM）和视觉语言模型（VLM）来解析场景图的语义与关系信息，使智能体能够更智能地进行任务推理并与环境交互。通过在四足机器人上部署该方法于多种环境和任务中，我们验证了其推理能力。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
