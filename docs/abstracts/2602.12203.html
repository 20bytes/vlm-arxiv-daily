<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images</h1>
      <h2>Abstract (EN)</h2>
      <p>Enterprise documents, such as forms and reports, embed critical information for downstream applications like data archiving, automated workflows, and analytics. Although generalist Vision Language Models (VLMs) perform well on established document understanding benchmarks, their ability to conduct holistic, fine-grained structured extraction across diverse document types and flexible schemas is not well studied. Existing Key Entity Extraction (KEE), Relation Extraction (RE), and Visual Question Answering (VQA) datasets are limited by narrow entity ontologies, simple queries, or homogeneous document types, often overlooking the need for adaptable and structured extraction. To address these gaps, we introduce ExStrucTiny, a new benchmark dataset for structured Information Extraction (IE) from document images, unifying aspects of KEE, RE, and VQA. Built through a novel pipeline combining manual and synthetic human-validated samples, ExStrucTiny covers more varied document types and extraction scenarios. We analyze open and closed VLMs on this benchmark, highlighting challenges such as schema adaptation, query under-specification, and answer localization. We hope our work provides a bedrock for improving generalist models for structured IE in documents.</p>
      <h2>摘要 (ZH)</h2>
      <p>企业文档（如表格和报告）中蕴含的关键信息对于数据归档、自动化工作流和分析等下游应用至关重要。尽管通用视觉语言模型在现有文档理解基准测试中表现良好，但其在不同文档类型和灵活模式间进行整体、细粒度结构化提取的能力尚未得到充分研究。现有的关键实体提取、关系提取和视觉问答数据集受限于狭窄的实体本体、简单查询或同质文档类型，往往忽视了适应性结构化提取的需求。为填补这些空白，我们提出了ExStrucTiny——一个面向文档图像结构化信息提取的新基准数据集，它统一了关键实体提取、关系提取和视觉问答的多个方面。通过结合人工与合成人工验证样本的新颖流程构建，ExStrucTiny涵盖了更丰富的文档类型和提取场景。我们在此基准上分析了开放和封闭的视觉语言模型，揭示了模式适配、查询欠规范和答案定位等挑战。我们希望这项工作能为提升通用模型在文档结构化信息提取方面的能力奠定基础。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
