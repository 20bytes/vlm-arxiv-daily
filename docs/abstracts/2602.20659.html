<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Recursive Belief Vision Language Model</h1>
      <h2>Abstract (EN)</h2>
      <p>Current vision-language-action (VLA) models struggle with long-horizon manipulation under partial observability. Most existing approaches remain observation-driven, relying on short context windows or repeated queries to vision-language models (VLMs). This leads to loss of task progress, action repetition under perceptual aliasing, and high inference latency. Semantic reasoning alone is not the primary bottleneck in long-horizon manipulation. Instead, VLAs lack persistent, action-conditioned state representations and exhibit limited temporal and physical reasoning, making them ill-suited for multi-stage control. This paper introduces RB-VLA, a belief-centric architecture trained with self-supervised world-model objectives that maintains a compact latent state encoding task-relevant history, dynamics, and object interactions. Queried once for high-level intent, the VLM provides task specification, while the belief tracks task progress and enables phase-aware, causally grounded control under partial observability without storing raw observations or scaling memory with time. The belief and intent jointly condition a diffusion policy for robust closed-loop execution. RB-VLA outperforms prior VLAs on long-horizon benchmarks, achieving 52.5% and 37.5% higher success on multi-stage pick-and-place and stacking tasks, respectively, compared to π0. It also reduces inference latency by up to 5x relative to baselines and eliminates memory growth across timesteps observed in existing VLAs. Ablations show that the belief module is the primary driver of performance, increasing success rates from 32.5% to 77.5%. These results demonstrate the effectiveness of belief-based state representations for long-horizon VLA policies.</p>
      <h2>摘要 (ZH)</h2>
      <p>当前视觉-语言-动作（VLA）模型在部分可观测性下的长时程操作任务中表现不佳。大多数现有方法仍依赖观测驱动，通过短上下文窗口或重复查询视觉语言模型（VLM）来执行任务，这导致任务进度丢失、感知混淆下的动作重复以及高推理延迟。语义推理本身并非长时程操作的主要瓶颈；相反，VLA模型缺乏持久且动作条件化的状态表示，并表现出有限的时序和物理推理能力，使其难以适应多阶段控制。本文提出RB-VLA，一种以信念为中心的架构，通过自监督世界模型目标进行训练，能够维护一个紧凑的潜在状态，编码任务相关历史、动态及物体交互。VLM仅被查询一次以获取高层意图，提供任务规范，而信念模块则跟踪任务进度，并在部分可观测性下实现阶段感知、因果基础的控制，无需存储原始观测数据或随时间扩展内存。信念与意图共同条件化扩散策略，以实现鲁棒的闭环执行。RB-VLA在长时程基准测试中优于先前VLA模型，在多阶段拾放和堆叠任务上分别比π0模型实现了52.5%和37.5%更高的成功率。同时，其推理延迟相较于基线模型降低高达5倍，并消除了现有VLA模型中随时间步增长的内存消耗。消融实验表明，信念模块是性能提升的主要驱动力，将成功率从32.5%提高至77.5%。这些结果证明了基于信念的状态表示在长时程VLA策略中的有效性。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
