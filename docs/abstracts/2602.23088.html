<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy</h1>
      <h2>Abstract (EN)</h2>
      <p>Foundation models increasingly offer potential to support interactive, agentic workflows that assist researchers during analysis and interpretation of image data. Such workflows often require coupling vision to language to provide a natural-language interface. However, paired image-text data needed to learn this coupling are scarce and difficult to obtain in many research and clinical settings. One such setting is microscopic analysis of cell-body-stained histological human brain sections, which enables the study of cytoarchitecture: cell density and morphology and their laminar and areal organization. Here, we propose a label-mediated method that generates meaningful captions from images by linking images and text only through a label, without requiring curated paired image-text data. Given the label, we automatically mine area descriptions from related literature and use them as synthetic captions reflecting canonical cytoarchitectonic attributes. An existing cytoarchitectonic vision foundation model (CytoNet) is then coupled to a large language model via an image-to-text training objective, enabling microscopy regions to be described in natural language. Across 57 brain areas, the resulting method produces plausible area-level descriptions and supports open-set use through explicit rejection of unseen areas. It matches the cytoarchitectonic reference label for in-scope patches with 90.6% accuracy and, with the area label masked, its descriptions remain discriminative enough to recover the area in an 8-way test with 68.6% accuracy. These results suggest that weak, label-mediated pairing can suffice to connect existing biomedical vision foundation models to language, providing a practical recipe for integrating natural-language in domains where fine-grained paired annotations are scarce.</p>
      <h2>摘要 (ZH)</h2>
      <p>基础模型日益展现出支持交互式、自主化工作流程的潜力，以辅助研究人员在图像数据分析与解读过程中。这类工作流程通常需要将视觉与语言耦合，以提供自然语言界面。然而，在许多研究和临床场景中，学习这种耦合所需的配对图像-文本数据稀缺且难以获取。其中一个场景是对细胞体染色的人脑组织切片进行显微分析，这有助于研究细胞构筑学：细胞密度与形态及其层状和区域组织。本文提出一种标签介导的方法，通过仅利用标签将图像与文本关联，无需精心配对的图像-文本数据，即可从图像生成有意义的描述。给定标签后，我们自动从相关文献中挖掘区域描述，并将其用作反映典型细胞构筑学属性的合成描述。随后，通过图像到文本的训练目标，将现有的细胞构筑学视觉基础模型（CytoNet）与大型语言模型耦合，使得显微图像区域能够以自然语言进行描述。在57个脑区中，该方法生成了合理的区域级描述，并通过明确拒绝未见区域支持开放集使用。对于范围内图像块，其细胞构筑学参考标签匹配准确率达90.6%；在区域标签被掩蔽的情况下，其描述仍具有足够区分度，在8项测试中以68.6%的准确率恢复区域信息。这些结果表明，弱监督的标签介导配对足以将现有生物医学视觉基础模型与语言连接，为在细粒度配对标注稀缺的领域中集成自然语言提供了实用方案。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
