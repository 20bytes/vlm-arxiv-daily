<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>CoWTracker: Tracking by Warping instead of Correlation</h1>
      <h2>Abstract (EN)</h2>
      <p>Dense point tracking is a fundamental problem in computer vision, with applications ranging from video analysis to robotic manipulation. State-of-the-art trackers typically rely on cost volumes to match features across frames, but this approach incurs quadratic complexity in spatial resolution, limiting scalability and efficiency. In this paper, we propose \method, a novel dense point tracker that eschews cost volumes in favor of warping. Inspired by recent advances in optical flow, our approach iteratively refines track estimates by warping features from the target frame to the query frame based on the current estimate. Combined with a transformer architecture that performs joint spatiotemporal reasoning across all tracks, our design establishes long-range correspondences without computing feature correlations. Our model is simple and achieves state-of-the-art performance on standard dense point tracking benchmarks, including TAP-Vid-DAVIS, TAP-Vid-Kinetics, and Robo-TAP. Remarkably, the model also excels at optical flow, sometimes outperforming specialized methods on the Sintel, KITTI, and Spring benchmarks. These results suggest that warping-based architectures can unify dense point tracking and optical flow estimation.</p>
      <h2>摘要 (ZH)</h2>
      <p>密集点跟踪是计算机视觉中的一个基础问题，其应用范围从视频分析到机器人操作。当前最先进的跟踪器通常依赖成本体积来跨帧匹配特征，但这种方法在空间分辨率上具有二次复杂度，限制了可扩展性和效率。本文提出了一种新颖的密集点跟踪器——CoWTracker，它摒弃了成本体积，转而采用变形方法。受光流领域最新进展的启发，我们的方法基于当前估计，通过将目标帧的特征变形到查询帧，迭代地优化跟踪估计。结合一个在所有轨迹上进行联合时空推理的Transformer架构，我们的设计无需计算特征相关性即可建立长距离对应关系。该模型结构简洁，在标准密集点跟踪基准测试（包括TAP-Vid-DAVIS、TAP-Vid-Kinetics和Robo-TAP）中达到了最先进的性能。值得注意的是，该模型在光流估计方面也表现出色，有时在Sintel、KITTI和Spring基准测试中甚至超越了专用方法。这些结果表明，基于变形的架构可以统一密集点跟踪和光流估计。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
