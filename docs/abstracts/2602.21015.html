<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>From Perception to Action: An Interactive Benchmark for Vision Reasoning</h1>
      <h2>Abstract (EN)</h2>
      <p>Understanding the physical structure is essential for real-world applications such as embodied agents, interactive design, and long-horizon manipulation. Yet, prevailing Vision-Language Model (VLM) evaluations still center on structure-agnostic, single-turn setups (e.g., VQA), which fail to assess agents&#x27; ability to reason about how geometry, contact, and support relations jointly constrain what actions are possible in a dynamic environment. To address this gap, we introduce the Causal Hierarchy of Actions and Interactions (CHAIN) benchmark, an interactive 3D, physics-driven testbed designed to evaluate whether models can understand, plan, and execute structured action sequences grounded in physical constraints. CHAIN shifts evaluation from passive perception to active problem solving, spanning tasks such as interlocking mechanical puzzles and 3D stacking and packing. We conduct a comprehensive study of state-of-the-art VLMs and diffusion-based models under unified interactive settings. Our results show that top-performing models still struggle to internalize physical structure and causal constraints, often failing to produce reliable long-horizon plans and cannot robustly translate perceived structure into effective actions. The project is available at https://social-ai-studio.github.io/CHAIN/.</p>
      <h2>摘要 (ZH)</h2>
      <p>理解物理结构对于具身智能体、交互式设计和长时程操作等现实应用至关重要。然而，当前主流的视觉语言模型评估仍集中于结构无关的单轮设置（如视觉问答），无法评估智能体在动态环境中推理几何、接触和支撑关系如何共同约束可行行动的能力。为填补这一空白，我们引入了因果行动与交互层次基准，这是一个交互式、基于物理的3D测试平台，旨在评估模型能否理解、规划并执行基于物理约束的结构化行动序列。该基准将评估从被动感知转向主动问题解决，涵盖互锁机械拼图和三维堆叠与装箱等任务。我们在统一的交互设置下对前沿视觉语言模型和扩散模型进行了全面研究。结果表明，表现最佳的模型仍难以内化物理结构和因果约束，常无法生成可靠的长时程计划，且无法稳健地将感知结构转化为有效行动。项目网址：https://social-ai-studio.github.io/CHAIN/。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
