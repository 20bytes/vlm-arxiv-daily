<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers</h1>
      <h2>Abstract (EN)</h2>
      <p>Automated vehicles lack natural communication channels with other road users, making external Human-Machine Interfaces (eHMIs) essential for conveying intent and maintaining trust in shared environments. However, most eHMI studies rely on developer-crafted message-action pairs, which are difficult to adapt to diverse and dynamic traffic contexts. A promising alternative is to use Large Language Models (LLMs) as action designers that generate context-conditioned eHMI actions, yet such designers lack perceptual verification and typically depend on fixed prompts or costly human-annotated feedback for improvement. We present See2Refine, a human-free, closed-loop framework that uses vision-language model (VLM) perceptual evaluation as automated visual feedback to improve an LLM-based eHMI action designer. Given a driving context and a candidate eHMI action, the VLM evaluates the perceived appropriateness of the action, and this feedback is used to iteratively revise the designer&#x27;s outputs, enabling systematic refinement without human supervision. We evaluate our framework across three eHMI modalities (lightbar, eyes, and arm) and multiple LLM model sizes. Across settings, our framework consistently outperforms prompt-only LLM designers and manually specified baselines in both VLM-based metrics and human-subject evaluations. Results further indicate that the improvements generalize across modalities and that VLM evaluations are well aligned with human preferences, supporting the robustness and effectiveness of See2Refine for scalable action design.</p>
      <h2>摘要 (ZH)</h2>
      <p>自动驾驶车辆缺乏与其他道路使用者的自然沟通渠道，因此外部人机界面（eHMI）对于在共享环境中传达意图和维持信任至关重要。然而，大多数eHMI研究依赖于开发者手工设计的消息-行为配对，难以适应多样且动态的交通场景。一种有前景的替代方案是利用大语言模型（LLM）作为行为设计器，生成基于上下文的eHMI行为，但此类设计器缺乏感知验证，通常依赖固定提示或昂贵的人工标注反馈进行改进。我们提出了See2Refine，一种无需人工干预的闭环框架，利用视觉-语言模型（VLM）的感知评估作为自动化视觉反馈，以优化基于LLM的eHMI行为设计器。给定驾驶场景和候选eHMI行为，VLM评估该行为的感知适宜性，并利用此反馈迭代修正设计器的输出，从而实现无需人工监督的系统性优化。我们在三种eHMI模态（光条、眼睛和手臂）及多种LLM模型规模下评估了该框架。在所有设置中，我们的框架在基于VLM的指标和人类受试者评估中均持续优于仅使用提示的LLM设计器及手动指定的基线方法。结果进一步表明，改进效果在不同模态间具有泛化性，且VLM评估与人类偏好高度一致，这支持了See2Refine在可扩展行为设计中的鲁棒性和有效性。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
