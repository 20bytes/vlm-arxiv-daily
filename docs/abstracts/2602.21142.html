<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>LUMEN: Longitudinal Multi-Modal Radiology Model for Prognosis and Diagnosis</h1>
      <h2>Abstract (EN)</h2>
      <p>Large vision-language models (VLMs) have evolved from general-purpose applications to specialized use cases such as in the clinical domain, demonstrating potential for decision support in radiology. One promising application is assisting radiologists in decision-making by the analysis of radiology imaging data such as chest X-rays (CXR) via a visual and natural language question-answering (VQA) interface. When longitudinal imaging is available, radiologists analyze temporal changes, which are essential for accurate diagnosis and prognosis. The manual longitudinal analysis is a time-consuming process, motivating the development of a training framework that can provide prognostic capabilities. We introduce a novel training framework LUMEN, that is optimized for longitudinal CXR interpretation, leveraging multi-image and multi-task instruction fine-tuning to enhance prognostic and diagnostic performance. We conduct experiments on the publicly available MIMIC-CXR and its associated Medical-Diff-VQA datasets. We further formulate and construct a novel instruction-following dataset incorporating longitudinal studies, enabling the development of a prognostic VQA task. Our method demonstrates significant improvements over baseline models in diagnostic VQA tasks, and more importantly, shows promising potential for prognostic capabilities. These results underscore the value of well-designed, instruction-tuned VLMs in enabling more accurate and clinically meaningful radiological interpretation of longitudinal radiological imaging data.</p>
      <h2>摘要 (ZH)</h2>
      <p>大型视觉语言模型（VLMs）已从通用应用发展到临床等专业领域，展现出在放射学决策支持中的潜力。一个前景广阔的应用是通过视觉与自然语言问答（VQA）界面分析胸部X光片（CXR）等放射影像数据，辅助放射科医生进行决策。当存在纵向影像数据时，放射科医生会分析时间变化，这对准确诊断和预后至关重要。手动纵向分析过程耗时，这促使我们开发一种能够提供预后能力的训练框架。我们提出了一种新颖的训练框架LUMEN，该框架针对纵向CXR解读进行了优化，利用多图像和多任务指令微调来提升预后与诊断性能。我们在公开可用的MIMIC-CXR及其相关Medical-Diff-VQA数据集上进行了实验。我们进一步构建了一个包含纵向研究的新型指令遵循数据集，以支持预后VQA任务的开发。我们的方法在诊断VQA任务上相比基线模型表现出显著改进，更重要的是，在预后能力方面显示出良好潜力。这些结果凸显了精心设计的指令微调VLMs在实现更准确、更具临床意义的纵向放射影像数据解读中的价值。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
