<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model</h1>
      <h2>Abstract (EN)</h2>
      <p>3D affordance grounding aims to highlight the actionable regions on 3D objects, which is crucial for robotic manipulation. Previous research primarily focused on learning affordance knowledge from static cues such as language and images, which struggle to provide sufficient dynamic interaction context that can reveal temporal and causal cues. To alleviate this predicament, we collect a comprehensive video-based 3D affordance dataset, \textit{VIDA}, which contains 38K human-object-interaction videos covering 16 affordance types, 38 object categories, and 22K point clouds. Based on \textit{VIDA}, we propose a strong baseline: VideoAfford, which activates multimodal large language models with additional affordance segmentation capabilities, enabling both world knowledge reasoning and fine-grained affordance grounding within a unified framework. To enhance action understanding capability, we leverage a latent action encoder to extract dynamic interaction priors from HOI videos. Moreover, we introduce a \textit{spatial-aware} loss function to enable VideoAfford to obtain comprehensive 3D spatial knowledge. Extensive experimental evaluations demonstrate that our model significantly outperforms well-established methods and exhibits strong open-world generalization with affordance reasoning abilities. All datasets and code will be publicly released to advance research in this area.</p>
      <h2>摘要 (ZH)</h2>
      <p>三维功能可及性接地的目标在于突出三维物体上可操作区域，这对机器人操控至关重要。先前研究主要集中于从语言和图像等静态线索中学习可及性知识，难以提供足够的动态交互上下文以揭示时序与因果线索。为缓解此困境，我们收集了一个全面的基于视频的三维可及性数据集——VIDA，该数据集包含38K个人-物交互视频，涵盖16种可及性类型、38个物体类别以及22K个点云。基于VIDA，我们提出了一个强基线模型：VideoAfford，该模型通过增强可及性分割能力激活多模态大语言模型，在统一框架内同时实现世界知识推理与细粒度可及性接地。为提升动作理解能力，我们利用潜在动作编码器从人-物交互视频中提取动态交互先验。此外，我们引入了一种空间感知损失函数，使VideoAfford能够获取全面的三维空间知识。大量实验评估表明，我们的模型显著优于现有成熟方法，并展现出强大的开放世界泛化能力与可及性推理能力。所有数据集与代码将公开发布，以推动该领域的研究进展。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
