<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning</h1>
      <h2>Abstract (EN)</h2>
      <p>Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.</p>
      <h2>摘要 (ZH)</h2>
      <p>尽管多模态大语言模型（MLLMs）发展迅速，但在正确答案依赖于场景在未见或替代视角下如何呈现时，视觉空间推理仍不可靠。近期研究通过结合世界模型进行视觉想象来增强推理，但何时想象真正必要、多少想象有益以及何时想象有害等问题仍未得到充分理解。实践中，不加区分的想象会增加计算负担，甚至因引入误导性证据而降低性能。本研究对测试时视觉想象作为空间推理的可控资源进行了深入分析，探讨了静态视觉证据何时足够、想象何时能改进推理，以及过度或不必要的想象如何影响准确性和效率。为支持此分析，我们提出了AVIC，一种基于世界模型的自适应测试时框架，该框架在选择性调用和缩放视觉想象前，明确推理当前视觉证据的充分性。在空间推理基准（SAT、MMSI）和具身导航基准（R2R）上的实验结果表明，想象在关键、边缘或有害场景中存在明确区分，且选择性控制能以显著更少的世界模型调用和语言标记匹配或超越固定想象策略。总体而言，我们的发现强调了分析和控制测试时想象对于高效可靠空间推理的重要性。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
