<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters</h1>
      <h2>Abstract (EN)</h2>
      <p>Embodied navigation holds significant promise for real-world applications such as last-mile delivery. However, most existing approaches are confined to either indoor or outdoor environments and rely heavily on strong assumptions, such as access to precise coordinate systems. While current outdoor methods can guide agents to the vicinity of a target using coarse-grained localization, they fail to enable fine-grained entry through specific building entrances, critically limiting their utility in practical deployment scenarios that require seamless outdoor-to-indoor transitions. To bridge this gap, we introduce a novel task: out-to-in prior-free instruction-driven embodied navigation. This formulation explicitly eliminates reliance on accurate external priors, requiring agents to navigate solely based on egocentric visual observations guided by instructions. To tackle this task, we propose a vision-centric embodied navigation framework that leverages image-based prompts to drive decision-making. Additionally, we present the first open-source dataset for this task, featuring a pipeline that integrates trajectory-conditioned video synthesis into the data generation process. Through extensive experiments, we demonstrate that our proposed method consistently outperforms state-of-the-art baselines across key metrics including success rate and path efficiency.</p>
      <h2>摘要 (ZH)</h2>
      <p>具身导航在最后一公里配送等现实应用中具有重要前景。然而，现有方法大多局限于室内或室外单一环境，并严重依赖精确坐标系等强假设。当前室外方法虽能通过粗粒度定位引导智能体接近目标，却无法实现通过特定建筑入口的细粒度进入，这在需要无缝室外到室内转换的实际部署场景中严重限制了其实用性。为弥合这一差距，我们提出一项新颖任务：无先验知识的室外到室内指令驱动具身导航。该框架明确摒弃对精确外部先验信息的依赖，要求智能体仅基于以指令引导的自我中心视觉观察进行导航。针对此任务，我们提出一种视觉中心化的具身导航框架，利用基于图像的提示驱动决策。此外，我们发布了该任务的首个开源数据集，其数据生成流程融合了轨迹条件视频合成技术。通过大量实验验证，我们提出的方法在成功率与路径效率等关键指标上均持续优于现有最先进基线模型。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
