<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Bridging the Indoor-Outdoor Gap: Vision-Centric Instruction-Guided Embodied Navigation for the Last Meters&lt;br&gt;弥合室内外鸿沟：面向最后几米的视觉中心化指令引导具身导航&lt;br&gt;[摘要](abstracts/2602.06427.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>Abstract not available.</p>
      <h2>摘要 (ZH)</h2>
      <p>具身导航在最后一公里配送等现实应用中具有重要前景。然而，现有方法大多局限于室内或室外单一环境，并严重依赖精确坐标系等强假设。当前室外方法虽能通过粗粒度定位引导智能体接近目标，却无法实现通过特定建筑入口的细粒度进入，这在需要无缝室外到室内转换的实际部署场景中严重限制了其实用性。为弥合这一差距，我们提出一项新颖任务：无先验知识的室外到室内指令驱动具身导航。该框架明确摒弃对精确外部先验信息的依赖，要求智能体仅基于以指令引导的自我中心视觉观察进行导航。针对此任务，我们提出一种视觉中心化的具身导航框架，利用基于图像的提示驱动决策。此外，我们发布了该任务的首个开源数据集，其数据生成流程融合了轨迹条件视频合成技术。通过大量实验验证，我们提出的方法在成功率与路径效率等关键指标上均持续优于现有最先进基线模型。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
