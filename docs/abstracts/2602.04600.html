<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data</h1>
      <h2>Abstract (EN)</h2>
      <p>Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios.</p>
      <h2>摘要 (ZH)</h2>
      <p>在无约束环境中实现泛化性操作要求机器人能够主动解决信息不确定性，即具备主动感知能力。然而，现有方法通常局限于有限的感知行为类型，限制了其在复杂环境中的适用性。本研究将主动感知形式化为一个由信息增益和决策分支驱动的非马尔可夫过程，并提出了视觉主动感知范式的结构化分类。基于这一视角，我们提出了CoMe-VLA框架——一种融合认知与记忆的视觉-语言-动作框架，该框架利用大规模人类第一人称数据来学习多功能的探索与操作先验。我们的框架集成了一个用于自主子任务转换的认知辅助头模块，以及一个通过融合本体感觉与视觉时序上下文来维持自我与环境一致感知的双轨记忆系统。通过将人类与机器人的手眼协调行为对齐到统一的第一人称动作空间中，我们分三个阶段逐步训练模型。在轮式人形机器人上进行的广泛实验表明，该方法在跨越多种主动感知场景的多样化长时程任务中展现出强大的鲁棒性与适应性。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
