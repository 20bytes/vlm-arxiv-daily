<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation</h1>
      <h2>Abstract (EN)</h2>
      <p>Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.</p>
      <h2>摘要 (ZH)</h2>
      <p>为何视觉语言导航必须依赖于详尽冗长的语言指令？尽管这些细节有助于决策制定，但它们从根本上违背了现实世界导航的目标。理想情况下，智能体应具备自主性，仅凭简单高层的意图引导在未知环境中导航。实现这一愿景带来了严峻挑战：超视距导航，即智能体必须在缺乏密集逐步指导的情况下定位远处不可见的目标。现有基于大语言模型的方法虽擅长遵循密集指令，但由于依赖短视距监督，常表现出短视行为。然而，单纯扩展监督视距会破坏大语言模型训练的稳定性。本研究发现，视频生成模型天生受益于长视距监督以对齐语言指令，使其特别适用于超视距导航任务。基于这一洞见，我们首次将视频生成模型引入该领域。然而，生成长达数十秒视频的过高延迟使其难以实际部署。为弥合这一差距，我们提出SparseVideoNav，通过生成跨越20秒视距的稀疏未来轨迹，实现亚秒级轨迹推断，相比未优化版本获得惊人的27倍加速。大量现实世界零样本实验表明，SparseVideoNav在超视距导航任务上的成功率达到最先进大语言模型基线的2.5倍，并首次在极具挑战性的夜间场景中实现了此类能力。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
