<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>JEPA-VLA: Video Predictive Embedding is Needed for VLA Models&lt;br&gt;JEPA-VLA：视觉语言动作模型需要视频预测性嵌入&lt;br&gt;[摘要](abstracts/2602.11832.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>Abstract not available.</p>
      <h2>摘要 (ZH)</h2>
      <p>近年来，基于预训练视觉语言模型构建的视觉语言动作模型在机器人操作领域取得了显著进展。然而，当前的视觉语言动作模型仍面临样本效率低下和泛化能力有限的问题。本文认为，这些局限性与一个被忽视的组件——预训练视觉表征密切相关，该组件在环境理解和策略先验两方面均提供不足的知识。通过深入分析，我们发现视觉语言动作模型中常用的视觉表征，无论是通过语言-图像对比学习还是基于图像的自监督学习预训练，在捕捉关键任务相关的环境信息以及诱导有效的策略先验（即成功执行任务时环境如何演变的预见性知识）方面仍存在不足。相比之下，我们发现基于视频预训练的预测性嵌入，特别是V-JEPA 2，能够灵活剔除不可预测的环境因素，并编码任务相关的时间动态，从而有效弥补现有视觉语言动作模型中视觉表征的关键缺陷。基于这些观察，我们提出了JEPA-VLA，一种简单而有效的方法，能够自适应地将预测性嵌入集成到现有的视觉语言动作模型中。实验表明，JEPA-VLA在包括LIBERO、LIBERO-plus、RoboTwin2.0以及真实机器人任务在内的一系列基准测试中均带来了显著的性能提升。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
