<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>RetouchIQ: MLLM Agents for Instruction-Based Image Retouching with Generalist Reward</h1>
      <h2>Abstract (EN)</h2>
      <p>Recent advances in multimodal large language models (MLLMs) have shown great potential for extending vision-language reasoning to professional tool-based image editing, enabling intuitive and creative editing. A promising direction is to use reinforcement learning (RL) to enable MLLMs to reason about and execute optimal tool-use plans within professional image-editing software. However, training remains challenging due to the lack of reliable, verifiable reward signals that can reflect the inherently subjective nature of creative editing. In this work, we introduce RetouchIQ, a framework that performs instruction-based executable image editing through MLLM agents guided by a generalist reward model. RetouchIQ interprets user-specified editing intentions and generates corresponding, executable image adjustments, bridging high-level aesthetic goals with precise parameter control. To move beyond conventional, rule-based rewards that compute similarity against a fixed reference image using handcrafted metrics, we propose a generalist reward model, an RL fine-tuned MLLM that evaluates edited results through a set of generated metrics on a case-by-case basis. Then, the reward model provides scalar feedback through multimodal reasoning, enabling reinforcement learning with high-quality, instruction-consistent gradients. We curate an extended dataset with 190k instruction-reasoning pairs and establish a new benchmark for instruction-based image editing. Experiments show that RetouchIQ substantially improves both semantic consistency and perceptual quality over previous MLLM-based and diffusion-based editing systems. Our findings demonstrate the potential of generalist reward-driven MLLM agents as flexible, explainable, and executable assistants for professional image editing.</p>
      <h2>摘要 (ZH)</h2>
      <p>多模态大语言模型（MLLMs）的最新进展，在将视觉-语言推理能力扩展至基于专业工具的图像编辑领域方面展现出巨大潜力，实现了直观且富有创意的编辑方式。一个颇具前景的方向是利用强化学习（RL）使MLLMs能够在专业图像编辑软件中推理并执行最优工具使用方案。然而，由于缺乏能够反映创意编辑固有主观性的可靠、可验证的奖励信号，训练过程仍面临挑战。本研究提出RetouchIQ框架，该框架通过由通用奖励模型引导的MLLM智能体，实现基于指令的可执行图像编辑。RetouchIQ能够解读用户指定的编辑意图，并生成相应的可执行图像调整方案，从而在高层审美目标与精确参数控制之间建立桥梁。为超越传统基于规则、使用手工设计指标计算与固定参考图像相似度的奖励机制，我们提出一种通用奖励模型——一种经过RL微调的MLLM，能够根据具体情况通过一组生成的指标评估编辑结果。随后，该奖励模型通过多模态推理提供标量反馈，从而实现具有高质量、指令一致梯度的强化学习。我们构建了一个包含19万条指令-推理对的数据集，并建立了基于指令的图像编辑新基准。实验表明，RetouchIQ在语义一致性和感知质量上均显著优于以往基于MLLM和扩散模型的编辑系统。我们的研究结果证明了通用奖励驱动的MLLM智能体作为专业图像编辑灵活、可解释且可执行的辅助工具的潜力。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
