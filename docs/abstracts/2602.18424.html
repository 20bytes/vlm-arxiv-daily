<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation</h1>
      <h2>Abstract (EN)</h2>
      <p>Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent&#x27;s mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent&#x27;s specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM&#x27;s navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav</p>
      <h2>摘要 (ZH)</h2>
      <p>视觉语言模型在视觉语言导航领域取得了显著进展，为导航决策提供了新的可能性，有望惠及机器人平台和人类用户。然而，现实世界的导航本质上受限于智能体的移动能力约束。例如，扫地机器人无法跨越楼梯，而四足机器人则可以。我们提出了能力条件导航基准，旨在评估视觉语言模型在给定智能体特定物理和操作能力的情况下，在复杂室内空间中导航的表现。该基准定义了五个代表性的人类和机器人智能体，每个智能体均描述了其物理尺寸、移动能力及环境交互能力。基准包含45个真实室内场景、473项导航任务和2365个问答对，以测试视觉语言模型是否能基于智能体能力在室内环境中行进。我们对13个现代视觉语言模型进行了评估，发现当前模型的导航性能随着移动约束的收紧而急剧下降，即使最先进的模型在处理需要空间维度推理的障碍类型时也面临困难。最后，我们讨论了能力感知导航的意义，以及未来视觉语言模型在具身空间推理方面的发展机遇。该基准可在https://github.com/makeabilitylab/CapNav获取。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
