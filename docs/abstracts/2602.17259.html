<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>FRAPPE: Infusing World Modeling into Generalist Policies via Multiple Future Representation Alignment</h1>
      <h2>Abstract (EN)</h2>
      <p>Enabling VLA models to predict environmental dynamics, known as world modeling, has been recognized as essential for improving robotic reasoning and generalization. However, current approaches face two main issues: 1. The training objective forces models to over-emphasize pixel-level reconstruction, which constrains semantic learning and generalization 2. Reliance on predicted future observations during inference often leads to error accumulation. To address these challenges, we introduce Future Representation Alignment via Parallel Progressive Expansion (FRAPPE). Our method adopts a two-stage fine-tuning strategy: In the mid-training phase, the model learns to predict the latent representations of future observations; In the post-training phase, we expand the computational workload in parallel and align the representation simultaneously with multiple different visual foundation models. By significantly improving fine-tuning efficiency and reducing dependence on action-annotated data, FRAPPE provides a scalable and data-efficient pathway to enhance world-awareness in generalist robotic policies. Experiments on the RoboTwin benchmark and real-world tasks demonstrate that FRAPPE outperforms state-of-the-art approaches and shows strong generalization in long-horizon and unseen scenarios.</p>
      <h2>摘要 (ZH)</h2>
      <p>使视觉语言动作（VLA）模型能够预测环境动态（即世界建模）已被视为提升机器人推理与泛化能力的关键。然而，现有方法面临两大问题：1. 训练目标迫使模型过度关注像素级重建，限制了语义学习与泛化能力；2. 推理时依赖预测的未来观测常导致误差累积。为应对这些挑战，我们提出了并行渐进扩展的未来表示对齐方法（FRAPPE）。该方法采用两阶段微调策略：在中期训练阶段，模型学习预测未来观测的潜在表示；在后训练阶段，我们并行扩展计算负载，并同时与多个不同的视觉基础模型进行表示对齐。通过显著提升微调效率并减少对动作标注数据的依赖，FRAPPE为增强通用机器人策略的世界感知能力提供了一条可扩展且数据高效的路径。在RoboTwin基准测试和真实世界任务上的实验表明，FRAPPE优于现有先进方法，并在长时程和未见场景中展现出强大的泛化能力。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
