<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models</h1>
      <h2>Abstract (EN)</h2>
      <p>Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases. Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail. To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution. It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens. Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation. The framework maintains highly competitive detection accuracy (at least 99.36\% AUC) and robust attack resilience (at least 88.61\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking.</p>
      <h2>摘要 (ZH)</h2>
      <p>水印技术已成为大型视觉语言模型（LVLMs）中内容溯源与知识产权保护的关键解决方案。然而，视觉无关的水印可能引入视觉上无关的标记，并通过施加无差别的伪随机偏差破坏视觉基础。此外，现有的视觉专用水印依赖于对视觉关键权重的静态一次性估计，并在确定受保护标记比例时忽略了权重分布密度。这种设计未能考虑生成过程中视觉依赖性的动态变化，并可能在长尾部分引入低质量标记。为解决这些挑战，我们提出了注意力引导动态水印（AGMark），这是一种新颖的框架，可在严格保持视觉保真度的同时嵌入可检测信号。在每个解码步骤中，AGMark首先基于注意力权重动态识别视觉相关性的语义关键证据，并结合上下文感知的连贯性线索，从而产生更自适应且校准良好的证据权重分布。随后，它通过联合考虑不确定性感知（标记熵）和证据校准（权重密度）来确定语义关键标记的比例，从而实现自适应词汇划分以避免无关标记。实证结果证实，AGMark优于传统方法，显著提升了生成质量，并在生成后期阶段尤其增强了视觉语义保真度。该框架在保持高度竞争力的检测准确率（至少99.36% AUC）和强大的攻击鲁棒性（至少88.61% AUC）的同时，未牺牲推理效率，有效确立了可靠性保持的多模态水印新标准。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
