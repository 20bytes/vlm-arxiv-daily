<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>World-Gymnast: Training Robots with Reinforcement Learning in a World Model</h1>
      <h2>Abstract (EN)</h2>
      <p>Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone&#x27;s household.</p>
      <h2>摘要 (ZH)</h2>
      <p>机器人通过与物理世界交互进行学习，从根本上受到物理交互成本的制约。两种替代方案——基于专家演示的监督微调（SFT）和基于软件模拟器的强化学习（RL）——分别受限于可用专家数据的数量以及操作任务中的仿真到现实差距。随着近期从真实世界视频-动作数据中学习的世界模型的出现，我们提出一个问题：在世界模型中训练策略是否比监督学习或软件仿真更能有效提升真实机器人的性能。我们提出了World-Gymnast方法，该方法通过在动作条件化的视频世界模型中展开策略，并利用视觉语言模型（VLM）对展开过程进行奖励，从而对视觉语言动作（VLA）策略进行强化学习微调。在Bridge机器人实验平台上，World-Gymnast的性能比SFT最高提升18倍，比软件模拟器最高提升2倍。更重要的是，World-Gymnast展示了基于世界模型的强化学习的引人注目的能力，包括在世界模型中对多样化语言指令和新场景进行训练、在新场景中进行测试时训练，以及在线迭代优化世界模型和策略。我们的结果表明，学习世界模型并在云端训练机器人策略，可能是弥合仅能在演示中工作的机器人与能在任何家庭中工作的机器人之间差距的关键。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
