<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking</h1>
      <h2>Abstract (EN)</h2>
      <p>Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.</p>
      <h2>摘要 (ZH)</h2>
      <p>视频推理需要理解视频中事件之间的因果关系，然而这些关系通常是隐含的，且人工标注成本高昂。现有的多模态大语言模型（MLLMs）通常通过密集描述或视频摘要来推断事件关系以进行视频推理，但这种建模仍缺乏因果理解。由于缺乏对视频事件内部及跨事件因果结构的显式建模，这些模型在视频推理过程中容易出现幻觉。本文提出GraphThinker，一种基于强化微调的方法，通过构建结构化的事件级场景图并增强视觉基础，共同减少视频推理中的幻觉。具体而言，我们首先利用MLLM构建基于事件的视频场景图（EVSG），显式建模事件内部及事件间的关系，并将这些形成的场景图作为中间思维过程融入MLLM中。在强化微调过程中，我们还引入了视觉注意力奖励机制，以加强视频基础并进一步缓解幻觉。我们在RexTime和VidHalluc两个数据集上评估GraphThinker，结果显示相较于现有方法，它能够更准确地捕捉对象和事件关系，实现更精确的事件定位，从而有效减少视频推理中的幻觉。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
