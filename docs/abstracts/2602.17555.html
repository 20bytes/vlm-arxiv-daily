<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking&lt;br&gt;GraphThinker：通过事件图思维强化视频推理&lt;br&gt;[摘要](abstracts/2602.17555.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>Abstract not available.</p>
      <h2>摘要 (ZH)</h2>
      <p>视频推理需要理解视频中事件之间的因果关系，然而这些关系通常是隐含的，且人工标注成本高昂。现有的多模态大语言模型（MLLMs）通常通过密集描述或视频摘要来推断事件关系以进行视频推理，但这种建模仍缺乏因果理解。由于缺乏对视频事件内部及跨事件因果结构的显式建模，这些模型在视频推理过程中容易出现幻觉。本文提出GraphThinker，一种基于强化微调的方法，通过构建结构化的事件级场景图并增强视觉基础，共同减少视频推理中的幻觉。具体而言，我们首先利用MLLM构建基于事件的视频场景图（EVSG），显式建模事件内部及事件间的关系，并将这些形成的场景图作为中间思维过程融入MLLM中。在强化微调过程中，我们还引入了视觉注意力奖励机制，以加强视频基础并进一步缓解幻觉。我们在RexTime和VidHalluc两个数据集上评估GraphThinker，结果显示相较于现有方法，它能够更准确地捕捉对象和事件关系，实现更精确的事件定位，从而有效减少视频推理中的幻觉。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
