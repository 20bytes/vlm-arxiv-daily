<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>One Agent to Guide Them All: Empowering MLLMs for Vision-and-Language Navigation via Explicit World Representation</h1>
      <h2>Abstract (EN)</h2>
      <p>A navigable agent needs to understand both high-level semantic instructions and precise spatial perceptions. Building navigation agents centered on Multimodal Large Language Models (MLLMs) demonstrates a promising solution due to their powerful generalization ability. However, the current tightly coupled design dramatically limits system performance. In this work, we propose a decoupled design that separates low-level spatial state estimation from high-level semantic planning. Unlike previous methods that rely on predefined, oversimplified textual maps, we introduce an interactive metric world representation that maintains rich and consistent information, allowing MLLMs to interact with and reason on it for decision-making. Furthermore, counterfactual reasoning is introduced to further elicit MLLMs&#x27; capacity, while the metric world representation ensures the physical validity of the produced actions. We conduct comprehensive experiments in both simulated and real-world environments. Our method establishes a new zero-shot state-of-the-art, achieving 48.8\% Success Rate (SR) in R2R-CE and 42.2\% in RxR-CE benchmarks. Furthermore, to validate the versatility of our metric representation, we demonstrate zero-shot sim-to-real transfer across diverse embodiments, including a wheeled TurtleBot 4 and a custom-built aerial drone. These real-world deployments verify that our decoupled framework serves as a robust, domain-invariant interface for embodied Vision-and-Language navigation.</p>
      <h2>摘要 (ZH)</h2>
      <p>可导航智能体需同时理解高层语义指令与精确空间感知。基于多模态大语言模型（MLLMs）构建导航智能体因其强大的泛化能力展现出广阔前景，但当前紧耦合的设计严重制约了系统性能。本研究提出一种解耦设计，将低层空间状态估计与高层语义规划分离。区别于以往依赖预定义、过度简化的文本地图的方法，我们引入一种交互式度量世界表征，该表征能保持丰富且一致的信息，使MLLMs能够与之交互并进行推理决策。此外，通过引入反事实推理进一步激发MLLMs的潜能，而度量世界表征确保了生成动作的物理有效性。我们在仿真与真实环境中进行了全面实验：本方法在零样本条件下刷新了最佳性能，在R2R-CE和RxR-CE基准测试中分别达到48.8%和42.2%的成功率。为验证度量表征的普适性，我们展示了跨多样实体（包括轮式TurtleBot 4机器人及定制空中无人机）的零样本仿真到现实迁移能力。这些真实场景部署证实，我们的解耦框架为具身视觉与语言导航提供了鲁棒且领域无关的接口。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
