<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning&lt;br&gt;迈向开放环境与指令：基于快慢交互推理的通用视觉语言导航&lt;br&gt;[摘要](abstracts/2601.09111.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>Abstract not available.</p>
      <h2>摘要 (ZH)</h2>
      <p>视觉语言导航旨在使智能体能够根据语言指令导航至目标位置。传统的视觉语言导航通常遵循封闭集假设，即训练与测试数据共享相同风格的输入图像和指令。然而，现实世界是开放的，充满了各种未见过的环境，这对封闭集方法构成了巨大挑战。为此，我们聚焦于通用场景适应任务，旨在通过引入多样化的环境和不一致的指令来学习泛化的导航能力。面对这一任务，当遭遇未见环境和指令时，主要挑战在于如何使智能体在导航过程中动态生成泛化策略。近期研究表明，人类通过快慢认知系统能够生成稳定的策略，从而增强对开放世界的适应能力。受此启发，我们提出了慢促快视觉语言导航方法，构建了一个动态交互的快慢推理框架。其中，快速推理模块作为一个端到端的策略网络，通过实时输入输出动作，并在历史存储库中积累执行记录以构建记忆。慢速推理模块则分析快速推理模块生成的记忆，通过深度反思提取能够增强决策泛化能力的经验。这些经验被结构化存储，并用于持续优化快速推理模块。与将快慢推理视为独立机制的传统方法不同，我们的框架实现了快慢交互。通过利用慢速推理产生的经验，这种交互使系统能够持续适应，并在面对未见场景时高效执行导航任务。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
