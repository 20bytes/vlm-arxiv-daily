<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Concept-Based Dictionary Learning for Inference-Time Safety in Vision Language Action Models&lt;br&gt;面向视觉语言动作模型推理时安全性的概念词典学习方法&lt;br&gt;[摘要](abstracts/2602.01834.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>Abstract not available.</p>
      <h2>摘要 (ZH)</h2>
      <p>视觉语言动作（VLA）模型通过将多模态指令转化为可执行行为，实现了感知-动作闭环，但这一能力也放大了安全风险：在大型语言模型中仅产生有害文本的越狱攻击，可能在具身系统中触发不安全的物理行为。现有防御方法（如对齐、过滤或提示强化）干预过晚或针对错误模态，导致融合后的表征仍可被利用。我们提出了一种基于概念的词典学习框架，用于推理时的安全控制。该方法通过从隐藏层激活中构建稀疏、可解释的词典，识别有害概念方向，并应用基于阈值的干预来抑制或阻断不安全激活。在Libero-Harm、BadRobot、RoboPair和IS-Bench上的实验表明，我们的方法实现了最先进的防御性能，将攻击成功率降低超过70%，同时保持任务成功率。关键的是，该框架为即插即用且模型无关，无需重新训练，并能与多种VLA模型无缝集成。据我们所知，这是首个面向具身系统的推理时基于概念的安全方法，推动了VLA模型的可解释性与安全部署。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
