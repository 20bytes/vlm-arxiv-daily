<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization&lt;br&gt;LongVPO：从锚定线索到自我推理的长视频偏好优化&lt;br&gt;[摘要](abstracts/2602.02341.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model&#x27;s scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model&#x27;s preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.</p>
      <h2>摘要 (ZH)</h2>
      <p>我们提出了LongVPO，一种新颖的两阶段直接偏好优化框架，使短上下文视觉语言模型能够稳健地理解超长视频，无需任何长视频标注。在第一阶段，我们通过将问题锚定到单个短视频片段、与干扰项交错排列，并应用视觉相似性和问题特异性过滤来合成偏好三元组，以减轻位置偏差并确保明确的监督。我们还通过仅评估锚定片段来近似参考模型在长上下文中的评分，从而降低计算开销。在第二阶段，我们在长视频上采用递归字幕生成流程来生成场景级元数据，然后使用大型语言模型构建多片段推理查询和不受偏好的响应，通过多片段推理任务来对齐模型的偏好。仅使用16K个合成示例且无需昂贵的人工标注，LongVPO在多个长视频基准测试中超越了最先进的开源模型，同时保持了强大的短视频性能（例如在MVBench上），为高效的长视频理解提供了一个可扩展的范式。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
