<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>EmbodMocap: In-the-Wild 4D Human-Scene Reconstruction for Embodied Agents</h1>
      <h2>Abstract (EN)</h2>
      <p>Human behaviors in the real world naturally encode rich, long-term contextual information that can be leveraged to train embodied agents for perception, understanding, and acting. However, existing capture systems typically rely on costly studio setups and wearable devices, limiting the large-scale collection of scene-conditioned human motion data in the wild. To address this, we propose EmbodMocap, a portable and affordable data collection pipeline using two moving iPhones. Our key idea is to jointly calibrate dual RGB-D sequences to reconstruct both humans and scenes within a unified metric world coordinate frame. The proposed method allows metric-scale and scene-consistent capture in everyday environments without static cameras or markers, bridging human motion and scene geometry seamlessly. Compared with optical capture ground truth, we demonstrate that the dual-view setting exhibits a remarkable ability to mitigate depth ambiguity, achieving superior alignment and reconstruction performance over single iphone or monocular models. Based on the collected data, we empower three embodied AI tasks: monocular human-scene-reconstruction, where we fine-tune on feedforward models that output metric-scale, world-space aligned humans and scenes; physics-based character animation, where we prove our data could be used to scale human-object interaction skills and scene-aware motion tracking; and robot motion control, where we train a humanoid robot via sim-to-real RL to replicate human motions depicted in videos. Experimental results validate the effectiveness of our pipeline and its contributions towards advancing embodied AI research.</p>
      <h2>摘要 (ZH)</h2>
      <p>现实世界中的人类行为自然编码了丰富、长期的上下文信息，可用于训练具身智能体进行感知、理解与行动。然而，现有的捕捉系统通常依赖昂贵的演播室设置和可穿戴设备，限制了野外场景条件下大规模人体运动数据的采集。为解决此问题，我们提出EmbodMocap——一种使用两部移动iPhone的便携、经济型数据采集流程。其核心思想是通过联合标定双RGB-D序列，在统一的度量世界坐标系中重建人体与场景。该方法无需静态相机或标记物，即可在日常环境中实现度量尺度且场景一致的捕捉，无缝衔接人体运动与场景几何。与光学捕捉真值对比表明，双视角设置展现出显著缓解深度模糊的能力，在人体对齐与重建性能上优于单iPhone或单目模型。基于采集的数据，我们赋能了三项具身人工智能任务：单目人-场景重建——通过微调前馈模型输出度量尺度、世界空间对齐的人体与场景；基于物理的角色动画——证明数据可用于扩展人-物交互技能与场景感知运动追踪；机器人运动控制——通过仿真到现实的强化学习训练人形机器人复现视频中的人类动作。实验结果验证了本流程的有效性及其对推动具身人工智能研究的贡献。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
