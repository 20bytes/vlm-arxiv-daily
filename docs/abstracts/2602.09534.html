<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>AUHead: Realistic Emotional Talking Head Generation via Action Units Control</h1>
      <h2>Abstract (EN)</h2>
      <p>Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage method (AUHead) to disentangle fine-grained emotion control, i.e. , Action Units (AUs), from audio and achieve controllable generation. In the first stage, we explore the AU generation abilities of large audio-language models (ALMs), by spatial-temporal AU tokenization and an &quot;emotion-then-AU&quot; chain-of-thought mechanism. It aims to disentangle AUs from raw speech, effectively capturing subtle emotional cues. In the second stage, we propose an AU-driven controllable diffusion model that synthesizes realistic talking-head videos conditioned on AU sequences. Specifically, we first map the AU sequences into the structured 2D facial representation to enhance spatial fidelity, and then model the AU-vision interaction within cross-attention modules. To achieve flexible AU-quality trade-off control, we introduce an AU disentanglement guidance strategy during inference, further refining the emotional expressiveness and identity consistency of the generated videos. Results on benchmark datasets demonstrate that our approach achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence, significantly surpassing existing techniques. Our implementation is available at https://github.com/laura990501/AUHead_ICLR</p>
      <h2>摘要 (ZH)</h2>
      <p>逼真的说话头部视频生成对于虚拟化身、电影制作和交互系统至关重要。现有方法因缺乏细粒度情感控制而难以实现细腻的情感表达。为解决此问题，我们提出了一种新颖的两阶段方法（AUHead），以从音频中解耦细粒度情感控制（即动作单元，AUs），并实现可控生成。在第一阶段，我们探索了大型音频-语言模型（ALMs）的AU生成能力，通过时空AU标记化和“先情感后AU”的思维链机制，旨在从原始语音中解耦AUs，有效捕捉微妙的情感线索。在第二阶段，我们提出了一种AU驱动的可控扩散模型，该模型基于AU序列合成逼真的说话头部视频。具体而言，我们首先将AU序列映射为结构化的二维面部表示以增强空间保真度，然后在交叉注意力模块中建模AU与视觉的交互。为实现灵活的AU-质量权衡控制，我们在推理过程中引入了AU解耦引导策略，进一步优化生成视频的情感表现力和身份一致性。在基准数据集上的结果表明，我们的方法在情感真实性、准确的唇部同步和视觉连贯性方面实现了有竞争力的性能，显著超越了现有技术。我们的实现代码可在https://github.com/laura990501/AUHead_ICLR获取。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
