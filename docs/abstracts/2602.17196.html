<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models</h1>
      <h2>Abstract (EN)</h2>
      <p>Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an &quot;Entropy Collapse Layer&quot; (ECL), where the information content of visual representations exhibits a sharp and consistent drop, which provides a principled criterion for selecting the pruning stage. Building on this observation, we propose EntropyPrune, a novel matrix-entropy-guided token pruning framework that quantifies the information value of individual visual tokens and prunes redundant ones without relying on attention maps. Moreover, to enable efficient computation, we exploit the spectral equivalence of dual Gram matrices, reducing the complexity of entropy computation and yielding up to a 64x theoretical speedup. Extensive experiments on diverse multimodal benchmarks demonstrate that EntropyPrune consistently outperforms state-of-the-art pruning methods in both accuracy and efficiency. On LLaVA-1.5-7B, our method achieves a 68.2% reduction in FLOPs while preserving 96.0% of the original performance. Furthermore, EntropyPrune generalizes effectively to high-resolution and video-based models, highlighting the strong robustness and scalability in practical MLLM acceleration. The code will be publicly available at https://github.com/YahongWang1/EntropyPrune.</p>
      <h2>摘要 (ZH)</h2>
      <p>多模态大语言模型（MLLMs）因每张图像需处理数百个视觉令牌而产生高昂推理成本。尽管令牌剪枝已被证明能有效加速推理，但何时何地进行剪枝仍主要依赖启发式方法。现有方法通常基于静态、经验选择的层，这限制了其可解释性和跨模型的可迁移性。本研究引入矩阵熵视角，识别出“熵坍缩层”（ECL），即视觉表示的信息内容在此层出现急剧且一致的下降，从而为选择剪枝阶段提供了原则性准则。基于此观察，我们提出EntropyPrune，一种新颖的矩阵熵引导令牌剪枝框架，该框架量化单个视觉令牌的信息价值，并在不依赖注意力图的情况下剪除冗余令牌。此外，为实现高效计算，我们利用对偶格拉姆矩阵的谱等价性，降低了熵计算的复杂度，理论加速比最高可达64倍。在多样化多模态基准上的广泛实验表明，EntropyPrune在准确性和效率上均持续优于最先进的剪枝方法。在LLaVA-1.5-7B模型上，我们的方法实现了68.2%的浮点运算量（FLOPs）减少，同时保持了96.0%的原始性能。此外，EntropyPrune能有效泛化至高分辨率和基于视频的模型，凸显了其在实用MLLM加速中的强鲁棒性和可扩展性。代码将公开于https://github.com/YahongWang1/EntropyPrune。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
