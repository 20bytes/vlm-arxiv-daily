<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?</h1>
      <h2>Abstract (EN)</h2>
      <p>Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.</p>
      <h2>摘要 (ZH)</h2>
      <p>视觉语言模型（VLMs）在跨文本与视觉输入的多模态理解方面取得了显著成就，但现有基准测试主要关注纯文本查询。在现实场景中，语言也常以图像中嵌入的可视化文本形式出现，这引发了一个问题：当前的VLMs是否能同等处理这类输入请求。我们推出了VISTA-Bench，这是一个从多模态感知、推理到单模态理解领域的系统性基准测试。它通过在受控渲染条件下对比纯文本与可视化文本问题，评估模型对可视化文本的理解能力。对超过20个代表性VLMs的广泛评估揭示了一个显著的模态差距：在纯文本查询上表现良好的模型，当相同语义内容以可视化文本呈现时，性能往往大幅下降。这种差距随着感知难度的增加而进一步放大，突显了模型对渲染变化的敏感性，尽管语义内容保持不变。总体而言，VISTA-Bench提供了一个原则性的评估框架，用于诊断这一局限性，并指导在标记化文本与像素之间实现更统一语言表征的进展。源数据集可在https://github.com/QingAnLiu/VISTA-Bench获取。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
