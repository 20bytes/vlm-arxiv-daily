<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?&lt;br&gt;VISTA-Bench：视觉语言模型真的能像理解纯文本一样理解图像中的文本吗？&lt;br&gt;[摘要](abstracts/2602.04802.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>Abstract not available.</p>
      <h2>摘要 (ZH)</h2>
      <p>视觉语言模型（VLMs）在跨文本与视觉输入的多模态理解方面取得了显著成就，但现有基准测试主要关注纯文本查询。在现实场景中，语言也常以图像中嵌入的可视化文本形式出现，这引发了一个问题：当前的VLMs是否能同等处理这类输入请求。我们推出了VISTA-Bench，这是一个从多模态感知、推理到单模态理解领域的系统性基准测试。它通过在受控渲染条件下对比纯文本与可视化文本问题，评估模型对可视化文本的理解能力。对超过20个代表性VLMs的广泛评估揭示了一个显著的模态差距：在纯文本查询上表现良好的模型，当相同语义内容以可视化文本呈现时，性能往往大幅下降。这种差距随着感知难度的增加而进一步放大，突显了模型对渲染变化的敏感性，尽管语义内容保持不变。总体而言，VISTA-Bench提供了一个原则性的评估框架，用于诊断这一局限性，并指导在标记化文本与像素之间实现更统一语言表征的进展。源数据集可在https://github.com/QingAnLiu/VISTA-Bench获取。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
