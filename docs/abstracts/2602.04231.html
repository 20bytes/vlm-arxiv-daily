<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>GeoLanG: Geometry-Aware Language-Guided Grasping with Unified RGB-D Multimodal Learning</h1>
      <h2>Abstract (EN)</h2>
      <p>Language-guided grasping has emerged as a promising paradigm for enabling robots to identify and manipulate target objects through natural language instructions, yet it remains highly challenging in cluttered or occluded scenes. Existing methods often rely on multi-stage pipelines that separate object perception and grasping, which leads to limited cross-modal fusion, redundant computation, and poor generalization in cluttered, occluded, or low-texture scenes. To address these limitations, we propose GeoLanG, an end-to-end multi-task framework built upon the CLIP architecture that unifies visual and linguistic inputs into a shared representation space for robust semantic alignment and improved generalization. To enhance target discrimination under occlusion and low-texture conditions, we explore a more effective use of depth information through the Depth-guided Geometric Module (DGGM), which converts depth into explicit geometric priors and injects them into the attention mechanism without additional computational overhead. In addition, we propose Adaptive Dense Channel Integration, which adaptively balances the contributions of multi-layer features to produce more discriminative and generalizable visual representations. Extensive experiments on the OCID-VLG dataset, as well as in both simulation and real-world hardware, demonstrate that GeoLanG enables precise and robust language-guided grasping in complex, cluttered environments, paving the way toward more reliable multimodal robotic manipulation in real-world human-centric settings.</p>
      <h2>摘要 (ZH)</h2>
      <p>语言引导抓取作为一种通过自然语言指令使机器人识别和操作目标对象的有前景范式，但在杂乱或遮挡场景中仍面临巨大挑战。现有方法通常依赖将物体感知与抓取分离的多阶段流程，导致跨模态融合有限、计算冗余，且在杂乱、遮挡或低纹理场景中泛化能力差。为应对这些局限，我们提出了GeoLanG，这是一个基于CLIP架构构建的端到端多任务框架，它将视觉和语言输入统一到共享表示空间中，以实现鲁棒的语义对齐和增强的泛化能力。为提升遮挡和低纹理条件下的目标辨别力，我们通过深度引导几何模块（DGGM）探索了更有效的深度信息利用方式，该模块将深度转换为显式几何先验，并在不增加额外计算开销的情况下将其注入注意力机制。此外，我们提出了自适应密集通道集成方法，可自适应平衡多层特征的贡献，以生成更具区分性和泛化能力的视觉表示。在OCID-VLG数据集以及仿真和真实硬件上的大量实验表明，GeoLanG能够在复杂、杂乱的环境中实现精确且鲁棒的语言引导抓取，为在真实世界以人为中心的环境中实现更可靠的多模态机器人操作铺平了道路。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
