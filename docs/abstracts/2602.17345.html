<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>What Breaks Embodied AI Security:LLM Vulnerabilities, CPS Flaws,or Something Else?</h1>
      <h2>Abstract (EN)</h2>
      <p>Embodied AI systems (e.g., autonomous vehicles, service robots, and LLM-driven interactive agents) are rapidly transitioning from controlled environments to safety critical real-world deployments. Unlike disembodied AI, failures in embodied intelligence lead to irreversible physical consequences, raising fundamental questions about security, safety, and reliability. While existing research predominantly analyzes embodied AI through the lenses of Large Language Model (LLM) vulnerabilities or classical Cyber-Physical System (CPS) failures, this survey argues that these perspectives are individually insufficient to explain many observed breakdowns in modern embodied systems. We posit that a significant class of failures arises from embodiment-induced system-level mismatches, rather than from isolated model flaws or traditional CPS attacks. Specifically, we identify four core insights that explain why embodied AI is fundamentally harder to secure: (i) semantic correctness does not imply physical safety, as language-level reasoning abstracts away geometry, dynamics, and contact constraints; (ii) identical actions can lead to drastically different outcomes across physical states due to nonlinear dynamics and state uncertainty; (iii) small errors propagate and amplify across tightly coupled perception-decision-action loops; and (iv) safety is not compositional across time or system layers, enabling locally safe decisions to accumulate into globally unsafe behavior. These insights suggest that securing embodied AI requires moving beyond component-level defenses toward system-level reasoning about physical risk, uncertainty, and failure propagation.</p>
      <h2>摘要 (ZH)</h2>
      <p>具身人工智能系统（如自动驾驶汽车、服务机器人和基于大语言模型的交互式智能体）正迅速从受控环境转向安全关键的实际部署。与非具身人工智能不同，具身智能的失败会导致不可逆转的物理后果，从而引发关于安全性、可靠性和稳健性的根本性问题。尽管现有研究主要从大语言模型漏洞或经典信息物理系统故障的角度分析具身人工智能，但本综述认为，这些视角单独来看均不足以解释现代具身系统中观察到的许多故障。我们提出，一类重要的故障源于具身化引发的系统级不匹配，而非孤立的模型缺陷或传统的信息物理系统攻击。具体而言，我们识别了四个核心见解，用以解释为何具身人工智能本质上更难保障安全：（i）语义正确性并不意味着物理安全，因为语言层面的推理抽象了几何、动力学和接触约束；（ii）由于非线性动力学和状态不确定性，相同的行动在不同物理状态下可能导致截然不同的结果；（iii）微小误差在紧密耦合的感知-决策-行动循环中传播并放大；（iv）安全性在时间或系统层级上不具备组合性，使得局部安全的决策可能累积为全局不安全的行为。这些见解表明，保障具身人工智能安全需要超越组件级防御，转向对物理风险、不确定性和故障传播的系统级推理。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
