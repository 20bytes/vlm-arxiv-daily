<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases</h1>
      <h2>Abstract (EN)</h2>
      <p>Multimodal large language models (MLLMs) are increasingly deployed in real-world systems, yet their safety under adversarial prompting remains underexplored. We present a two-phase evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red teamers. Phase 1 assessed GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus; Phase 2 evaluated their successors (GPT-5, Claude Sonnet 4.5, Pixtral Large, and Qwen Omni) yielding 82,256 human harm ratings. Large, persistent differences emerged across model families: Pixtral models were consistently the most vulnerable, whereas Claude models appeared safest due to high refusal rates. Attack success rates (ASR) showed clear alignment drift: GPT and Claude models exhibited increased ASR across generations, while Pixtral and Qwen showed modest decreases. Modality effects also shifted over time: text-only prompts were more effective in Phase 1, whereas Phase 2 produced model-specific patterns, with GPT-5 and Claude 4.5 showing near-equivalent vulnerability across modalities. These findings demonstrate that MLLM harmlessness is neither uniform nor stable across updates, underscoring the need for longitudinal, multimodal benchmarks to track evolving safety behaviour.</p>
      <h2>摘要 (ZH)</h2>
      <p>多模态大语言模型（MLLMs）正越来越多地部署于实际系统中，但其在对抗性提示下的安全性仍缺乏深入探究。我们采用一个由26名专业红队人员编写的固定基准测试集（包含726个对抗性提示），对MLLMs的有害性进行了两阶段评估。第一阶段评估了GPT-4o、Claude Sonnet 3.5、Pixtral 12B和Qwen VL Plus；第二阶段评估了它们的后续版本（GPT-5、Claude Sonnet 4.5、Pixtral Large和Qwen Omni），共获得82,256个人类有害性评分。不同模型系列间存在显著且持续的差异：Pixtral模型始终最为脆弱，而Claude模型因高拒绝率显得最安全。攻击成功率（ASR）显示出明显的对齐漂移现象：GPT和Claude模型在代际更新中ASR有所上升，而Pixtral和Qwen则呈现小幅下降。模态效应也随时间变化：第一阶段中纯文本提示更为有效，而第二阶段则出现模型特定的模式，GPT-5和Claude 4.5在不同模态下表现出近乎同等的脆弱性。这些发现表明，MLLMs的有害性在模型更新中既非一致也非稳定，凸显了需要建立纵向、多模态的基准测试来追踪其安全行为的演变。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
