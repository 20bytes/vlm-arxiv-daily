<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>AgenticLab: A Real-World Robot Agent Platform that Can See, Think, and Act&lt;br&gt;AgenticLab：一个能够观察、思考与行动的真实世界机器人智能体平台&lt;br&gt;[摘要](abstracts/2602.01662.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>Recent advances in large vision-language models (VLMs) have demonstrated generalizable open-vocabulary perception and reasoning, yet their real-robot manipulation capability remains unclear for long-horizon, closed-loop execution in unstructured, in-the-wild environments. Prior VLM-based manipulation pipelines are difficult to compare across different research groups&#x27; setups, and many evaluations rely on simulation, privileged state, or specially designed setups. We present AgenticLab, a model-agnostic robot agent platform and benchmark for open-world manipulation. AgenticLab provides a closed-loop agent pipeline for perception, task decomposition, online verification, and replanning. Using AgenticLab, we benchmark state-of-the-art VLM-based agents on real-robot tasks in unstructured environments. Our benchmark reveals several failure modes that offline vision-language tests (e.g., VQA and static image understanding) fail to capture, including breakdowns in multi-step grounding consistency, object grounding under occlusion and scene changes, and insufficient spatial reasoning for reliable manipulation. We will release the full hardware and software stack to support reproducible evaluation and accelerate research on general-purpose robot agents.</p>
      <h2>摘要 (ZH)</h2>
      <p>近年来，大型视觉语言模型（VLMs）在泛化性开放词汇感知与推理方面取得了显著进展，然而其在非结构化、真实世界环境中执行长时程、闭环操作的实际机器人操控能力仍不明确。现有的基于VLM的操控流程难以在不同研究团队的实验设置间进行比较，且多数评估依赖于仿真环境、特权状态或专门设计的实验条件。本文提出AgenticLab，一个模型无关的机器人智能体平台与开放世界操控基准。AgenticLab提供了一套闭环智能体流程，涵盖感知、任务分解、在线验证与重规划。借助AgenticLab，我们在非结构化环境中的真实机器人任务上对当前最先进的基于VLM的智能体进行了基准测试。我们的基准揭示了一系列离线视觉语言测试（如视觉问答与静态图像理解）未能捕捉的故障模式，包括多步语义落地一致性失效、遮挡与场景变化下的物体定位困难，以及空间推理能力不足以支持可靠操控等问题。我们将发布完整的硬件与软件栈，以支持可复现的评估，并加速通用机器人智能体的研究进程。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
