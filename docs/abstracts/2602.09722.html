<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Rethinking Visual-Language-Action Model Scaling: Alignment, Mixture, and Regularization</h1>
      <h2>Abstract (EN)</h2>
      <p>While Vision-Language-Action (VLA) models show strong promise for generalist robot control, it remains unclear whether -- and under what conditions -- the standard &quot;scale data&quot; recipe translates to robotics, where training data is inherently heterogeneous across embodiments, sensors, and action spaces. We present a systematic, controlled study of VLA scaling that revisits core training choices for pretraining across diverse robots. Using a representative VLA framework that combines a vision-language backbone with flow-matching, we ablate key design decisions under matched conditions and evaluate in extensive simulation and real-robot experiments. To improve the reliability of real-world results, we introduce a Grouped Blind Ensemble protocol that blinds operators to model identity and separates policy execution from outcome judgment, reducing experimenter bias. Our analysis targets three dimensions of VLA scaling. (1) Physical alignment: we show that a unified end-effector (EEF)-relative action representation is critical for robust cross-embodiment transfer. (2) Embodiment mixture: we find that naively pooling heterogeneous robot datasets often induces negative transfer rather than gains, underscoring the fragility of indiscriminate data scaling. (3) Training regularization: we observe that intuitive strategies, such as sensory dropout and multi-stage fine-tuning, do not consistently improve performance at scale. Together, this study challenge some common assumptions about embodied scaling and provide practical guidance for training large-scale VLA policies from diverse robotic data. Project website: https://research.beingbeyond.com/rethink_vla</p>
      <h2>摘要 (ZH)</h2>
      <p>尽管视觉-语言-动作（VLA）模型在通用机器人控制方面展现出巨大潜力，但标准化的“数据规模化”方法是否适用于机器人领域仍不明确，因为机器人训练数据在具体实现、传感器和动作空间上天然具有异质性。本文对VLA模型的规模化进行了系统且受控的研究，重新审视了跨不同机器人预训练的核心训练选择。我们采用一个代表性的VLA框架，将视觉-语言骨干网络与流匹配相结合，在匹配条件下消融关键设计决策，并通过广泛的仿真和真实机器人实验进行评估。为提高真实世界结果的可靠性，我们引入了分组盲测集成协议，该协议使操作员对模型身份不知情，并将策略执行与结果判断分离，从而减少实验者偏差。我们的分析聚焦于VLA规模化的三个维度：（1）物理对齐：研究表明，统一的末端执行器相对动作表示对于实现稳健的跨实现迁移至关重要。（2）实现混合：我们发现，简单混合异构机器人数据集往往导致负迁移而非性能提升，这凸显了不加区分的数据规模化的脆弱性。（3）训练正则化：我们观察到，直观的策略（如感官丢弃和多阶段微调）在大规模训练中并不能持续提升性能。总之，本研究挑战了关于具身智能规模化的一些常见假设，并为从多样化机器人数据中训练大规模VLA策略提供了实用指导。项目网站：https://research.beingbeyond.com/rethink_vla</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
