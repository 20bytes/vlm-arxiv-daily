<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>NavDreamer: Video Models as Zero-Shot 3D Navigators&lt;br&gt;NavDreamer：视频模型作为零样本三维导航器&lt;br&gt;[摘要](abstracts/2602.09765.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>Abstract not available.</p>
      <h2>摘要 (ZH)</h2>
      <p>以往的视觉-语言-动作模型在导航任务中面临关键限制：数据稀缺且多样，依赖劳动密集型收集，以及静态表示无法捕捉时间动态和物理规律。我们提出NavDreamer，一个基于视频的三维导航框架，利用生成式视频模型作为语言指令与导航轨迹之间的通用接口。我们的核心假设是，视频编码时空信息和物理动态的能力，结合互联网规模的数据可用性，能够在导航中实现强大的零样本泛化。为减轻生成预测的随机性，我们引入一种基于采样的优化方法，利用视觉语言模型对轨迹进行评分和选择。通过逆动力学模型，从生成的视频计划中解码出可执行的路径点以执行导航。为系统评估该范式在多种视频模型骨干上的表现，我们提出了一个涵盖物体导航、精确导航、空间定位、语言控制和场景推理的综合基准。大量实验证明了其在未见过的物体和环境中的稳健泛化能力，消融研究揭示，导航的高层决策特性使其特别适合基于视频的规划。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
