<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>NavDreamer: Video Models as Zero-Shot 3D Navigators</h1>
      <h2>Abstract (EN)</h2>
      <p>Previous Vision-Language-Action models face critical limitations in navigation: scarce, diverse data from labor-intensive collection and static representations that fail to capture temporal dynamics and physical laws. We propose NavDreamer, a video-based framework for 3D navigation that leverages generative video models as a universal interface between language instructions and navigation trajectories. Our main hypothesis is that video&#x27;s ability to encode spatiotemporal information and physical dynamics, combined with internet-scale availability, enables strong zero-shot generalization in navigation. To mitigate the stochasticity of generative predictions, we introduce a sampling-based optimization method that utilizes a VLM for trajectory scoring and selection. An inverse dynamics model is employed to decode executable waypoints from generated video plans for navigation. To systematically evaluate this paradigm in several video model backbones, we introduce a comprehensive benchmark covering object navigation, precise navigation, spatial grounding, language control, and scene reasoning. Extensive experiments demonstrate robust generalization across novel objects and unseen environments, with ablation studies revealing that navigation&#x27;s high-level decision-making nature makes it particularly suited for video-based planning.</p>
      <h2>摘要 (ZH)</h2>
      <p>以往的视觉-语言-动作模型在导航任务中面临关键限制：数据稀缺且多样，依赖劳动密集型收集，以及静态表示无法捕捉时间动态和物理规律。我们提出NavDreamer，一个基于视频的三维导航框架，利用生成式视频模型作为语言指令与导航轨迹之间的通用接口。我们的核心假设是，视频编码时空信息和物理动态的能力，结合互联网规模的数据可用性，能够在导航中实现强大的零样本泛化。为减轻生成预测的随机性，我们引入一种基于采样的优化方法，利用视觉语言模型对轨迹进行评分和选择。通过逆动力学模型，从生成的视频计划中解码出可执行的路径点以执行导航。为系统评估该范式在多种视频模型骨干上的表现，我们提出了一个涵盖物体导航、精确导航、空间定位、语言控制和场景推理的综合基准。大量实验证明了其在未见过的物体和环境中的稳健泛化能力，消融研究揭示，导航的高层决策特性使其特别适合基于视频的规划。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
