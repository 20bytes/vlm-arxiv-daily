<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning</h1>
      <h2>Abstract (EN)</h2>
      <p>Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard&#x27;s state-of-the-art performance and validate its excellent robustness and generalization capacity.</p>
      <h2>摘要 (ZH)</h2>
      <p>多模态大语言模型（MLLMs）通过统一的跨模态推理显著推动了视频虚假信息检测的发展，但其通常依赖于固定深度的推理过程，并对内部生成的假设过度信任，尤其在关键证据稀疏、碎片化或需外部验证的场景中表现不足。为应对这些局限，我们提出了FactGuard，一种用于视频虚假信息检测的智能体框架，它将验证过程构建为基于MLLMs的迭代推理流程。FactGuard能够显式评估任务模糊性，并选择性调用外部工具以获取关键证据，从而实现推理路径的渐进式优化。为进一步强化这一能力，我们引入了一种两阶段训练策略，结合了领域特定的智能体监督微调与决策感知的强化学习，以优化工具使用并校准风险敏感型决策。在FakeSV、FakeTT和FakeVV数据集上的大量实验表明，FactGuard实现了最先进的性能，并验证了其出色的鲁棒性和泛化能力。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
