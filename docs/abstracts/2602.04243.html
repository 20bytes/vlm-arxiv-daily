<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Viewpoint Matters: Dynamically Optimizing Viewpoints with Masked Autoencoder for Visual Manipulation&lt;br&gt;视角至关重要：利用掩码自编码器动态优化视觉操控的视角&lt;br&gt;[摘要](abstracts/2602.04243.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>Abstract not available.</p>
      <h2>摘要 (ZH)</h2>
      <p>机器人操控仍面临挑战，而模仿学习（IL）使机器人能够从专家演示中学习任务。当前的IL方法通常依赖于固定的相机设置，即相机被手动放置在静态位置，这极大地限制了系统的适应性和覆盖范围。受人类主动感知的启发——人类会动态调整视角以捕捉最相关且噪声最少的信息，我们提出了MAE-Select，一种用于单相机机器人系统中主动视角选择的新颖框架。MAE-Select充分利用了预训练的多视角掩码自编码器表示，并在每个时间块动态选择下一个最具信息量的视角，无需标注视角数据。大量实验表明，MAE-Select提升了单相机系统的能力，在某些情况下甚至超越了多相机设置。项目将在https://mae-select.github.io上公开。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
