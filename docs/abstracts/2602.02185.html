<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models&lt;br&gt;Vision-DeepResearch基准：重新思考多模态大语言模型的视觉与文本搜索能力&lt;br&gt;[摘要](abstracts/2602.02185.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.</p>
      <h2>摘要 (ZH)</h2>
      <p>多模态大语言模型（MLLMs）已推动视觉问答（VQA）的发展，并支持利用搜索引擎进行复杂视觉-文本事实查找的Vision-DeepResearch系统。然而，评估这些视觉与文本搜索能力仍具挑战，现有基准存在两大局限。首先，现有基准并非以视觉搜索为核心：本需视觉搜索的答案常通过文本问题中的跨文本线索泄露，或可从当前MLLMs的先验世界知识中推断。其次，评估场景过于理想化：在图像搜索方面，所需信息常可通过与完整图像的近似精确匹配获取；而文本搜索则过于直接且挑战性不足。为解决这些问题，我们构建了包含2000个VQA实例的Vision-DeepResearch基准（VDR-Bench）。所有问题均通过细致多阶段筛选流程与严格专家评审创建，旨在评估Vision-DeepResearch系统在真实世界条件下的表现。此外，针对当前MLLMs视觉检索能力不足的问题，我们提出一种简单的多轮裁剪搜索工作流程。该策略被证明能有效提升模型在真实视觉检索场景中的性能。总体而言，我们的研究结果为未来多模态深度研究系统的设计提供了实用指导。代码将在https://github.com/Osilly/Vision-DeepResearch发布。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
