<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Scale Can&#x27;t Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning</h1>
      <h2>Abstract (EN)</h2>
      <p>The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., &quot;at the game today!&quot; is a more likely caption than &quot;a photo of 37 people standing behind a field&quot;. We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.</p>
      <h2>摘要 (ZH)</h2>
      <p>视觉-语言模型（VLMs）在推理能力上的缺失一直是研究讨论的焦点。我们认为，这种行为源于其训练数据中的报告偏差。也就是说，人们默认在描述视觉内容时，会省略监督某些类型推理所需的隐含信息；例如，“今天在比赛现场！”比“一张37人站在场地后面的照片”更可能作为标题。我们通过语用学理论的视角，调查了流行VLMs（OpenCLIP、LLaVA-1.5和Molmo）的基础数据，发现尽管这些语料库达到网络规模且/或为合成生成，报告偏差仍导致四种推理技能（空间、时间、否定和计数）的表示不足。通过一组精心设计的基准测试，我们证明：（i）VLMs在训练数据中因报告偏差而受到抑制的上述推理类型上表现不佳；（ii）与普遍看法相反，扩大数据规模、模型规模以及扩展到多种语言，并不会默认导致这些技能的出现；但（iii）有希望的是，纳入专门收集以获取隐含信息的标注是有效的。我们的发现强调了需要更审慎的训练数据整理方法，而非依赖规模来期待推理能力的自然涌现。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
