<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective</h1>
      <h2>Abstract (EN)</h2>
      <p>Recent advances in vision-language models (VLMs) have shown promise for human-level embodied intelligence. However, existing benchmarks for VLM-driven embodied agents often rely on high-level commands or discretized action spaces, which are non-native settings that differ markedly from real-world control. In addition, current benchmarks focus primarily on high-level tasks and lack joint evaluation and analysis at both low and high levels. To address these limitations, we present NativeEmbodied, a challenging benchmark for VLM-driven embodied agents that uses a unified, native low-level action space. Built on diverse simulated scenes, NativeEmbodied includes three representative high-level tasks in complex scenarios to evaluate overall performance. For more detailed analysis, we further decouple the skills required by complex tasks and construct four types of low-level tasks, each targeting a fundamental embodied skill. This joint evaluation across task and skill granularities enables fine-grained assessment of embodied agents. Experiments with state-of-the-art VLMs reveal clear deficiencies in several fundamental embodied skills, and further analysis shows that these bottlenecks significantly limit performance on high-level tasks. NativeEmbodied highlights key challenges for current VLM-driven embodied agents and provides insights to guide future research.</p>
      <h2>摘要 (ZH)</h2>
      <p>近期视觉语言模型（VLMs）的进展显示出实现人类水平具身智能的潜力。然而，现有针对VLM驱动的具身智能体的基准测试通常依赖于高层指令或离散化的动作空间，这些非原生设定与现实世界的控制方式存在显著差异。此外，当前基准主要关注高层任务，缺乏在低层与高层之间的联合评估与分析。为应对这些局限，我们提出了NativeEmbodied，这是一个针对VLM驱动具身智能体的挑战性基准，采用统一的、原生低层动作空间。基于多样化的模拟场景构建，NativeEmbodied包含三个复杂场景中的代表性高层任务，以评估整体性能。为进行更细致的分析，我们进一步解耦了复杂任务所需的技能，并构建了四类低层任务，每类针对一项基础具身技能。这种跨任务与技能粒度的联合评估，使得对具身智能体的精细化评估成为可能。通过对前沿VLM的实验，我们发现了其在多项基础具身技能上的明显不足，进一步分析表明这些瓶颈显著限制了其在高层任务上的表现。NativeEmbodied揭示了当前VLM驱动具身智能体面临的关键挑战，并为未来研究提供了指导性见解。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
