<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>How Foundational Skills Influence VLM-based Embodied Agents:A Native Perspective&lt;br&gt;基础技能如何影响基于视觉语言模型的具身智能体：一个原生视角&lt;br&gt;[摘要](abstracts/2602.20687.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>Abstract not available.</p>
      <h2>摘要 (ZH)</h2>
      <p>近期视觉语言模型（VLMs）的进展显示出实现人类水平具身智能的潜力。然而，现有针对VLM驱动的具身智能体的基准测试通常依赖于高层指令或离散化的动作空间，这些非原生设定与现实世界的控制方式存在显著差异。此外，当前基准主要关注高层任务，缺乏在低层与高层之间的联合评估与分析。为应对这些局限，我们提出了NativeEmbodied，这是一个针对VLM驱动具身智能体的挑战性基准，采用统一的、原生低层动作空间。基于多样化的模拟场景构建，NativeEmbodied包含三个复杂场景中的代表性高层任务，以评估整体性能。为进行更细致的分析，我们进一步解耦了复杂任务所需的技能，并构建了四类低层任务，每类针对一项基础具身技能。这种跨任务与技能粒度的联合评估，使得对具身智能体的精细化评估成为可能。通过对前沿VLM的实验，我们发现了其在多项基础具身技能上的明显不足，进一步分析表明这些瓶颈显著限制了其在高层任务上的表现。NativeEmbodied揭示了当前VLM驱动具身智能体面临的关键挑战，并为未来研究提供了指导性见解。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
