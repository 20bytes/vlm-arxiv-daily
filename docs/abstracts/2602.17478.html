<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery</h1>
      <h2>Abstract (EN)</h2>
      <p>Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum material flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the dependence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quantum materials, comprising multimodal, physics-informed question-answer pairs designed to teach Multimodal Large Language Models (MLLMs) to understand the appearance and thickness of flakes. Then, we propose Physics-Aware Instruction Tuning (QuPAINT), a multimodal architecture that incorporates a Physics-Informed Attention module to fuse visual embeddings with optical priors, enabling more robust and discriminative flake representations. Finally, we establish QF-Bench, a comprehensive benchmark spanning multiple materials, substrates, and imaging settings, offering standardized protocols for fair and reproducible evaluation.</p>
      <h2>摘要 (ZH)</h2>
      <p>由于层依赖的对比度微妙、标注数据有限以及不同实验室和成像设置间的显著差异，从光学显微镜图像中表征二维量子材料具有挑战性。现有视觉模型因缺乏物理先验知识，且无法泛化至新材料或硬件条件，在此领域表现不佳。本研究提出了一种新的物理感知多模态框架，从数据和模型两方面应对这些限制。我们首先介绍了Synthia，一种基于物理的合成数据生成器，可模拟量子材料薄片在薄膜干涉下的真实光学响应。Synthia生成多样且高质量的样本，有助于减少对专家手动标注的依赖。我们引入了QMat-Instruct，这是首个针对量子材料的大规模指令数据集，包含多模态、物理信息化的问答对，旨在教导多模态大语言模型理解薄片的外观和厚度。随后，我们提出了物理感知指令调优方法QuPAINT，这是一种多模态架构，通过物理信息注意力模块融合视觉嵌入与光学先验，从而实现更鲁棒和更具区分性的薄片表征。最后，我们建立了QF-Bench，一个涵盖多种材料、基底和成像设置的综合性基准，为公平且可复现的评估提供了标准化协议。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
