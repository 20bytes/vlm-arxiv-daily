<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>\textsc{NaVIDA}: Vision-Language Navigation with Inverse Dynamics Augmentation</h1>
      <h2>Abstract (EN)</h2>
      <p>Vision-and-Language Navigation (VLN) requires agents to interpret natural language instructions and act coherently in visually rich environments. However, most existing methods rely on reactive state-action mappings without explicitly modeling how actions causally transform subsequent visual observations. Lacking such vision-action causality, agents cannot anticipate the visual changes induced by its own actions, leading to unstable behaviors, weak generalization, and cumulative error along trajectory. To address these issues, we introduce \textsc{NaVIDA} (\textbf{Nav}igation with \textbf{I}nverse \textbf{D}ynamics \textbf{A}ugmentation), a unified VLN framework that couples policy learning with action-grounded visual dynamics and adaptive execution. \textsc{NaVIDA} augments training with chunk-based inverse-dynamics supervision to learn causal relationship between visual changes and corresponding actions. To structure this supervision and extend the effective planning range, \textsc{NaVIDA} employs hierarchical probabilistic action chunking (HPAC), which organizes trajectories into multi-step chunks and provides discriminative, longer-range visual-change cues. To further curb error accumulation and stabilize behavior at inference, an entropy-guided mechanism adaptively sets the execution horizon of action chunks. Extensive experiments show that \textsc{NaVIDA} achieves superior navigation performance compared to state-of-the-art methods with fewer parameters (3B vs. 8B). Real-world robot evaluations further validate the practical feasibility and effectiveness of our approach. Code and data will be available upon acceptance.</p>
      <h2>摘要 (ZH)</h2>
      <p>视觉语言导航（VLN）要求智能体能够理解自然语言指令，并在视觉丰富的环境中连贯地执行动作。然而，现有方法大多依赖于反应式的状态-动作映射，未能显式建模动作如何因果性地改变后续视觉观察。由于缺乏这种视觉-动作因果关系，智能体无法预测自身动作引发的视觉变化，导致行为不稳定、泛化能力弱以及轨迹上的累积误差。为解决这些问题，我们提出了NaVIDA（基于逆动力学增强的导航），这是一个统一的VLN框架，将策略学习与动作驱动的视觉动态建模及自适应执行相结合。NaVIDA通过基于动作块的逆动力学监督增强训练，以学习视觉变化与对应动作之间的因果关系。为构建这种监督并扩展有效规划范围，NaVIDA采用分层概率动作块化（HPAC）方法，将轨迹组织为多步动作块，并提供具有区分性的长程视觉变化线索。为进一步抑制推理过程中的误差累积并稳定行为，一种基于熵的引导机制自适应地设置动作块的执行范围。大量实验表明，与现有最先进方法相比，NaVIDA以更少的参数量（30亿 vs. 80亿）实现了更优的导航性能。真实世界机器人评估进一步验证了本方法的实际可行性和有效性。代码与数据将在论文录用后公开。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
