<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Spatial-VLN: Zero-Shot Vision-and-Language Navigation With Explicit Spatial Perception and Exploration&lt;br&gt;Spatial-VLN：具备显式空间感知与探索能力的零样本视觉语言导航&lt;br&gt;[摘要](abstracts/2601.12766.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>Abstract not available.</p>
      <h2>摘要 (ZH)</h2>
      <p>利用大型语言模型（LLM）的零样本视觉语言导航（VLN）代理在泛化方面表现出色，但存在空间感知不足的问题。针对复杂连续环境，我们将关键感知瓶颈归纳为三类空间挑战：门交互、多房间导航和模糊指令执行，现有方法在这些方面持续面临高失败率。我们提出了Spatial-VLN，一种感知引导的探索框架，旨在克服这些挑战。该框架包含两个核心模块：空间感知增强（SPE）模块通过全景过滤结合专门的门与区域专家，生成空间连贯、跨视图一致的感知表征；在此基础上，探索式多专家推理（EMR）模块利用并行LLM专家处理路径点级语义和区域级空间转换。当专家预测出现分歧时，查询-探索机制被激活，引导代理主动探测关键区域以解决感知模糊性。在VLN-CE上的实验表明，Spatial-VLN仅使用低成本LLM即实现了最先进的性能。此外，为验证实际应用性，我们引入了一种基于价值的路径点采样策略，有效弥合了仿真到现实的差距。大量真实环境评估证实，该框架在复杂环境中具有卓越的泛化能力和鲁棒性。代码与演示视频发布于https://yueluhhxx.github.io/Spatial-VLN-web/。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
