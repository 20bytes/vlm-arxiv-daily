<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective</h1>
      <h2>Abstract (EN)</h2>
      <p>Recently, reducing redundant visual tokens in vision-language models (VLMs) to accelerate VLM inference has emerged as a hot topic. However, most existing methods rely on heuristics constructed based on inter-visual-token similarity or cross-modal visual-text similarity, which gives rise to certain limitations in compression performance and practical deployment. In contrast, we propose PIO-FVLM from the perspective of inference objectives, which transforms visual token compression into preserving output result invariance and selects tokens primarily by their importance to this goal. Specially, vision tokens are reordered with the guidance of token-level gradient saliency generated by our designed layer-local proxy loss, a coarse constraint from the current layer to the final result. Then the most valuable vision tokens are selected following the non-maximum suppression (NMS) principle. The proposed PIO-FVLM is training-free and compatible with FlashAttention, friendly to practical application and deployment. It can be deployed independently as an encoder-free method, or combined with encoder compression approaches like VisionZip for use as an encoder-involved method. On LLaVA-Next-7B, PIO-FVLM retains just 11.1% of visual tokens but maintains 97.2% of the original performance, with a 2.67$\times$ prefill speedup, 2.11$\times$ inference speedup, 6.22$\times$ lower FLOPs, and 6.05$\times$ reduced KV Cache overhead. Our code is available at https://github.com/ocy1/PIO-FVLM.</p>
      <h2>摘要 (ZH)</h2>
      <p>近年来，通过减少视觉-语言模型（VLMs）中的冗余视觉令牌以加速VLM推理已成为热门话题。然而，现有方法大多依赖于基于视觉令牌间相似性或跨模态视觉-文本相似性构建的启发式规则，这导致其在压缩性能和实际部署中存在一定局限性。相比之下，我们从推理目标的角度出发，提出了PIO-FVLM，将视觉令牌压缩转化为保持输出结果不变性的问题，并依据令牌对此目标的重要性进行筛选。具体而言，我们通过设计的层局部代理损失（一种从当前层到最终结果的粗略约束）生成令牌级梯度显著性，并以此指导视觉令牌重新排序。随后，遵循非极大值抑制（NMS）原则选取最有价值的视觉令牌。所提出的PIO-FVLM无需训练，且与FlashAttention兼容，便于实际应用与部署。它可作为无编码器方法独立部署，也可与VisionZip等编码器压缩方法结合，作为编码器参与的方法使用。在LLaVA-Next-7B模型上，PIO-FVLM仅保留11.1%的视觉令牌，却能维持97.2%的原始性能，同时实现预填充速度提升2.67倍、推理速度提升2.11倍、计算量（FLOPs）降低6.22倍，并减少6.05倍的KV缓存开销。代码已开源：https://github.com/ocy1/PIO-FVLM。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
