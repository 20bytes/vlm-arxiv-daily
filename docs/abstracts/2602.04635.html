<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Relational Scene Graphs for Object Grounding of Natural Language Commands</h1>
      <h2>Abstract (EN)</h2>
      <p>Robots are finding wider adoption in human environments, increasing the need for natural human-robot interaction. However, understanding a natural language command requires the robot to infer the intended task and how to decompose it into executable actions, and to ground those actions in the robot&#x27;s knowledge of the environment, including relevant objects, agents, and locations. This challenge can be addressed by combining the capabilities of Large language models (LLMs) to understand natural language with 3D scene graphs (3DSGs) for grounding inferred actions in a semantic representation of the environment. However, many 3DSGs lack explicit spatial relations between objects, even though humans often rely on these relations to describe an environment. This paper investigates whether incorporating open- or closed-vocabulary spatial relations into 3DSGs can improve the ability of LLMs to interpret natural language commands. To address this, we propose an LLM-based pipeline for target object grounding from open-vocabulary language commands and a vision language model (VLM)-based pipeline to add open-vocabulary spatial edges to 3DSGs from images captured while mapping. Finally, two LLMs are evaluated in a study assessing their performance on the downstream task of target object grounding. Our study demonstrates that explicit spatial relations improve the ability of LLMs to ground objects. Moreover, open-vocabulary relation generation with VLMs proves feasible from robot-captured images, but their advantage over closed-vocabulary relations is found to be limited.</p>
      <h2>摘要 (ZH)</h2>
      <p>随着机器人在人类环境中的应用日益广泛，自然的人机交互需求愈发迫切。然而，理解自然语言指令要求机器人推断预期任务、将其分解为可执行动作，并将这些动作基于机器人对环境（包括相关物体、智能体和位置）的认知进行定位。这一挑战可通过结合大语言模型（LLMs）理解自然语言的能力与三维场景图（3DSGs）在环境语义表征中定位推断动作的能力来解决。然而，许多3DSGs缺乏物体间的显式空间关系，尽管人类在描述环境时常常依赖这些关系。本文探讨了将开放或封闭词汇的空间关系融入3DSGs是否能提升LLMs解释自然语言指令的能力。为此，我们提出了一种基于LLM的管道，用于从开放词汇语言指令中定位目标物体，以及一种基于视觉语言模型（VLM）的管道，用于从建图过程中捕获的图像向3DSGs添加开放词汇空间边。最后，通过一项研究评估了两种LLMs在目标物体定位下游任务中的表现。我们的研究表明，显式空间关系能有效提升LLMs的物体定位能力。此外，基于VLM的开放词汇关系生成在机器人捕获图像中具有可行性，但其相较于封闭词汇关系的优势较为有限。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
