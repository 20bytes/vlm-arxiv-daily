<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>SignVLA: A Gloss-Free Vision-Language-Action Framework for Real-Time Sign Language-Guided Robotic Manipulation</h1>
      <h2>Abstract (EN)</h2>
      <p>We present, to our knowledge, the first sign language-driven Vision-Language-Action (VLA) framework for intuitive and inclusive human-robot interaction. Unlike conventional approaches that rely on gloss annotations as intermediate supervision, the proposed system adopts a gloss-free paradigm and directly maps visual sign gestures to semantic instructions. This design reduces annotation cost and avoids the information loss introduced by gloss representations, enabling more natural and scalable multimodal interaction.   In this work, we focus on a real-time alphabet-level finger-spelling interface that provides a robust and low-latency communication channel for robotic control. Compared with large-scale continuous sign language recognition, alphabet-level interaction offers improved reliability, interpretability, and deployment feasibility in safety-critical embodied environments. The proposed pipeline transforms continuous gesture streams into coherent language commands through geometric normalization, temporal smoothing, and lexical refinement, ensuring stable and consistent interaction.   Furthermore, the framework is designed to support future integration of transformer-based gloss-free sign language models, enabling scalable word-level and sentence-level semantic understanding. Experimental results demonstrate the effectiveness of the proposed system in grounding sign-derived instructions into precise robotic actions under diverse interaction scenarios. These results highlight the potential of the framework to advance accessible, scalable, and multimodal embodied intelligence.</p>
      <h2>摘要 (ZH)</h2>
      <p>我们提出了，据我们所知，首个基于手语驱动的视觉-语言-动作（VLA）框架，旨在实现直观且包容的人机交互。与传统方法依赖注释标注作为中间监督不同，该系统采用无需注释的范式，直接将视觉手语手势映射为语义指令。这一设计降低了标注成本，避免了注释表示带来的信息损失，从而实现了更自然和可扩展的多模态交互。在本工作中，我们专注于实时字母级别的手指拼写界面，为机器人控制提供了一个鲁棒且低延迟的通信通道。与大规模连续手语识别相比，字母级别交互在安全关键的具身环境中提供了更高的可靠性、可解释性和部署可行性。所提出的流程通过几何归一化、时间平滑和词汇精炼，将连续手势流转化为连贯的语言命令，确保了稳定且一致的交互。此外，该框架设计支持未来集成基于Transformer的无注释手语模型，以实现可扩展的单词级别和句子级别语义理解。实验结果表明，所提出的系统在多样化交互场景下，能够有效将手语衍生的指令转化为精确的机器人动作。这些结果突显了该框架在推动可访问、可扩展和多模态具身智能方面的潜力。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
