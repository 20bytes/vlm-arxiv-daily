<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs</h1>
      <h2>Abstract (EN)</h2>
      <p>Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.</p>
      <h2>摘要 (ZH)</h2>
      <p>视觉-语言-动作模型（VLAs）有望将语言指令落地到机器人控制中，但在实践中往往无法忠实地遵循语言指令。当面对缺乏强场景特定监督的指令时，VLAs会遭受反事实失败：它们基于数据集偏见诱导的视觉捷径采取行动，反复执行已习得的行为，并选择训练中频繁出现的对象，而忽略语言意图。为系统研究此问题，我们引入了LIBERO-CF——首个针对VLAs的反事实基准测试，通过在视觉上合理的LIBERO布局中分配替代指令来评估语言遵循能力。我们的评估表明，反事实失败在现有最先进的VLAs中普遍存在且尚未得到充分探索。我们提出了反事实动作引导（CAG），一种简单而有效的双分支推理方案，能显式地正则化VLAs中的语言条件。CAG将标准VLA策略与无语言条件的视觉-动作（VA）模块相结合，实现了动作选择过程中的反事实比较。这一设计减少了对视觉捷径的依赖，提高了在低观测任务上的鲁棒性，且无需额外演示或修改现有架构或预训练模型。大量实验证明了其在不同VLA模型中的即插即用集成能力及持续的性能提升。例如，在LIBERO-CF上，CAG通过免训练策略将低观测任务的语言遵循准确率提升9.7%（π₀.₅指标），任务成功率提升3.6%；当与VA模型结合时，这两项指标分别进一步提升了15.5%和8.5%。在真实世界评估中，CAG平均减少了9.4%的反事实失败，并将任务成功率提高了17.2%。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
