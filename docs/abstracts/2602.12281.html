<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment&lt;br&gt;扩展验证在视觉-语言-动作对齐中比扩展策略学习更有效&lt;br&gt;[摘要](abstracts/2602.12281.html)</h1>
      <h2>Abstract (EN)</h2>
      <p>Abstract not available.</p>
      <h2>摘要 (ZH)</h2>
      <p>通用机器人的长期愿景依赖于其理解和执行自然语言指令的能力。视觉-语言-动作（VLA）模型在这一目标上取得了显著进展，但其生成的动作仍可能与给定指令存在偏差。本文研究了测试时验证作为缩小“意图-动作差距”的手段。我们首先描述了具身指令跟随的测试时扩展规律，并证明联合扩展重述指令的数量和生成动作的多样性，能显著提升测试时样本的多样性，通常比独立扩展每个维度更有效地恢复正确动作。为利用这些扩展规律，我们提出了CoVer——一种用于视觉-语言-动作对齐的对比验证器，并展示了其架构能随着计算资源和数据的增加而优雅扩展。接着，我们引入了“启动时计算”和分层验证推理流程用于VLA模型。在部署时，我们的框架通过视觉语言模型（VLM）预计算一组多样化的重述指令，为每条指令重复生成动作候选，然后使用验证器选择最优的高层提示和低层动作片段。与在相同数据上扩展策略预训练相比，我们的验证方法在SIMPLER基准测试中实现了22%的分布内增益和13%的分布外增益，并在真实世界实验中进一步提升了45%。在PolaRiS基准测试中，CoVer在任务进度上取得了14%的增益，成功率提升了9%。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
