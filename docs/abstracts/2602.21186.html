<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning</h1>
      <h2>Abstract (EN)</h2>
      <p>While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R, a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM, effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R.</p>
      <h2>摘要 (ZH)</h2>
      <p>尽管视觉语言模型在二维视觉理解方面表现出色，但其对三维空间——空间智能基石——的理解与推理能力仍显浅薄。现有方法试图通过依赖显式的三维模态，或为视觉语言模型注入部分视角条件化的几何先验来弥合这一领域鸿沟。然而，此类方法限制了可扩展性，并最终将隐含地从稀疏线索重建整体三维几何这一不适定任务强加于语言模型。本文主张，空间智能可仅从二维视觉中自然涌现，而非通过显式的空间指令调优强加实现。为此，我们提出了Spa3R，一种自监督框架，能够直接从无姿态的多视角图像中学习统一且视角不变的空间表示。Spa3R基于我们提出的预测性空间场建模范式构建，通过学习基于紧凑潜在表示合成任意未见视角的特征场，从而内化对底层三维场景的整体且连贯的理解。我们进一步通过轻量级适配器将预训练的Spa3R编码器集成到现有视觉语言模型中，形成Spa3-VLM，有效将语言推理锚定于全局空间上下文中。在具有挑战性的VSI-Bench上的实验表明，Spa3-VLM在三维视觉问答任务中达到了58.6%的最新准确率，显著超越了先前方法。这些结果凸显了预测性空间场建模作为推进空间智能的可扩展路径的潜力。代码发布于https://github.com/hustvl/Spa3R。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
