<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Abstract</title>
  <style>
    body { margin: 0; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; background: #f7f7f5; color: #1f2937; }
    .wrap { max-width: 900px; margin: 0 auto; padding: 28px 20px 60px; }
    .card { background: #fff; border: 1px solid #e5e7eb; border-radius: 12px; padding: 20px 24px; }
    h1 { font-size: 20px; margin: 0 0 12px; }
    h2 { font-size: 16px; margin: 20px 0 8px; }
    p { line-height: 1.6; white-space: pre-wrap; }
    a { color: #2563eb; }
  </style>
</head>
<body>
  <div class="wrap">
    <div class="card">
      <h1>RoboGene: Boosting VLA Pre-training via Diversity-Driven Agentic Framework for Real-World Task Generation</h1>
      <h2>Abstract (EN)</h2>
      <p>The pursuit of general-purpose robotic manipulation is hindered by the scarcity of diverse, real-world interaction data. Unlike data collection from web in vision or language, robotic data collection is an active process incurring prohibitive physical costs. Consequently, automated task curation to maximize data value remains a critical yet under-explored challenge. Existing manual methods are unscalable and biased toward common tasks, while off-the-shelf foundation models often hallucinate physically infeasible instructions. To address this, we introduce RoboGene, an agentic framework designed to automate the generation of diverse, physically plausible manipulation tasks across single-arm, dual-arm, and mobile robots. RoboGene integrates three core components: diversity-driven sampling for broad task coverage, self-reflection mechanisms to enforce physical constraints, and human-in-the-loop refinement for continuous improvement. We conduct extensive quantitative analysis and large-scale real-world experiments, collecting datasets of 18k trajectories and introducing novel metrics to assess task quality, feasibility, and diversity. Results demonstrate that RoboGene significantly outperforms state-of-the-art foundation models (e.g., GPT-4o, Gemini 2.5 Pro). Furthermore, real-world experiments show that VLA models pre-trained with RoboGene achieve higher success rates and superior generalization, underscoring the importance of high-quality task generation. Our project is available at https://robogene-boost-vla.github.io.</p>
      <h2>摘要 (ZH)</h2>
      <p>通用机器人操作的发展受到多样化真实世界交互数据稀缺的阻碍。与视觉或语言领域可从网络收集数据不同，机器人数据收集是一个主动过程，涉及高昂的物理成本。因此，自动化任务生成以最大化数据价值，成为一个关键但尚未充分探索的挑战。现有手动方法难以扩展且偏向常见任务，而现成的基础模型常产生物理上不可行的指令幻觉。为解决这一问题，我们提出了RoboGene，一个旨在为单臂、双臂及移动机器人自动生成多样化、物理可行的操作任务的智能体框架。RoboGene整合了三个核心组件：用于广泛任务覆盖的多样性驱动采样、强制执行物理约束的自我反思机制，以及持续改进的人机协同优化。我们进行了广泛的定量分析和大规模真实世界实验，收集了包含1.8万条轨迹的数据集，并引入了新指标以评估任务质量、可行性和多样性。结果表明，RoboGene显著优于最先进的基础模型（如GPT-4o、Gemini 2.5 Pro）。此外，真实世界实验显示，使用RoboGene预训练的视觉语言动作模型实现了更高的成功率和更优的泛化能力，凸显了高质量任务生成的重要性。项目网址：https://robogene-boost-vla.github.io。</p>
      <p><a href="../index.html">← Back</a></p>
    </div>
  </div>
</body>
</html>
